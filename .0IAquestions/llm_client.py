"""
Client LLM  pour Jarvis avec streaming natif
Avec support streaming web et CMD
"""

import ollama
import yaml
from pathlib import Path
from hypothalamus.logger import log


class JarvisLLM:
    """LLM Jarvis unifi√© avec support streaming natif"""
    
    def __init__(self, personality="Jarvis"):
        # Charger config
        config_path = Path(__file__).parent.parent / "config" / "settings.yaml"
        if not config_path.exists():
            # Fallback si pas de config projet
            config_path = Path(__file__).parent / "settings.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.model = self.config['llm']['model_default']
        self.personality = personality
        
        log.success(f"LLM pr√™t ({self.model}) - Mode: {personality}", "üß†")

    def estimate_complexity(self, text: str) -> str:
        """Analyse simple de la complexit√© (mots-cl√©s + longueur)"""
        text_lower = text.lower()
        word_count = len(text_lower.split())

        expert_keywords = [
            "analyse", "explique", "pourquoi", "comment", "comparer", "th√©orie",
            "concept", "quantique", "algorithme", "d√©veloppe", "projet", "plan",
            "fonctionnement", "m√©canisme", "histoire", "impact", "diff√©rence"
        ]

        simple_keywords = [
            "salut", "bonjour", "heure", "merci", "date",
            "temp√©rature", "m√©t√©o", "au revoir"
        ]

        # Cas simples : social / commande
        if any(k in text_lower for k in simple_keywords):
            return "Express"

        # Cas complexes : question profonde / notion avanc√©e
        if any(k in text_lower for k in expert_keywords):
            return "Expert"

        # Sinon on se base sur la longueur
        if word_count <= 8:
            return "Express"
        elif word_count <= 30:
            return "Standard"
        else:
            return "Expert"

    def generate_response_stream(self, user_input: str):
        """
        üî• STREAMING NATIF - Yield les tokens un par un depuis Ollama
        Utilis√© par l'interface web pour affichage temps r√©el
        """
        # 1Ô∏è‚É£ Estimation de la complexit√© locale
        complexity = self.estimate_complexity(user_input)

        # 2Ô∏è‚É£ R√©glages dynamiques selon complexit√©
        if complexity == "Express":
            temperature = 0.3
            max_tokens = 500
        elif complexity == "Standard":
            temperature = 0.5
            max_tokens = 1200
        else:  # Expert
            temperature = 0.7
            max_tokens = 3000

        log.info(f"Complexit√© estim√©e : {complexity} ({temperature=}, {max_tokens=})")

        # 3Ô∏è‚É£ Description du ton selon la personnalit√©
        if self.personality == "Jarvis":
            assistant_desc = (
                "Tu es Jarvis, un assistant fran√ßais intelligent, pr√©cis et un peu ironique. "
                "R√©ponds toujours en fran√ßais, de fa√ßon claire, naturelle et concise."
            )
        else:
            assistant_desc = (
                "Tu es Samantha, une assistante fran√ßaise douce, empathique et professionnelle. "
                "R√©ponds toujours en fran√ßais, de fa√ßon fluide, naturelle et concise."
            )

        # 4Ô∏è‚É£ Construire le prompt complet
        prompt = f"""{assistant_desc}

Question ({complexity}): {user_input}

R√©ponse:"""

        # 5Ô∏è‚É£ Appel √† Ollama avec streaming natif
        try:
            log.debug("D√©marrage streaming Ollama...")
            
            # üî• STREAMING NATIF OLLAMA
            stream = ollama.generate(
                model=self.model,
                prompt=prompt,
                stream=True,  # ‚ö° STREAMING ACTIV√â
                options={
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            )
            
            # Yield chaque token re√ßu en temps r√©el
            token_count = 0
            for chunk in stream:
                if 'response' in chunk:
                    token = chunk['response']
                    if token:  # Ignorer les tokens vides
                        token_count += 1
                        yield token
            
            log.debug(f"Streaming termin√©: {token_count} tokens")

        except Exception as e:
            log.error(f"Erreur streaming Ollama: {e}")
            yield "D√©sol√©, une erreur est survenue pendant la r√©ponse."

    def generate_response(self, user_input: str) -> str:
        """
        M√©thode de compatibilit√© (non-streaming)
        R√©cup√®re tout le stream et le joint pour retourner une string compl√®te
        Utilis√© pour compatibilit√© avec ancien code ou usage simple
        """
        # R√©cup√©rer tout le stream et le joindre
        tokens = list(self.generate_response_stream(user_input))
        return ''.join(tokens)

    def ask(self, user_input: str) -> str:
        """M√©thode courte (compatibilit√©)"""
        return self.generate_response(user_input)

# Test standalone
if __name__ == "__main__":
    print("üß™ Test LLM Unifi√©")
    
    try:
        llm = JarvisLLM("Samantha")
        
        print("\nüî• Test streaming:")
        print("Question: Raconte-moi une blague")
        print("R√©ponse: ", end="", flush=True)
        
        for token in llm.generate_response_stream("Raconte-moi une blague"):
            print(token, end="", flush=True)
        
        print("\n\n‚úÖ Test termin√©")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")