
==================================================
FICHIER: .\aspi.py
==================================================

import os

# Nom du fichier de sortie
OUTPUT_FILE = "TOUT_MON_PROJET.txt"

# Dossiers √† ignorer (IMPORTANT pour ne pas aspirer venv ou git)
IGNORE_DIRS = {'.git', 'venv', 'env', '__pycache__', '.idea', '.vscode', 'node_modules'}
# Extensions de fichiers √† inclure
INCLUDE_EXTS = {'.py', '.js', '.html', '.css', '.md', '.json', '.txt'}

def pack_project():
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as outfile:
        # On parcourt tout le dossier
        for root, dirs, files in os.walk('.'):
            # On retire les dossiers ignor√©s de la liste de parcours
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]
            
            for file in files:
                ext = os.path.splitext(file)[1]
                if ext in INCLUDE_EXTS and file != "prepare_for_gemini.py" and file != OUTPUT_FILE:
                    file_path = os.path.join(root, file)
                    
                    # On √©crit un s√©parateur clair pour l'IA
                    outfile.write(f"\n{'='*50}\n")
                    outfile.write(f"FICHIER: {file_path}\n")
                    outfile.write(f"{'='*50}\n\n")
                    
                    try:
                        with open(file_path, 'r', encoding='utf-8') as infile:
                            outfile.write(infile.read())
                    except Exception as e:
                        outfile.write(f"Error reading file: {e}")
                    
                    outfile.write("\n") # Saut de ligne entre fichiers

    print(f"‚úÖ Termin√© ! Le fichier '{OUTPUT_FILE}' est pr√™t √† √™tre envoy√© √† Gemini.")

if __name__ == "__main__":
    pack_project()

==================================================
FICHIER: .\jarvis.py
==================================================

"""
jarvis.py - Point d'entr√©e unifi√© (Interface Web)
Lance automatiquement l'interface web et ouvre le navigateur
"""

import sys
import time
import webbrowser
import threading
from pathlib import Path
import uvicorn
from colorama import init, Fore, Style
from cortex_prefrontal.model_manager import ModelManager
import asyncio
import json
from typing import Dict, Any


# Initialiser colorama
init()

def print_banner():
    """Banni√®re Jarvis avec info web"""
    print(f"""{Fore.CYAN}
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         ü§ñ JARVIS v0.2            ‚ïë
‚ïë    Assistant Vocal Intelligent    ‚ïë
‚ïë     Interface Web Unifi√©e         ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
{Style.RESET_ALL}""")

def check_dependencies():
    """V√©rifier que toutes les d√©pendances sont install√©es"""
    missing = []
    
    try:
        import fastapi
    except ImportError:
        missing.append("fastapi")
    
    try:
        import uvicorn
    except ImportError:
        missing.append("uvicorn")
    
    try:
        import ollama
    except ImportError:
        missing.append("ollama")
    
    if missing:
        print(f"{Fore.RED}‚ùå D√©pendances manquantes: {', '.join(missing)}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}üí° Installez avec: pip install {' '.join(missing)}{Style.RESET_ALL}")
        return False
    
    return True

def check_ollama_running():
    """V√©rifier qu'Ollama est d√©marr√©"""
    try:
        import ollama
        models = ollama.list()
        print(f"{Fore.GREEN}‚úÖ Ollama connect√© ({len(models.get('models', []))} mod√®les){Style.RESET_ALL}")
        return True
    except Exception as e:
        print(f"{Fore.RED}‚ùå Ollama non accessible: {e}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}üí° D√©marrez Ollama puis relancez Jarvis{Style.RESET_ALL}")
        return False

def open_browser_delayed(url: str, delay: float = 2.0):
    """Ouvre le navigateur apr√®s un d√©lai"""
    time.sleep(delay)
    try:
        webbrowser.open(url)
        print(f"{Fore.GREEN}üåê Navigateur ouvert sur {url}{Style.RESET_ALL}")
    except Exception as e:
        print(f"{Fore.YELLOW}‚ö†Ô∏è Impossible d'ouvrir le navigateur: {e}{Style.RESET_ALL}")
        print(f"{Fore.BLUE}üí° Ouvrez manuellement: {url}{Style.RESET_ALL}")

def create_web_app():
    """Cr√©e l'application FastAPI"""
    from fastapi import FastAPI, WebSocket, WebSocketDisconnect
    from fastapi.staticfiles import StaticFiles
    from fastapi.responses import FileResponse
    from contextlib import asynccontextmanager

    # Variables globales pour les gestionnaires
    websocket_relay = None
    interface_bridge = None
    config_coordinator = None
    conversation_flow = None

    # Gestionnaire de mod√®les
    model_manager = ModelManager()

    # Gestionnaire de clonage vocal
    from lobes_temporaux.voice_cloner import VoiceCloner
    voice_cloner = VoiceCloner()
    print(f"{Fore.GREEN}üé≠ Voice Cloner initialis√©{Style.RESET_ALL}")

    @asynccontextmanager
    async def lifespan(app: FastAPI):
        """Gestionnaire de cycle de vie FastAPI"""
        # Startup minimal pour √©viter les blocages
        print(f"{Fore.BLUE}üöÄ D√©marrage FastAPI (initialisation diff√©r√©e)...{Style.RESET_ALL}")
        
        # NOUVEAU : Initialiser le gestionnaire de mod√®les
        print(f"{Fore.GREEN}üß† Gestionnaire de mod√®les initialis√©{Style.RESET_ALL}")
        
        # Variables globales mises √† jour mais pas initialis√©es ici
        nonlocal websocket_relay, interface_bridge, config_coordinator, conversation_flow
        yield
        print(f"{Fore.YELLOW}üõë Arr√™t FastAPI...{Style.RESET_ALL}")

    app = FastAPI(lifespan=lifespan)

    @app.get("/api/models/status")
    async def get_models_status():
        """Retourne le statut de tous les mod√®les"""
        try:
            status = model_manager.get_model_status()
            return {"success": True, "data": status}
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erreur API models status: {e}{Style.RESET_ALL}")
            return {"success": False, "error": str(e)}
    
    @app.post("/api/models/install/{model_id}")
    async def install_model(model_id: str):
        """Lance l'installation d'un mod√®le"""
        try:
            if model_manager.is_model_available(model_id):
                return {"success": False, "error": "Mod√®le d√©j√† install√©"}
            
            # Lancer l'installation en arri√®re-plan
            asyncio.create_task(model_manager.download_model(model_id))
            
            print(f"{Fore.BLUE}üì• Installation {model_id} lanc√©e{Style.RESET_ALL}")
            return {
                "success": True, 
                "message": f"Installation de {model_id} lanc√©e",
                "model_id": model_id
            }
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erreur API install model {model_id}: {e}{Style.RESET_ALL}")
            return {"success": False, "error": str(e)}
    
    @app.post("/api/models/switch/{model_id}")
    async def switch_model(model_id: str):
        """Bascule vers un mod√®le diff√©rent"""
        try:
            if not model_manager.is_model_available(model_id):
                return {"success": False, "error": f"Mod√®le {model_id} non install√©"}
            
            success = model_manager.set_current_model(model_id)
            
            if success:
                print(f"{Fore.GREEN}‚úÖ Bascul√© vers {model_id}{Style.RESET_ALL}")
                return {
                    "success": True,
                    "message": f"Bascul√© vers {model_id}",
                    "current_model": model_id
                }
            else:
                return {"success": False, "error": f"√âchec du basculement vers {model_id}"}
                
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erreur API switch model {model_id}: {e}{Style.RESET_ALL}")
            return {"success": False, "error": str(e)}
    
    @app.get("/api/models/current")
    async def get_current_model():
        """Retourne le mod√®le actuellement utilis√©"""
        try:
            current = model_manager.get_current_model()
            return {
                "success": True,
                "current_model": current,
                "available": model_manager.is_model_available(current) if current else False
            }
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erreur API current model: {e}{Style.RESET_ALL}")
            return {"success": False, "error": str(e)}

    # Fonction d'initialisation diff√©r√©e (appel√©e au premier WebSocket)
    def init_modules_lazy():
        """Initialisation paresseuse des modules"""
        nonlocal websocket_relay, interface_bridge, config_coordinator, conversation_flow
        
        if websocket_relay is None:  # Premi√®re fois
            print(f"{Fore.CYAN}üîß Initialisation diff√©r√©e des modules...{Style.RESET_ALL}")
            
            from thalamus.websocket_relay import WebSocketRelay
            from thalamus.interface_bridge import InterfaceBridge
            from hypothalamus.config_coordinator import ConfigCoordinator
            from lobes_temporaux.conversation_flow import ConversationFlow
            
            websocket_relay = WebSocketRelay()
            interface_bridge = InterfaceBridge()
            conversation_flow = ConversationFlow() 
            config_coordinator = ConfigCoordinator(conversation_flow)
            
            
            print(f"{Fore.GREEN}‚úÖ Modules initialis√©s !{Style.RESET_ALL}")
        
        return websocket_relay, config_coordinator, conversation_flow

    # Cr√©er l'application
    app = FastAPI(title="Jarvis Assistant - Architecture Neuroanatomique", lifespan=lifespan)

    # Servir les fichiers statiques
    app.mount("/static", StaticFiles(directory="web_interface"), name="static")
    app.mount("/config", StaticFiles(directory="config"), name="config")

    # Routes principales
    @app.get("/")
    async def root():
        """Page principale"""
        return FileResponse('web_interface/index.html')

    # Routes API - D√©l√©gation selon architecture neuroanatomique
    @app.get("/api/config")
    async def get_config():
        """Configuration actuelle (Hypothalamus)"""
        try:
            _, coordinator, _ = init_modules_lazy()
            if coordinator:
                return coordinator.get_current_config()
            return {"error": "Config coordinator non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.post("/api/config")
    async def update_config(config: dict):
        """Mettre √† jour la configuration (Hypothalamus)"""
        try:
            _, coordinator, _ = init_modules_lazy()
            if coordinator:
                return await coordinator.update_config(config)
            return {"error": "Config coordinator non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.get("/api/conversation")
    async def get_conversation():
        """Historique de conversation (Lobes Temporaux)"""
        try:
            _, _, flow = init_modules_lazy()
            if flow:
                return flow.get_history()
            return {"error": "Conversation flow non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.delete("/api/conversation")
    async def clear_conversation():
        """Effacer l'historique (Lobes Temporaux)"""
        try:
            _, _, flow = init_modules_lazy()
            if flow:
                return flow.clear_history()
            return {"error": "Conversation flow non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.get("/api/voices")
    async def get_available_voices():
        """Voix disponibles (Hypothalamus)"""
        try:
            _, coordinator, _ = init_modules_lazy()
            if coordinator:
                return coordinator.get_available_voices()
            return {"error": "Config coordinator non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}
    
    @app.get("/api/models")
    async def get_models():
        try:
            status = model_manager.get_model_status()
            installed_models = [
                model_id for model_id, model_info in status['models'].items() 
                if model_info.get('installed', False)
            ]
            return {
                "success": True,
                "models": installed_models,
                "current_model": status.get('current_model')
            }
        except Exception as e:
            return {"success": False, "error": str(e)}
        
    @app.get("/api/devices")
    async def get_available_devices():
        """P√©riph√©riques audio disponibles (Hypothalamus)"""
        try:
            _, coordinator, _ = init_modules_lazy()
            if coordinator:
                return coordinator.get_available_devices()
            return {"error": "Config coordinator non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.get("/api/backgrounds")
    async def get_backgrounds():
        """Endpoint pour r√©cup√©rer la liste des arri√®re-plans"""
        # V√©rifier si les modules sont initialis√©s
        if interface_bridge is None:
            # Initialisation lazy si n√©cessaire
            _, _, _ = init_modules_lazy()
        
        return interface_bridge.get_available_backgrounds()

    # WebSocket - Thalamus (Hub communication)
    @app.websocket("/ws")
    async def websocket_endpoint(websocket: WebSocket):
        """WebSocket principal - Thalamus relay avec initialisation diff√©r√©e"""
        try:
            # Initialisation diff√©r√©e des modules
            relay, coordinator, flow = init_modules_lazy()
            
            if not all([relay, coordinator, flow]):
                await websocket.close(code=1011, reason="Modules neuroanatomiques non initialis√©s")
                return
            
            await relay.handle_connection(
                websocket, 
                flow,        # Lobes temporaux
                coordinator  # Hypothalamus
            )
        except WebSocketDisconnect:
            pass  # D√©connexion normale
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erreur Thalamus WebSocket: {e}{Style.RESET_ALL}")

    # Routes Voice Lab
    @app.post("/api/voice/clone")
    async def clone_voice(request: dict):
        """Clone une voix √† partir d'un √©chantillon audio"""
        try:
            import base64
            
            # D√©coder l'audio base64
            audio_data = base64.b64decode(request['audio_data'])
            
            result = await voice_cloner.clone_voice(
                audio_data=audio_data,
                voice_name=request['voice_name'],
                description=request.get('description', ''),
                file_type=request.get('file_type', 'audio')
            )
            
            return result
            
        except Exception as e:
            log.error(f"Erreur clonage voix: {e}")
            return {"success": False, "error": str(e)}

    @app.get("/api/voice/cloned/list")
    async def list_cloned_voices():
        """Liste uniquement les voix clon√©es"""
        try:
            voices = voice_cloner.list_cloned_voices()
            return {"success": True, "voices": voices}
        except Exception as e:
            return {"success": False, "error": str(e), "voices": []}

    @app.get("/api/voice/all/list")
    async def list_all_voices():
        """Liste toutes les voix (pr√©d√©finies + clon√©es)"""
        try:
            result = voice_cloner.get_all_voices()
            return result
        except Exception as e:
            return {"success": False, "error": str(e)}

    @app.post("/api/voice/test")
    async def test_voice(request: dict):
        """Teste une voix avec du texte"""
        try:
            voice_id = request['voice_id']
            text = request.get('text', 'Test de voix')
            
            # Synth√©tiser l'audio
            audio_data = await voice_cloner.synthesize_with_voice(text, voice_id)
            
            if audio_data:
                # Jouer l'audio via TTS existant
                from lobes_temporaux.tts import TextToSpeech
                tts = TextToSpeech()
                
                # Sauvegarder temporairement et jouer
                import tempfile
                with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as f:
                    f.write(audio_data)
                    temp_path = f.name
                
                tts.play_audio_file(temp_path)
                
                # Nettoyer
                import os
                os.remove(temp_path)
                
                return {"success": True}
            else:
                return {"success": False, "error": "Synth√®se √©chou√©e"}
                
        except Exception as e:
            return {"success": False, "error": str(e)}

    @app.post("/api/voice/set-default")
    async def set_default_voice(request: dict):
        """D√©finit la voix par d√©faut"""
        try:
            voice_id = request['voice_id']
            result = voice_cloner.set_default_voice(voice_id)
            
            if result['success']:
                # Mettre √† jour la conversation flow
                _, _, conversation_flow = init_modules_lazy()
                if conversation_flow:
                    # Recharger le TTS avec la nouvelle voix
                    voice_config = voice_cloner.voices_config['cloned_voices'].get(voice_id)
                    if not voice_config:
                        voice_config = voice_cloner.voices_config['voices'].get(voice_id)
                    
                    if voice_config:
                        await conversation_flow.reload_tts(
                            voice_config.get('model', 'edge-tts'),
                            voice_config['name'],
                            voice_config.get('edge_voice')
                        )
            
            return result
            
        except Exception as e:
            return {"success": False, "error": str(e)}

    @app.put("/api/voice/rename/{voice_id}")
    async def rename_voice(voice_id: str, request: dict):
        """Renomme une voix clon√©e"""
        try:
            result = voice_cloner.rename_voice(
                voice_id,
                request['new_name'],
                request.get('new_description')
            )
            return result
        except Exception as e:
            return {"success": False, "error": str(e)}

    @app.delete("/api/voice/delete/{voice_id}")
    async def delete_voice(voice_id: str):
        """Supprime une voix clon√©e"""
        try:
            return voice_cloner.delete_voice(voice_id)
        except Exception as e:
            return {"success": False, "error": str(e)}

    @app.get("/api/voice/stats")
    async def get_voice_stats():
        """Retourne les statistiques des voix"""
        try:
            status = voice_cloner.get_status()
            return {"success": True, **status}
        except Exception as e:
            return {"success": False, "error": str(e)}

    return app

def main():
    """Point d'entr√©e principal"""
    print_banner()
    
    # V√©rifications pr√©alables
    print(f"{Fore.BLUE}üîç V√©rification des pr√©requis...{Style.RESET_ALL}")
    
    if not check_dependencies():
        return 1
    
    if not check_ollama_running():
        return 1
    
    # Cr√©er l'application web
    app = create_web_app()
    
    # URL de l'interface
    url = f"http://localhost:8000"
    
    # Programmer l'ouverture du navigateur
    browser_thread = threading.Thread(
        target=open_browser_delayed, 
        args=(url, 3.0),
        daemon=True
    )
    browser_thread.start()
    
    # D√©marrer le serveur
    print(f"{Fore.BLUE}üåê D√©marrage de l'interface web...{Style.RESET_ALL}")
    print(f"{Fore.GREEN}üìç Interface accessible sur: {url}{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}üí° Le navigateur va s'ouvrir automatiquement{Style.RESET_ALL}")
    print(f"{Fore.CYAN}üîÑ Appuyez Ctrl+C pour arr√™ter{Style.RESET_ALL}\n")
    
    try:
        # Lancer uvicorn
        uvicorn.run(
            app,
            host="127.0.0.1",
            port=8000,
            log_level="error",  # Moins verbeux
            access_log=False    # Pas de logs d'acc√®s
        )
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}üõë Arr√™t demand√© par l'utilisateur{Style.RESET_ALL}")
        print(f"{Fore.GREEN}üëã Au revoir !{Style.RESET_ALL}")
        return 0
    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Erreur fatale: {e}{Style.RESET_ALL}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

==================================================
FICHIER: .\README.md
==================================================

# ü§ñ Jarvis - Assistant Vocal Intelligent

Assistant vocal local utilisant Llama 3.1, Whisper et Coqui TTS.

## üöÄ Fonctionnalit√©s

- ‚úÖ Reconnaissance vocale (Whisper)
- ‚úÖ LLM local (Llama 3.1:8b via Ollama)
- ‚úÖ Synth√®se vocale avec voice cloning (XTTS)
- ‚úÖ Streaming audio optimis√©
- ‚úÖ Multi-personnalit√©s (Jarvis/Samantha)

## üìã Pr√©requis

- Python 3.10+
- Ollama install√© avec llama3.1:8b
- 16GB+ RAM recommand√©s

## üîß Installation
```bash
git clone https://github.com/christophelancirica/Jarvis.git
cd Jarvis
python -m venv venv
venv\Scripts\activate  # Windows
pip install -r requirements.txt
```

## üéØ Utilisation
```bash
python jarvis.py
```

## üß† Architecture

- `cortex_prefrontal/` : Gestion LLM
- `lobes_temporaux/` : STT/TTS
- `hypothalamus/` : Logger syst√®me

==================================================
FICHIER: .\requirements.txt
==================================================

# ============================================================
# JARVIS - Requirements.txt COMPLET ET FIX√â
# ============================================================
# Installation : pip install -r requirements.txt

# ============================================================
# LLM Local
# ============================================================
ollama>=0.1.6

# ============================================================
# Speech-to-Text (STT)
# ============================================================
faster-whisper>=0.10.0
openai-whisper>=20231117
webrtcvad>=2.0.10

# ============================================================
# Transformers - VERSION FIXE CRITIQUE
# ============================================================
# NE PAS MODIFIER - Requis pour BeamSearchScorer
transformers==4.33.0

# ============================================================
# Text-to-Speech (TTS)
# ============================================================
edge-tts>=6.1.9  # Microsoft TTS
pygame>=2.5.2     # Pour lecture audio Edge-TTS

# ============================================================
# Voice Cloning - VERSIONS CRITIQUES
# ============================================================
# PyTorch - VERSION FIXE pour √©viter warnings weights_only
torch==2.1.0
torchaudio==2.1.0
torchcodec==0.8

# Coqui TTS pour clonage vocal
TTS>=0.22.0

# ============================================================
# Audio Processing
# ============================================================
pyaudio>=0.2.13
sounddevice>=0.4.6
soundfile>=0.12.1
scipy>=1.10.0,<1.14.0
librosa==0.11.0
numphy==1.22.3
numba>=0.57.0

# ============================================================
# Web Framework
# ============================================================
fastapi>=0.104.0
uvicorn>=0.24.0
websockets>=12.0
python-multipart>=0.0.6

# ============================================================
# Utilities
# ============================================================
pyyaml>=6.0
colorama>=0.4.6
keyboard>=0.13.5
psutil>=5.9.6
Pillow>=10.0.0
num2words

# ============================================================
# D√©pendances additionnelles TTS
# ============================================================
einops>=0.7.0
encodec>=0.1.1
inflect>=7.0.0
anyascii>=0.3.2
pydub>=0.25.1

# ============================================================
# NOTES IMPORTANTES
# ============================================================

# VERSIONS CRITIQUES √Ä NE PAS MODIFIER :
# - transformers==4.33.0  ‚Üí Pour BeamSearchScorer
# - torch==2.3.1          ‚Üí √âvite probl√®me weights_only de PyTorch 2.6+
# - numpy<2.0.0           ‚Üí TTS pas compatible numpy 2.0

# FFmpeg REQUIS :
# Windows : winget install "FFmpeg (Essentials Build)"
# Linux   : sudo apt install ffmpeg

# INSTALLATION COMPL√àTE :
# 1. python -m venv venv
# 2. venv\Scripts\activate
# 3. pip install -r requirements.txt
# 4. Installer FFmpeg (voir ci-dessus)

==================================================
FICHIER: .\.0IAquestions\app.js
==================================================

// Variables globales
let ws = null;
let isConnected = false;
let isListening = false;
let currentTheme = 'light';
let debugVisible = false;
let stats = {
    messages: 0,
    tokens: 0,
    totalTime: 0,
    ttsEfficiency: 100
};

// Initialisation
document.addEventListener('DOMContentLoaded', function() {
    initializeWebSocket();
    loadSettings();
    updateUI();
    initializeSliders();  // Nouvelle fonction
});

// Initialiser les sliders avec mise √† jour temps r√©el
function initializeSliders() {
    // Slider vitesse de parole
    const voiceSpeed = document.getElementById('voice-speed');
    const voiceSpeedValue = document.getElementById('voice-speed-value');
    if (voiceSpeed && voiceSpeedValue) {
        voiceSpeed.addEventListener('input', function() {
            voiceSpeedValue.textContent = this.value + 'x';
        });
    }
    
    // Slider volume
    const voiceVolume = document.getElementById('voice-volume');
    const voiceVolumeValue = document.getElementById('voice-volume-value');
    if (voiceVolume && voiceVolumeValue) {
        voiceVolume.addEventListener('input', function() {
            voiceVolumeValue.textContent = this.value + '%';
        });
    }
    
    // Slider sensibilit√© audio
    const audioSensitivity = document.getElementById('audio-sensitivity');
    const audioSensitivityValue = document.getElementById('audio-sensitivity-value');
    if (audioSensitivity && audioSensitivityValue) {
        audioSensitivity.addEventListener('input', function() {
            audioSensitivityValue.textContent = this.value;
        });
    }
    
    // Slider temp√©rature LLM
    const llmTemperature = document.getElementById('llm-temperature');
    const llmTemperatureValue = document.getElementById('llm-temperature-value');
    if (llmTemperature && llmTemperatureValue) {
        llmTemperature.addEventListener('input', function() {
            llmTemperatureValue.textContent = this.value;
        });
    }
}

// WebSocket
function initializeWebSocket() {
    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    const wsUrl = `${protocol}//${window.location.host}/ws`;
    
    ws = new WebSocket(wsUrl);
    
    ws.onopen = function() {
        isConnected = true;
        updateConnectionStatus();
        addLogEntry('Connexion WebSocket √©tablie', 'success');
        
        // Charger imm√©diatement la configuration actuelle
        loadCurrentConfig();
    };
    
    ws.onmessage = function(event) {
        const data = JSON.parse(event.data);
        handleWebSocketMessage(data);
    };
    
    ws.onclose = function() {
        isConnected = false;
        updateConnectionStatus();
        addLogEntry('Connexion WebSocket ferm√©e', 'warning');
        
        // Tentative de reconnexion
        setTimeout(() => {
            if (!isConnected) {
                addLogEntry('Tentative de reconnexion...', 'info');
                initializeWebSocket();
            }
        }, 3000);
    };
    
    ws.onerror = function(error) {
        addLogEntry(`Erreur WebSocket: ${error}`, 'error');
    };
}

function handleWebSocketMessage(data) {
    switch(data.type) {
        case 'status':
            // Message de statut syst√®me (initialisation)
            if (data.personality) {
                updatePersonality(data.personality);
                // Message de bienvenue une fois initialis√©
                addSystemMessage(`‚úÖ ${data.personality} est pr√™t !`);
            }
            break;
            
        case 'llm_token':
            appendToCurrentResponse(data.content);
            break;
            
        case 'tts_queued':
            if (data.metadata.success) {
                addLogEntry(`TTS: Phrase ajout√©e (${data.metadata.length} chars)`, 'info');
            } else {
                addLogEntry('TTS: Queue pleine, phrase ignor√©e', 'warning');
                updateTTSEfficiency(false);
            }
            break;
            
        case 'first_token':
            addLogEntry(`Premier token LLM: ${data.metadata.ttft.toFixed(2)}s`, 'info');
            break;
            
        case 'llm_complete':
            finishCurrentResponse();
            updateStats(data.metadata);
            break;
            
        case 'transcription':
            // üîß FIX: Ne plus ajouter le message ici pour √©viter la duplication
            // addUserMessage(data.content);  ‚Üê SUPPRIMER CETTE LIGNE
            
            // Mettre le texte dans l'input et envoyer
            setInputValue(data.content);
            
            // Traiter automatiquement
            sendMessage();
            break;
            
        case 'listening_start':
            setListeningState(true);
            break;
            
        case 'listening_end':
            setListeningState(false);
            break;
            
        case 'config_updated':
            // Nouveau: Retour des param√®tres appliqu√©s
            if (data.success) {
                addLogEntry(`‚úÖ ${data.message}`, 'success');
                // Recharger la config affich√©e
                loadCurrentSettings();
            } else {
                addLogEntry(`‚ùå Erreur param√®tres: ${data.message}`, 'error');
            }
            break;
            
        case 'message_processing_start':
            // Nouveau: Message en cours de traitement
            startNewAssistantResponse();
            break;
            
        case 'error':
            // Logs d'erreur uniquement dans debug, pas dans conversation
            addLogEntry(`Erreur: ${data.content}`, 'error');
            break;
            
        case 'pong':
            // Keep-alive response
            break;
    }
}

//  Fonction sendMessage 
function sendMessage() {
    const input = document.getElementById('message-input');
    const message = input.value.trim();
    
    if (!message || !isConnected) return;
    
    // Ajouter le message √† l'interface
    addUserMessage(message);
    
    // Envoyer via WebSocket
    ws.send(JSON.stringify({
        type: 'text_message',
        content: message
    }));
    
    // Vider l'input
    input.value = '';
    
    // Statistiques
    stats.messages++;
    updateStatsDisplay();
    
    addLogEntry(`Message envoy√©: ${message.substring(0, 50)}...`, 'info');
}

function toggleVoiceInput() {
    if (!isConnected) {
        addLogEntry('Connexion requise pour l\'entr√©e vocale', 'error');
        return;
    }
    
    if (isListening) {
        // Ne pas permettre d'arr√™ter manuellement - l'√©coute s'arr√™te automatiquement
        addLogEntry('√âcoute en cours, veuillez patienter...', 'info');
        return;
    } else {
        // D√©marrer l'√©coute
        ws.send(JSON.stringify({
            type: 'voice_input'
        }));
        
        addLogEntry('D√©marrage de l\'√©coute vocale', 'info');
    }
}

function setListeningState(listening) {
    isListening = listening;
    const micButton = document.getElementById('mic-button');
    const micStatus = micButton.querySelector('.mic-status');
    
    if (listening) {
        micButton.classList.add('active');
        micStatus.textContent = '√âcoute...';
    } else {
        micButton.classList.remove('active');
        micStatus.textContent = 'Parler';
    }
}

// Gestion de l'interface de conversation
function addUserMessage(content) {
    const container = document.getElementById('dialogue-container');
    const messageDiv = createMessageBubble('user', content);
    container.appendChild(messageDiv);
    scrollToBottom();
}

function addSystemMessage(content, type = 'info') {
    const container = document.getElementById('dialogue-container');
    const messageDiv = createMessageBubble('system', content);
    container.appendChild(messageDiv);
    scrollToBottom();
}

function startNewAssistantResponse() {
    const container = document.getElementById('dialogue-container');
    const messageDiv = createMessageBubble('assistant', '');
    messageDiv.id = 'current-response';
    container.appendChild(messageDiv);
    scrollToBottom();
}

function appendToCurrentResponse(token) {
    const currentResponse = document.getElementById('current-response');
    if (currentResponse) {
        const content = currentResponse.querySelector('.message-content');
        if (content) {
            content.textContent += token;
            scrollToBottom();
            
            // Compter les tokens
            stats.tokens++;
            if (stats.tokens % 10 === 0) {
                updateStatsDisplay();
            }
        }
    }
}

function finishCurrentResponse() {
    const currentResponse = document.getElementById('current-response');
    if (currentResponse) {
        currentResponse.removeAttribute('id');
        addTimeStamp(currentResponse);
        scrollToBottom();
    }
}

function createMessageBubble(type, content) {
    const messageDiv = document.createElement('div');
    messageDiv.className = `message-bubble ${type}`;
    
    const contentDiv = document.createElement('div');
    contentDiv.className = 'message-content';
    contentDiv.textContent = content;
    
    messageDiv.appendChild(contentDiv);
    
    return messageDiv;
}

function addTimeStamp(messageDiv) {
    const timeDiv = document.createElement('div');
    timeDiv.className = 'message-time';
    timeDiv.textContent = new Date().toLocaleTimeString();
    messageDiv.appendChild(timeDiv);
}

function scrollToBottom() {
    const container = document.getElementById('dialogue-container');
    container.scrollTop = container.scrollHeight;
}

function clearConversation() {
    if (confirm('Effacer toute la conversation ?')) {
        const container = document.getElementById('dialogue-container');
        container.innerHTML = `
            <div class="welcome-message">
                <div class="message-bubble system">
                    <div class="message-content">
                        <p>Conversation effac√©e</p>
                    </div>
                </div>
            </div>
        `;
        
        addLogEntry('Conversation effac√©e', 'info');
    }
}

// Gestion des param√®tres et modales
function openSettings() {
    const modal = document.getElementById('settings-modal');
    modal.classList.add('show');
    loadCurrentSettings();
}

function closeSettings() {
    const modal = document.getElementById('settings-modal');
    modal.classList.remove('show');
}

function showHelp() {
    const modal = document.getElementById('help-modal');
    modal.classList.add('show');
}

function closeHelp() {
    const modal = document.getElementById('help-modal');
    modal.classList.remove('show');
}

function switchSettingsTab(tabName) {
    // Cacher tous les onglets
    document.querySelectorAll('.settings-tab').forEach(tab => {
        tab.classList.remove('active');
    });
    
    // D√©sactiver tous les boutons
    document.querySelectorAll('.settings-tabs .tab-btn').forEach(btn => {
        btn.classList.remove('active');
    });
    
    // Activer l'onglet s√©lectionn√©
    document.getElementById(`settings-${tabName}`).classList.add('active');
    event.target.classList.add('active');
}

function switchDebugTab(tabName) {
    // Cacher tous les onglets debug
    document.querySelectorAll('.debug-tab').forEach(tab => {
        tab.classList.remove('active');
    });
    
    // D√©sactiver tous les boutons
    document.querySelectorAll('.debug-tabs .tab-btn').forEach(btn => {
        btn.classList.remove('active');
    });
    
    // Activer l'onglet s√©lectionn√©
    document.getElementById(`debug-${tabName}`).classList.add('active');
    event.target.classList.add('active');
}

// Gestion des th√®mes
function toggleTheme() {
    const themes = ['light', 'dark', 'jarvis'];
    const currentIndex = themes.indexOf(currentTheme);
    const nextIndex = (currentIndex + 1) % themes.length;
    
    setTheme(themes[nextIndex]);
}

function setTheme(theme) {
    currentTheme = theme;
    document.body.className = `theme-${theme}`;
    
    const themeIcon = document.getElementById('theme-icon');
    const themeText = document.getElementById('theme-text');
    
    // Configuration des th√®mes (facilement modifiable)
    const themeConfig = {
        'light': {
            icon: 'üåô',
            text: 'Mode Sombre',  // ‚Üê Personnalisable ici
            next: 'Passer en mode sombre'
        },
        'dark': {
            icon: 'ü§ñ', 
            text: 'Mode Jarvis',  // ‚Üê Personnalisable ici
            next: 'Passer en mode Jarvis'
        },
        'jarvis': {
            icon: '‚òÄÔ∏è',
            text: 'Mode Clair',   // ‚Üê Personnalisable ici  
            next: 'Passer en mode clair'
        }
    };
    
    const config = themeConfig[theme];
    if (config) {
        themeIcon.textContent = config.icon;
        themeText.textContent = config.text;
        
        // Mettre √† jour le titre du bouton pour l'accessibilit√©
        const themeButton = themeText.closest('.nav-btn');
        if (themeButton) {
            themeButton.title = config.next;
        }
    }
    
    saveSettings();
    addLogEntry(`Th√®me chang√©: ${config?.text || theme}`, 'info');
}

// Debug et logs
function toggleDebug() {
    debugVisible = !debugVisible;
    const debugSection = document.getElementById('debug-section');
    const mainContent = document.querySelector('.main-content');
    
    if (debugVisible) {
        debugSection.classList.remove('hidden');
        mainContent.classList.remove('debug-hidden');
    } else {
        debugSection.classList.add('hidden');
        mainContent.classList.add('debug-hidden');
    }
    
    addLogEntry(`Debug: ${debugVisible ? 'activ√©' : 'd√©sactiv√©'}`, 'info');
}

function addLogEntry(message, type = 'info') {
    const container = document.getElementById('log-container');
    
    const logDiv = document.createElement('div');
    logDiv.className = `log-entry ${type}`;
    
    const timeSpan = document.createElement('span');
    timeSpan.className = 'log-time';
    timeSpan.textContent = new Date().toLocaleTimeString();
    
    const messageSpan = document.createElement('span');
    messageSpan.className = 'log-message';
    messageSpan.textContent = message;
    
    logDiv.appendChild(timeSpan);
    logDiv.appendChild(messageSpan);
    container.appendChild(logDiv);
    
    // Limiter le nombre de logs
    const maxLogs = 100;
    while (container.children.length > maxLogs) {
        container.removeChild(container.firstChild);
    }
    
    // Scroll automatique
    container.scrollTop = container.scrollHeight;
}

// Statistiques
function updateStats(metadata) {
    if (metadata.total_time) {
        stats.totalTime = (stats.totalTime + metadata.total_time) / 2; // Moyenne mobile
    }
    
    updateStatsDisplay();
}

function updateStatsDisplay() {
    document.getElementById('stat-messages').textContent = stats.messages;
    document.getElementById('stat-tokens').textContent = stats.tokens;
    document.getElementById('stat-avgtime').textContent = `${stats.totalTime.toFixed(1)}s`;
    document.getElementById('stat-tts-efficiency').textContent = `${stats.ttsEfficiency.toFixed(0)}%`;
}

function updateTTSEfficiency(success) {
    // Mise √† jour simple de l'efficacit√© TTS
    if (success) {
        stats.ttsEfficiency = Math.min(100, stats.ttsEfficiency + 0.1);
    } else {
        stats.ttsEfficiency = Math.max(0, stats.ttsEfficiency - 1);
    }
}

// Configuration
function updatePersonality(personalityDisplay) {
    // Extraire le nom depuis "Assistant virtuel - Nom"
    const name = personalityDisplay.replace('Assistant virtuel - ', '');
    
    // Mettre √† jour le titre de la page
    document.getElementById('page-title').textContent = `Assistant virtuel - ${name}`;
    
    // Mettre √† jour le logo dans le header
    document.getElementById('assistant-name').textContent = name;
    
    // Mettre √† jour l'affichage de config (si l'√©l√©ment existe)
    const configElement = document.getElementById('config-personality');
    if (configElement) {
        configElement.textContent = personalityDisplay;
    }
    
    // Log du changement
    addLogEntry(`Assistant mis √† jour: ${name}`, 'info');
}

function updateConnectionStatus() {
    const indicator = document.getElementById('status-indicator');
    
    if (isConnected) {
        indicator.classList.remove('offline');
        indicator.title = 'Connect√©';
    } else {
        indicator.classList.add('offline');
        indicator.title = 'D√©connect√©';
    }
}

// Sauvegarde/Chargement des param√®tres
function saveSettings() {
    const settings = {
        theme: currentTheme,
        debugVisible: debugVisible
    };
    
    localStorage.setItem('jarvis-settings', JSON.stringify(settings));
}

function loadSettings() {
    const saved = localStorage.getItem('jarvis-settings');
    if (saved) {
        const settings = JSON.parse(saved);
        
        if (settings.theme) {
            setTheme(settings.theme);
        }
        
        if (settings.debugVisible) {
            debugVisible = settings.debugVisible;
            updateUI();
        }
    }
}

function loadCurrentConfig() {
    // Charger la config actuelle depuis l'API au d√©marrage
    fetch('/api/config')
        .then(response => response.json())
        .then(data => {
            if (data.success && data.config) {
                const config = data.config;
                
                // Mettre √† jour la personnalit√© dans l'interface
                if (config.display_name) {
                    updatePersonality(config.display_name);
                    addLogEntry(`‚úÖ ${config.display_name} charg√©`, 'success');
                } else if (config.personality) {
                    const displayName = `Assistant virtuel - ${config.personality}`;
                    updatePersonality(displayName);
                    addLogEntry(`‚úÖ ${displayName} charg√©`, 'success');
                }
                
                // Mettre √† jour le th√®me si diff√©rent
                if (config.theme && config.theme !== currentTheme) {
                    setTheme(config.theme);
                }
                
                // Mettre √† jour l'affichage de configuration
                updateConfigDisplay(config);
                
            } else {
                addLogEntry('‚ö†Ô∏è Config serveur non disponible', 'warning');
            }
        })
        .catch(error => {
            addLogEntry(`‚ùå Erreur chargement config: ${error}`, 'error');
        });
}

function loadCurrentSettings() {
    // Charger les param√®tres actuels depuis le backend
    addLogEntry('Chargement param√®tres depuis le serveur...', 'info');
    
    fetch('/api/config')
        .then(response => response.json())
        .then(data => {
            if (data.success && data.config) {
                const config = data.config;
                
                // Interface
                document.getElementById('interface-theme').value = config.theme || currentTheme;
                const animationsCheckbox = document.getElementById('interface-animations');
                if (animationsCheckbox) {
                    animationsCheckbox.checked = config.interface_animations !== false;
                }
                
                // Voix et personnalit√©
                const personalitySelect = document.getElementById('voice-personality');
                if (personalitySelect && config.personality) {
                    personalitySelect.value = config.personality;
                }
                
                // Audio (sliders avec valeurs)
                const voiceSpeed = document.getElementById('voice-speed');
                const voiceSpeedValue = document.getElementById('voice-speed-value');
                if (voiceSpeed && config.voice_speed) {
                    voiceSpeed.value = config.voice_speed;
                    if (voiceSpeedValue) voiceSpeedValue.textContent = config.voice_speed;
                }
                
                const voiceVolume = document.getElementById('voice-volume');
                const voiceVolumeValue = document.getElementById('voice-volume-value');
                if (voiceVolume && config.voice_volume) {
                    voiceVolume.value = config.voice_volume;
                    if (voiceVolumeValue) voiceVolumeValue.textContent = config.voice_volume + '%';
                }
                
                const audioSensitivity = document.getElementById('audio-sensitivity');
                const audioSensitivityValue = document.getElementById('audio-sensitivity-value');
                if (audioSensitivity && config.audio_sensitivity) {
                    audioSensitivity.value = config.audio_sensitivity;
                    if (audioSensitivityValue) audioSensitivityValue.textContent = config.audio_sensitivity;
                }
                
                // LLM
                const llmTemperature = document.getElementById('llm-temperature');
                const llmTemperatureValue = document.getElementById('llm-temperature-value');
                if (llmTemperature && config.llm_temperature) {
                    llmTemperature.value = config.llm_temperature;
                    if (llmTemperatureValue) llmTemperatureValue.textContent = config.llm_temperature;
                }
                
                // Mettre √† jour l'affichage de config
                updateConfigDisplay(config);
                
                addLogEntry('‚úÖ Param√®tres charg√©s depuis le serveur', 'success');
            } else {
                addLogEntry('‚ùå Erreur chargement config serveur', 'error');
            }
        })
        .catch(error => {
            addLogEntry(`‚ùå Erreur API config: ${error}`, 'error');
            console.error('Erreur chargement config:', error);
        });
}

function updateConfigDisplay(config) {
    // Affichage config dans l'interface
    const elements = {
        'config-llm': config.llm_model || 'llama3.1:8b',
        'config-tts': config.tts_model || 'edge-tts', 
        'config-personality': config.display_name || config.personality || 'Samantha',
        'config-audio': config.device_index !== null ? `Device ${config.device_index}` : 'Auto',
        'config-theme': config.theme || 'light'
    };
    
    // Mettre √† jour les √©l√©ments qui existent
    Object.entries(elements).forEach(([id, value]) => {
        const element = document.getElementById(id);
        if (element) {
            element.textContent = value;
        }
    });
    
    // Mettre √† jour la personnalit√© dans le header si disponible
    if (config.display_name) {
        updatePersonality(config.display_name);
    }
}

function saveSettings() {
    // R√©cup√©rer tous les param√®tres du modal
    const newConfig = {
        personality: document.getElementById('voice-personality').value,
        theme: document.getElementById('interface-theme').value,
        voice_speed: parseFloat(document.getElementById('voice-speed').value),
        voice_volume: parseInt(document.getElementById('voice-volume').value),
        audio_sensitivity: parseInt(document.getElementById('audio-sensitivity').value),
        llm_temperature: parseFloat(document.getElementById('llm-temperature').value),
        interface_animations: document.getElementById('interface-animations').checked
    };
    
    // Envoyer via WebSocket pour application imm√©diate
    if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({
            type: 'config_update',
            config: newConfig
        }));
        
        addLogEntry('Param√®tres envoy√©s pour application', 'info');
        closeSettings();
    } else {
        addLogEntry('Connexion WebSocket requise', 'error');
    }
}

function testMicrophone() {
    if (!isConnected) {
        addLogEntry('Connexion WebSocket requise', 'error');
        alert('Veuillez d\'abord vous connecter √† Jarvis');
        return;
    }
    
    // Informer l'utilisateur
    addLogEntry('üé§ Test du microphone: parlez maintenant...', 'info');
    
    // Cr√©er un message de test dans l'interface
    const testMessageDiv = document.createElement('div');
    testMessageDiv.className = 'message-bubble system';
    testMessageDiv.id = 'mic-test-message';
    testMessageDiv.innerHTML = `
        <div class="message-content">
            <p>üé§ Test du microphone en cours...</p>
            <p style="font-size: 0.9em; opacity: 0.8;">Parlez maintenant pour tester votre micro</p>
        </div>
    `;
    
    const container = document.getElementById('dialogue-container');
    container.appendChild(testMessageDiv);
    scrollToBottom();
    
    // Lancer l'√©coute vocale
    ws.send(JSON.stringify({
        type: 'voice_input'
    }));
    
    // Apr√®s 5 secondes, mettre √† jour le message si aucune transcription
    setTimeout(() => {
        const testMsg = document.getElementById('mic-test-message');
        if (testMsg) {
            testMsg.innerHTML = `
                <div class="message-content">
                    <p>‚úÖ Test termin√©</p>
                    <p style="font-size: 0.9em; opacity: 0.8;">Si vous voyez votre transcription ci-dessus, votre micro fonctionne !</p>
                </div>
            `;
        }
    }, 5000);
}

// Fonction testVoice corrig√©e pour vraiment tester la voix
function testVoice() {
    const personality = document.getElementById('voice-personality').value;
    const speed = document.getElementById('voice-speed').value;
    const volume = document.getElementById('voice-volume').value;
    
    addLogEntry(`üé§ Test voix: ${personality} (vitesse: ${speed}, volume: ${volume}%)`, 'info');
    
    // Envoyer un message de test via WebSocket
    if (ws && ws.readyState === WebSocket.OPEN) {
        // Message de test personnalis√© selon la personnalit√©
        let testMessages = {
            'Jarvis': "Bonjour, je suis Jarvis, votre assistant personnel. Comment puis-je vous aider aujourd'hui?",
            'Samantha': "Bonjour, je m'appelle Samantha. Je suis ravie de vous rencontrer. Comment allez-vous?",
            'Eloise': "Salut ! Je suis Eloise, ton assistante. Pr√™te √† t'aider avec enthousiasme !",
            'Josephine': "Bonjour, Josephine √† votre service. N'h√©sitez pas √† me solliciter."
        };
        
        const testMessage = testMessages[personality] || testMessages['Samantha'];
        
        // D'abord appliquer temporairement les param√®tres pour le test
        ws.send(JSON.stringify({
            type: 'config_update',
            config: {
                personality: personality,
                voice_speed: parseFloat(speed),
                voice_volume: parseInt(volume)
            }
        }));
        
        // Puis envoyer le message de test apr√®s un court d√©lai
        setTimeout(() => {
            ws.send(JSON.stringify({
                type: 'text_message',
                content: testMessage
            }));
        }, 500);
        
    } else {
        addLogEntry('‚ùå WebSocket non connect√© pour le test', 'error');
    }
}

function applySettings() {
    // Appliquer les param√®tres sans fermer la modale
    const newConfig = {
        personality: document.getElementById('voice-personality').value,
        theme: document.getElementById('interface-theme').value,
        voice_speed: parseFloat(document.getElementById('voice-speed').value),
        voice_volume: parseInt(document.getElementById('voice-volume').value),
        audio_sensitivity: parseInt(document.getElementById('audio-sensitivity').value),
        llm_temperature: parseFloat(document.getElementById('llm-temperature').value),
        interface_animations: document.getElementById('interface-animations').checked
    };
    
    if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({
            type: 'config_update',
            config: newConfig
        }));
        
        addLogEntry('‚úÖ Param√®tres appliqu√©s', 'success');
        
        // Mettre √† jour le th√®me imm√©diatement dans l'interface
        if (newConfig.theme !== currentTheme) {
            setTheme(newConfig.theme);
        }
        
    } else {
        addLogEntry('‚ùå WebSocket non connect√©', 'error');
    }
}

function resetSettings() {
    if (confirm('R√©initialiser tous les param√®tres ?')) {
        localStorage.removeItem('jarvis-settings');
        setTheme('light');
        debugVisible = false;
        updateUI();
        addLogEntry('Param√®tres r√©initialis√©s', 'info');
    }
}

// Fonctions de test
function testVoice() {
    if (!isConnected) {
        alert('Connexion requise');
        return;
    }
    
    const testText = "Ceci est un test de la voix.";
    addLogEntry('Test de la voix...', 'info');
    
    // Envoyer un message de test
    ws.send(JSON.stringify({
        type: 'text_message',
        content: testText
    }));
}

function testMicrophone() {
    addLogEntry('Test du microphone...', 'info');
    toggleVoiceInput();
}

// Fonctions utilitaires
function handleInputKeydown(event) {
    if (event.ctrlKey && event.key === 'Enter') {
        event.preventDefault();
        sendMessage();
    }
}

function setInputValue(value) {
    document.getElementById('message-input').value = value;
}

function exportConversation() {
    const messages = document.querySelectorAll('.message-bubble:not(.system)');
    let exportText = `Conversation Jarvis - ${new Date().toLocaleString()}\n\n`;
    
    messages.forEach(msg => {
        const type = msg.classList.contains('user') ? 'Vous' : 'Jarvis';
        const content = msg.querySelector('.message-content').textContent;
        const time = msg.querySelector('.message-time')?.textContent || '';
        
        exportText += `[${time}] ${type}: ${content}\n\n`;
    });
    
    // T√©l√©charger le fichier
    const blob = new Blob([exportText], { type: 'text/plain' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `conversation-jarvis-${new Date().toISOString().split('T')[0]}.txt`;
    a.click();
    URL.revokeObjectURL(url);
    
    addLogEntry('Conversation export√©e', 'success');
}

function updateUI() {
    const debugSection = document.getElementById('debug-section');
    const mainContent = document.querySelector('.main-content');
    
    if (debugVisible) {
        debugSection.classList.remove('hidden');
        mainContent.classList.remove('debug-hidden');
    } else {
        debugSection.classList.add('hidden');
        mainContent.classList.add('debug-hidden');
    }
    
    updateConnectionStatus();
    updateStatsDisplay();
}

// Keep-alive
setInterval(() => {
    if (ws && ws.readyState === WebSocket.OPEN) {
        ws.send(JSON.stringify({ type: 'ping' }));
    }
}, 30000);

==================================================
FICHIER: .\.0IAquestions\config_coordinator.py
==================================================

"""
config_coordinator.py - Coordination configuration temps r√©el (Hypothalamus) - VERSION CORRIG√âE
Responsabilit√© : Param√®tres, voix, et configuration syst√®me unifi√©e
"""

import json
import time
from pathlib import Path
from typing import Dict, Any
import sys

# Imports des modules existants hypothalamus (R√âUTILISATION)
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.voice_manager import VoiceManager
from hypothalamus.device_manager import DeviceManager
from hypothalamus.logger import log

class ConfigCoordinator:
    """Coordinateur de configuration unifi√© (Hypothalamus)"""
    
    def __init__(self):
        self.current_config = {
            'personality': 'Samantha',
            'display_name': 'Assistant virtuel - Samantha',
            'tts_model': 'edge-tts',
            'edge_voice': 'fr-FR-DeniseNeural',
            'device_index': None,
            'llm_model': 'llama3.1:8b',
            'theme': 'light',
            'voice_speed': 1.0,
            'voice_volume': 90,
            'audio_sensitivity': 5,
            'llm_temperature': 0.7,
            'interface_animations': True
        }
        
        # R√âUTILISATION des modules existants au lieu de dupliquer
        self.voice_manager = VoiceManager()
        self.device_manager = DeviceManager()
        
        # √âtat des instances actuelles (pour reload)
        self.current_conversation_flow = None
        
        log.info("ConfigCoordinator initialis√© (Hypothalamus - R√©utilise modules existants)")
    
    def get_current_config(self) -> Dict[str, Any]:
        """Retourne la configuration actuelle"""
        return {
            'success': True,
            'config': self.current_config.copy()
        }
    
    async def update_config(self, new_config: Dict[str, Any]) -> Dict[str, Any]:
        """Met √† jour la configuration (sans appliquer imm√©diatement)"""
        try:
            # Valider et fusionner les nouveaux param√®tres
            validated_config = self._validate_config(new_config)
            
            # Mettre √† jour la config interne
            self.current_config.update(validated_config)
            
            log.info(f"Configuration mise √† jour: {list(validated_config.keys())}")
            
            return {
                'success': True,
                'message': 'Configuration mise √† jour (cliquez Appliquer pour activer)',
                'config': self.current_config.copy()
            }
            
        except Exception as e:
            log.error(f"Erreur mise √† jour config: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def apply_config(self, config_changes: Dict[str, Any]) -> Dict[str, Any]:
        """Applique la configuration en temps r√©el"""
        try:
            results = []
            
            # 1. Changements de personnalit√©/voix (R√âUTILISE voice_manager)
            if 'personality' in config_changes or 'edge_voice' in config_changes:
                result = await self._apply_voice_changes(config_changes)
                results.append(result)
            
            # 2. Changements d'interface
            if 'theme' in config_changes:
                self.current_config['theme'] = config_changes['theme']
                results.append("Th√®me mis √† jour")
            
            # 3. Changements audio
            if any(key in config_changes for key in ['voice_speed', 'voice_volume', 'audio_sensitivity']):
                result = self._apply_audio_changes(config_changes)
                results.append(result)
            
            # 4. Sauvegarde persistante (R√âUTILISE modules existants)
            self._save_config()
            
            success_message = "; ".join(results)
            log.success(f"Configuration appliqu√©e: {success_message}")
            
            return {
                'success': True,
                'message': f'Param√®tres appliqu√©s: {success_message}',
                'config': self.current_config.copy()
            }
            
        except Exception as e:
            log.error(f"Erreur application config: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    async def _apply_voice_changes(self, changes: Dict[str, Any]) -> str:
        """Applique les changements de voix/personnalit√© (R√âUTILISE voice_manager)"""
        
        if 'personality' in changes:
            new_personality = changes['personality']
            
            # üîß FIX: Mapping des noms de personnalit√©s vers les cl√©s du voice_manager
            personality_to_id = {
                'Jarvis': '1',
                'Samantha': '2',
                'Eloise': '3',
                'Josephine': '4'
            }
            
            # Trouver la cl√© correspondante
            voice_id = personality_to_id.get(new_personality)
            
            if voice_id and voice_id in self.voice_manager.available_voices:
                voice_info = self.voice_manager.available_voices[voice_id]
                
                # Mettre √† jour la config
                self.current_config.update({
                    'personality': new_personality,
                    'display_name': f'Assistant virtuel - {new_personality}',
                    'tts_model': voice_info['model'],
                    'edge_voice': voice_info.get('edge_voice') or voice_info.get('voice')
                })
                
                # Sauvegarder avec le voice_manager existant
                self.voice_manager.save_voice(
                    voice_id=voice_id,
                    personality=new_personality,
                    model=voice_info['model'],
                    edge_voice=voice_info.get('edge_voice') or voice_info.get('voice')
                )
                
                # Notifier le gestionnaire de conversation du changement
                if self.current_conversation_flow:
                    await self.current_conversation_flow.reload_tts(
                        voice_info['model'], 
                        new_personality, 
                        voice_info.get('edge_voice') or voice_info.get('voice')
                    )
                
                return f"Voix chang√©e vers {new_personality}"
            else:
                raise ValueError(f"Personnalit√© inconnue: {new_personality}")
        
        return "Voix mise √† jour"
    
    def _apply_audio_changes(self, changes: Dict[str, Any]) -> str:
        """Applique les changements audio"""
        audio_changes = []
        
        if 'voice_speed' in changes:
            self.current_config['voice_speed'] = float(changes['voice_speed'])
            audio_changes.append("vitesse")
        
        if 'voice_volume' in changes:
            self.current_config['voice_volume'] = int(changes['voice_volume'])
            audio_changes.append("volume")
        
        if 'audio_sensitivity' in changes:
            self.current_config['audio_sensitivity'] = int(changes['audio_sensitivity'])
            audio_changes.append("sensibilit√©")
        
        return f"Audio mis √† jour ({', '.join(audio_changes)})"
    
    def _validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Valide une configuration avant application"""
        validated = {}
        
        # Validation personnalit√©
        if 'personality' in config:
            valid_personalities = ['Jarvis', 'Samantha', 'Eloise', 'Josephine']
            if config['personality'] in valid_personalities:
                validated['personality'] = config['personality']
        
        # Validation th√®me
        if 'theme' in config:
            valid_themes = ['light', 'dark', 'jarvis']
            if config['theme'] in valid_themes:
                validated['theme'] = config['theme']
        
        # Validation ranges num√©riques
        if 'voice_speed' in config:
            speed = float(config['voice_speed'])
            validated['voice_speed'] = max(0.5, min(2.0, speed))
        
        if 'voice_volume' in config:
            volume = int(config['voice_volume'])
            validated['voice_volume'] = max(0, min(100, volume))
        
        if 'audio_sensitivity' in config:
            sensitivity = int(config['audio_sensitivity'])
            validated['audio_sensitivity'] = max(1, min(10, sensitivity))
        
        if 'llm_temperature' in config:
            temp = float(config['llm_temperature'])
            validated['llm_temperature'] = max(0.1, min(1.0, temp))
        
        # Validation boolean
        if 'interface_animations' in config:
            validated['interface_animations'] = bool(config['interface_animations'])
        
        return validated
    
    def _save_config(self):
        """Sauvegarde la configuration de mani√®re persistante (R√âUTILISE modules)"""
        try:
            # Sauvegarder la voix avec le voice_manager existant
            if self.current_config.get('personality'):
                # üîß FIX: Utiliser le bon voice_id
                personality_to_id = {
                    'Jarvis': '1',
                    'Samantha': '2',
                    'Eloise': '3',
                    'Josephine': '4'
                }
                voice_id = personality_to_id.get(self.current_config['personality'], '2')
                
                self.voice_manager.save_voice(
                    voice_id=voice_id,
                    personality=self.current_config['personality'],
                    model=self.current_config['tts_model'],
                    edge_voice=self.current_config.get('edge_voice')
                )
            
            log.debug("Configuration sauvegard√©e (modules hypothalamus)")
            
        except Exception as e:
            log.error(f"Erreur sauvegarde config: {e}")
    
    def get_available_voices(self) -> Dict[str, Any]:
        """Retourne les voix disponibles (R√âUTILISE voice_manager)"""
        return {
            'success': True,
            'voices': self.voice_manager.available_voices
        }
    
    def get_available_devices(self) -> Dict[str, Any]:
        """Retourne les p√©riph√©riques audio disponibles (R√âUTILISE device_manager)"""
        try:
            # Utiliser le device manager existant
            import pyaudio
            p = pyaudio.PyAudio()
            
            devices = []
            for i in range(p.get_device_count()):
                try:
                    info = p.get_device_info_by_index(i)
                    if info['maxInputChannels'] > 0:
                        devices.append({
                            'index': i,
                            'name': info['name'],
                            'channels': info['maxInputChannels']
                        })
                except:
                    continue
            
            p.terminate()
            
            return {
                'success': True,
                'devices': devices
            }
            
        except Exception as e:
            log.error(f"Erreur r√©cup√©ration devices: {e}")
            return {
                'success': False,
                'error': str(e),
                'devices': []
            }
    
    def set_conversation_flow(self, flow):
        """D√©finit le gestionnaire de conversation pour reload TTS"""
        self.current_conversation_flow = flow
    
    def get_display_name(self) -> str:
        """Retourne le nom d'affichage format√©"""
        return self.current_config.get('display_name', 'Assistant virtuel')

==================================================
FICHIER: .\.0IAquestions\conversation_flow.py
==================================================

"""
conversation_flow.py - Flux de conversation unifi√© (Lobes Temporaux)
Responsabilit√© : Messages, streaming LLM, STT/TTS
Migr√© depuis web_modules/conversation_handler.py et adapt√© pour lobes_temporaux
"""

import time
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Callable, Optional
import sys

# Imports des modules - R√©utilisation modules existants
sys.path.append(str(Path(__file__).parent.parent))
from cortex_prefrontal.llm_client import JarvisLLM  # LLM unifi√© avec streaming
from lobes_temporaux.stt import SpeechToText  # Module local
from lobes_temporaux.tts import TextToSpeech  # Module local
from hypothalamus.device_manager import DeviceManager
from hypothalamus.voice_manager import VoiceManager
from hypothalamus.logger import log

class ConversationFlow:
    """Flux de conversation unifi√© avec vrai streaming (Lobes Temporaux)"""
    
    def __init__(self):
        # Instances des modules (R√âUTILISATION maximale)
        self.llm = None  # LLM unifi√©
        self.stt = None  # Module local lobes_temporaux
        self.tts = None  # Module local lobes_temporaux
        
        # Configuration actuelle
        self.personality = None
        self.is_initialized = False
        
        # Historique de conversation
        self.conversation_history = []
        
        # Callback WebSocket
        self.websocket_callback: Optional[Callable] = None
        
        # Stats de session
        self.session_stats = {
            'messages_count': 0,
            'total_tokens': 0,
            'total_time': 0.0,
            'avg_response_time': 0.0,
            'avg_ttft': 0.0
        }
        
        log.info("ConversationFlow cr√©√© (Lobes Temporaux - R√©utilise modules)")
    
    async def auto_initialize(self) -> bool:
        """Initialisation automatique sans interaction utilisateur"""
        try:
            log.info("Initialisation automatique ConversationFlow...")
            
            # 1. Configuration microphone automatique (R√âUTILISE device_manager)
            device_mgr = DeviceManager()
            saved_index, saved_name = device_mgr.load_saved_device()
            
            if saved_index is not None:
                exists, _ = device_mgr.verify_device(saved_index)
                device_index = saved_index if exists else None
            else:
                device_index = self._find_first_microphone()
            
            if device_index is None:
                log.error("Aucun microphone disponible")
                return False
            
            # 2. Configuration voix automatique (R√âUTILISE voice_manager)
            voice_mgr = VoiceManager()
            saved_id, saved_personality, saved_model, edge_voice = voice_mgr.load_saved_voice()
            
            if saved_personality and saved_model:
                personality, tts_model = saved_personality, saved_model
            else:
                # Configuration par d√©faut
                personality = "Samantha"
                tts_model = "edge-tts"
                edge_voice = "fr-FR-DeniseNeural"
            
            # 3. Initialiser les modules (R√âUTILISATION COMPL√àTE)
            self.llm = JarvisLLM(personality=personality)  # Cortex pr√©frontal
            self.stt = SpeechToText(device_index=device_index)  # Module local
            self.tts = TextToSpeech(  # Module local
                model_name=tts_model, 
                personality=personality, 
                edge_voice=edge_voice
            )
            
            self.personality = personality
            self.is_initialized = True
            
            log.success(f"ConversationFlow initialis√© - {personality} pr√™t (streaming actif)")
            return True
            
        except Exception as e:
            log.error(f"Erreur initialisation ConversationFlow: {e}")
            return False
    
    def _find_first_microphone(self) -> Optional[int]:
        """Trouve le premier microphone disponible"""
        try:
            import pyaudio
            p = pyaudio.PyAudio()
            
            for i in range(p.get_device_count()):
                try:
                    info = p.get_device_info_by_index(i)
                    if info['maxInputChannels'] > 0:
                        p.terminate()
                        log.info(f"Microphone trouv√©: {info['name']}")
                        return i
                except:
                    continue
            
            p.terminate()
            return None
            
        except Exception as e:
            log.error(f"Erreur recherche microphone: {e}")
            return None
    
    async def process_text_message(self, message: str):
        """Traite un message texte utilisateur avec VRAI streaming"""
        if not self.is_initialized:
            await self._send_error("Syst√®me non initialis√©")
            return
        
        try:
            log.info(f"Message texte re√ßu: {message[:50]}...")
            
            # Ajouter √† l'historique
            self._add_to_history('user', message)
            
            # Notifier le d√©but de traitement
            await self._send_event('message_processing_start', message)
            
            # üî• VRAI STREAMING depuis Ollama (LLM unifi√©)
            session_start = time.time()
            full_response = ""
            token_count = 0
            first_token_time = None
            
            # Stream les tokens un par un
            for token in self.llm.generate_response_stream(message):
                # Premier token - mesurer TTFT
                if first_token_time is None:
                    first_token_time = time.time() - session_start
                    await self._send_event('first_token', token, {
                        'ttft': first_token_time
                    })
                
                # Envoyer chaque token √† l'interface
                await self._send_event('llm_token', token)
                
                # Construire la r√©ponse compl√®te
                full_response += token
                token_count += 1
                
                # Petite pause pour √©viter l'inondation WebSocket
                if token_count % 10 == 0:
                    await asyncio.sleep(0.001)
            
            # Fin du streaming
            total_time = time.time() - session_start
            tokens_per_second = token_count / max(total_time, 0.001)
            
            await self._send_event('llm_complete', full_response, {
                'total_time': total_time,
                'token_count': token_count,
                'ttft': first_token_time or 0,
                'tokens_per_second': tokens_per_second
            })
            
            # Ajouter la r√©ponse compl√®te √† l'historique
            self._add_to_history('assistant', full_response, token_count)
            
            # Mettre √† jour les stats
            self._update_session_stats({
                'total_time': total_time,
                'token_count': token_count,
                'ttft': first_token_time or 0
            })
            
            # TTS de la r√©ponse compl√®te (en parall√®le, module local)
            if self.tts and full_response.strip():
                asyncio.create_task(self._speak_response(full_response))
            
            log.success(f"Message trait√©: {token_count} tokens en {total_time:.2f}s ({tokens_per_second:.1f} tok/s)")
            
        except Exception as e:
            log.error(f"Erreur traitement message: {e}")
            await self._send_error(f"Erreur traitement: {str(e)}")
    
    async def _speak_response(self, text: str):
        """Fait parler le TTS en arri√®re-plan (module local)"""
        try:
            await asyncio.get_event_loop().run_in_executor(
                None, 
                self.tts.speak, 
                text
            )
        except Exception as e:
            log.error(f"Erreur TTS: {e}")
    
    async def process_voice_input(self):
        """Traite l'entr√©e vocale (module STT local)"""
        if not self.is_initialized or not self.stt:
            await self._send_error("STT non disponible")
            return
        
        try:
            log.info("D√©but de l'√©coute vocale")
            
            # Signal de d√©but d'√©coute
            await self._send_event('listening_start', '')
            
            # √âcouter (bloquant, mais dans un thread s√©par√©, module local)
            loop = asyncio.get_event_loop()
            transcription = await loop.run_in_executor(
                None, 
                self.stt.listen_with_vad, 
                30,  # timeout
                1.5  # silence_duration
            )
            
            # Signal de fin d'√©coute
            await self._send_event('listening_end', '')
            
            if transcription and transcription.strip():
                log.success(f"Transcription: '{transcription}'")
                
                # Envoyer la transcription
                await self._send_event('transcription', transcription)
                
                # Traiter comme un message normal
                await self.process_text_message(transcription)
            else:
                log.info("Aucune voix d√©tect√©e (silence)")
                # Pas d'erreur visible, juste un log
            
        except Exception as e:
            log.error(f"Erreur STT: {e}")
            await self._send_event('listening_end', '')
            await self._send_error(f"Erreur microphone: {str(e)}")
    
    async def reload_tts(self, tts_model: str, personality: str, edge_voice: str = None):
        """Recharge le TTS avec une nouvelle configuration (R√âUTILISE modules)"""
        try:
            log.info(f"Rechargement TTS: {personality} ({tts_model})")
            
            # Cr√©er nouveau TTS (module local)
            self.tts = TextToSpeech(
                model_name=tts_model,
                personality=personality,
                edge_voice=edge_voice
            )
            
            # Recharger aussi le LLM avec nouvelle personnalit√© (cortex_prefrontal)
            self.llm = JarvisLLM(personality=personality)
            
            # Mettre √† jour la personnalit√©
            self.personality = personality
            
            log.success(f"TTS et LLM recharg√©s: {personality}")
            
        except Exception as e:
            log.error(f"Erreur rechargement TTS: {e}")
            raise
    
    def get_personality(self) -> str:
        """Retourne la personnalit√© actuelle"""
        return self.personality or "Samantha"
    
    def get_history(self) -> Dict[str, Any]:
        """Retourne l'historique de conversation"""
        return {
            'success': True,
            'history': self.conversation_history,
            'stats': self.session_stats
        }
    
    def clear_history(self) -> Dict[str, Any]:
        """Efface l'historique de conversation"""
        self.conversation_history.clear()
        self.session_stats = {
            'messages_count': 0,
            'total_tokens': 0,
            'total_time': 0.0,
            'avg_response_time': 0.0,
            'avg_ttft': 0.0
        }
        
        log.info("Historique de conversation effac√©")
        return {'success': True}
    
    def set_websocket_callback(self, callback: Callable):
        """D√©finit le callback WebSocket pour les √©v√©nements"""
        self.websocket_callback = callback
    
    def _add_to_history(self, sender: str, content: str, token_count: int = 0):
        """Ajoute un message √† l'historique"""
        entry = {
            'sender': sender,
            'content': content,
            'timestamp': time.time(),
            'token_count': token_count
        }
        
        self.conversation_history.append(entry)
        
        # Limiter l'historique (garder les 100 derniers messages)
        if len(self.conversation_history) > 100:
            self.conversation_history = self.conversation_history[-100:]
    
    def _update_session_stats(self, result: Dict[str, Any]):
        """Met √† jour les statistiques de session"""
        self.session_stats['messages_count'] += 1
        self.session_stats['total_tokens'] += result.get('token_count', 0)
        self.session_stats['total_time'] += result.get('total_time', 0)
        
        # TTFT moyen
        if 'ttft' in result:
            current_ttft = self.session_stats.get('avg_ttft', 0)
            count = self.session_stats['messages_count']
            self.session_stats['avg_ttft'] = (current_ttft * (count - 1) + result['ttft']) / count
        
        # Temps de r√©ponse moyen
        if self.session_stats['messages_count'] > 0:
            self.session_stats['avg_response_time'] = (
                self.session_stats['total_time'] / self.session_stats['messages_count']
            )
    
    async def _send_event(self, event_type: str, content: str, metadata: Dict = None):
        """Envoie un √©v√©nement via WebSocket"""
        if self.websocket_callback:
            event_data = {
                'type': event_type,
                'content': content,
                'timestamp': time.time(),
                'metadata': metadata or {}
            }
            await self.websocket_callback(event_data)
        else:
            log.warning(f"‚ö†Ô∏è WebSocket callback non d√©fini pour: {event_type}")
    
    async def _send_error(self, error_message: str):
        """Envoie une erreur via WebSocket"""
        await self._send_event('error', error_message)
    
    async def initialize(self):
        """Point d'entr√©e d'initialisation"""
        # L'initialisation r√©elle se fait dans auto_initialize()
        pass
    
    def stop(self):
        """Arr√™te proprement le gestionnaire"""
        log.info("ConversationFlow arr√™t√©")

==================================================
FICHIER: .\.0IAquestions\device_manager.py
==================================================

"""
Device Manager - Gestion des p√©riph√©riques audio
D√©tecte et sauvegarde le meilleur microphone
"""

import pyaudio
import wave
import json
from pathlib import Path
import audioop

class DeviceManager:
    def __init__(self):
        self.config_file = Path("config/audio_device.json")
        self.config_file.parent.mkdir(exist_ok=True)
    
    def load_saved_device(self):
        """Charge le device sauvegard√©"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get('device_index'), config.get('device_name')
        return None, None
    
    def save_device(self, device_index, device_name):
        """Sauvegarde le device choisi"""
        config = {
            'device_index': device_index,
            'device_name': device_name
        }
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Micro sauvegard√© : {device_name} (index {device_index})")
    
    def verify_device(self, device_index):
        """V√©rifie qu'un device existe toujours"""
        try:
            p = pyaudio.PyAudio()
            info = p.get_device_info_by_index(device_index)
            p.terminate()
            
            if info['maxInputChannels'] > 0:
                return True, info['name']
            return False, None
        except:
            return False, None
    
    def test_device(self, device_index, device_name):
        """
        Teste un device en enregistrant 3 secondes
        Retourne (success, volume_max)
        """
        CHUNK = 1024
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        RATE = 16000
        DURATION = 3
        
        print(f"\nüé§ Test : {device_name}")
        print(f"   Parle FORT pendant 3 secondes...")
        
        p = pyaudio.PyAudio()
        
        try:
            stream = p.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                input_device_index=device_index,
                frames_per_buffer=CHUNK
            )
            
            frames = []
            for i in range(0, int(RATE / CHUNK * DURATION)):
                data = stream.read(CHUNK, exception_on_overflow=False)
                frames.append(data)
            
            stream.stop_stream()
            stream.close()
            
            # Analyser le volume
            volumes = [audioop.rms(frame, 2) for frame in frames]
            volume_max = max(volumes)
            volume_avg = sum(volumes) / len(volumes)
            
            print(f"   üìä Volume max: {volume_max}, moyen: {volume_avg:.0f}")
            
            # Consid√©rer comme fonctionnel si volume > 100
            if volume_max > 100:
                print(f"   ‚úÖ FONCTIONNE (volume suffisant)")
                return True, volume_max
            else:
                print(f"   ‚ö†Ô∏è  Trop silencieux")
                return False, volume_max
                
        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
            return False, 0
        finally:
            p.terminate()
    
    def find_best_microphone(self):
        """
        Scanne tous les devices et trouve le meilleur micro
        Retourne (device_index, device_name)
        """
        print("\n" + "="*60)
        print("üîç Recherche du meilleur microphone...")
        print("="*60)
        
        p = pyaudio.PyAudio()
        
        # Lister tous les devices d'entr√©e
        input_devices = []
        for i in range(p.get_device_count()):
            try:
                info = p.get_device_info_by_index(i)
                if info['maxInputChannels'] > 0:
                    input_devices.append((i, info['name']))
            except:
                pass
        
        p.terminate()
        
        if not input_devices:
            print("‚ùå Aucun microphone d√©tect√© !")
            return None, None
        
        print(f"\nüìã {len(input_devices)} microphone(s) d√©tect√©(s)")
        
        # Tester chaque device
        working_devices = []
        for device_index, device_name in input_devices:
            success, volume = self.test_device(device_index, device_name)
            if success:
                working_devices.append((device_index, device_name, volume))
        
        if not working_devices:
            print("\n‚ùå Aucun microphone ne fonctionne correctement !")
            print("V√©rifiez :")
            print("  - Que le micro est branch√©")
            print("  - Les permissions Windows")
            print("  - Que vous parlez assez fort")
            return None, None
        
        # Si un seul fonctionne, le choisir automatiquement
        if len(working_devices) == 1:
            device_index, device_name, volume = working_devices[0]
            print(f"\n‚úÖ Micro s√©lectionn√© automatiquement : {device_name}")
            return device_index, device_name
        
        # Si plusieurs fonctionnent, demander √† l'utilisateur
        print(f"\n‚úÖ {len(working_devices)} microphone(s) fonctionnel(s) :")
        for i, (idx, name, volume) in enumerate(working_devices, 1):
            print(f"   {i}. {name} (volume: {volume})")
        
        while True:
            try:
                choice = input(f"\nChoisis un micro (1-{len(working_devices)}) : ").strip()
                choice_idx = int(choice) - 1
                if 0 <= choice_idx < len(working_devices):
                    device_index, device_name, _ = working_devices[choice_idx]
                    return device_index, device_name
                else:
                    print(f"‚ùå Choix invalide (1-{len(working_devices)})")
            except (ValueError, KeyboardInterrupt):
                print("\n‚ùå Annul√©")
                return None, None
    
    def setup_microphone(self):
        """
        Configuration compl√®te du microphone
        Retourne device_index ou None
        """
        print("\nüéôÔ∏è  CONFIGURATION MICROPHONE")
        
        # V√©rifier si un device est d√©j√† sauvegard√©
        saved_index, saved_name = self.load_saved_device()
        
        if saved_index is not None:
            print(f"\nüìÅ Micro sauvegard√© : {saved_name} (index {saved_index})")
            
            # V√©rifier qu'il existe toujours
            exists, current_name = self.verify_device(saved_index)
            
            if exists:
                print(f"‚úÖ Le micro est toujours disponible")
                
                # Demander si on veut le garder ou refaire le test
                choice = input("Utiliser ce micro ? (O/n) : ").strip().lower()
                if choice in ['', 'o', 'oui', 'y', 'yes']:
                    return saved_index
            else:
                print(f"‚ö†Ô∏è  Le micro n'est plus disponible !")
        
        # Rechercher un nouveau micro
        device_index, device_name = self.find_best_microphone()
        
        if device_index is not None:
            self.save_device(device_index, device_name)
            return device_index
        
        return None

==================================================
FICHIER: .\.0IAquestions\index.html
==================================================

<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Jarvis - Assistant Vocal</title>
    <link rel="stylesheet" href="static/styles.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ü§ñ</text></svg>">
</head>
<body class="theme-light">
    <!-- Header avec navigation -->
    <header class="header">
        <div class="header-left">
            <div class="logo">
                <span class="logo-icon">ü§ñ</span>
                <span class="logo-text" id="assistant-name">Jarvis</span>
                <span class="status-indicator" id="status-indicator">‚óè</span>
            </div>
        </div>
        
        <nav class="header-nav">
            <button class="nav-btn" onclick="openSettings()">
                <span class="icon">‚öôÔ∏è</span>
                Param√®tres
            </button>
            <button class="nav-btn" onclick="toggleDebug()">
                <span class="icon">üîç</span>
                Debug
            </button>
            <button class="nav-btn" onclick="toggleTheme()">
                <span class="icon" id="theme-icon">üåô</span>
                <span id="theme-text">Gray - blue</span>
            </button>
            <button class="nav-btn" onclick="showHelp()">
                <span class="icon">‚ùì</span>
                Aide
            </button>
        </nav>
    </header>

    <!-- Contenu principal -->
    <main class="main-content">
        <!-- Zone dialogue (gauche haut) -->
        <section class="dialogue-section">
            <div class="dialogue-header">
                <h2>üí¨ Conversation</h2>
                <div class="dialogue-controls">
                    <button class="control-btn" onclick="clearConversation()" title="Effacer la conversation">
                        üóëÔ∏è
                    </button>
                    <button class="control-btn" onclick="exportConversation()" title="Exporter">
                        üíæ
                    </button>
                </div>
            </div>
            
            <div class="dialogue-container" id="dialogue-container">
                <div class="welcome-message">
                    <div class="message-bubble system">
                        <div class="message-content">
                            <p>üöÄ Connexion √† votre assistant virtuel en cours...</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Zone saisie (gauche bas) -->
        <section class="input-section">
            <div class="input-container">
                <div class="input-row">
                    <textarea 
                        id="message-input" 
                        placeholder="Tapez votre message ici ou utilisez le microphone..."
                        rows="3"
                        onkeydown="handleInputKeydown(event)"
                    ></textarea>
                    
                    <div class="input-buttons">
                        <button 
                            class="mic-button" 
                            id="mic-button"
                            onclick="toggleVoiceInput()"
                            title="Activer/D√©sactiver le microphone"
                        >
                            <span class="mic-icon">üé§</span>
                            <span class="mic-status">Parler</span>
                        </button>
                        
                        <button 
                            class="send-button"
                            onclick="sendMessage()"
                            title="Envoyer le message"
                        >
                            ‚û§
                        </button>
                    </div>
                </div>
            </div>
        </section>

        <!-- Zone debug/logs (droite) -->
        <aside class="debug-section" id="debug-section">
            <div class="debug-header">
                <h3>üîç Debug & Logs</h3>
                <button class="close-debug" onclick="toggleDebug()">‚úï</button>
            </div>
            
            <div class="debug-tabs">
                <button class="tab-btn active" onclick="switchDebugTab('logs')">Logs</button>
                <button class="tab-btn" onclick="switchDebugTab('stats')">Stats</button>
                <button class="tab-btn" onclick="switchDebugTab('config')">Config</button>
            </div>
            
            <div class="debug-content">
                <!-- Onglet Logs -->
                <div id="debug-logs" class="debug-tab active">
                    <div class="log-container" id="log-container">
                        <div class="log-entry info">
                            <span class="log-time">--:--:--</span>
                            <span class="log-message">Interface web initialis√©e</span>
                        </div>
                    </div>
                </div>
                
                <!-- Onglet Stats -->
                <div id="debug-stats" class="debug-tab">
                    <div class="stats-grid">
                        <div class="stat-card">
                            <div class="stat-value" id="stat-messages">0</div>
                            <div class="stat-label">Messages</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-value" id="stat-tokens">0</div>
                            <div class="stat-label">Tokens</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-value" id="stat-avgtime">0.0s</div>
                            <div class="stat-label">Temps Moyen</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-value" id="stat-tts-efficiency">100%</div>
                            <div class="stat-label">Efficacit√© TTS</div>
                        </div>
                    </div>
                </div>
                
                <!-- Onglet Config -->
                <div id="debug-config" class="debug-tab">
                    <div class="config-item">
                        <label>Personnalit√©:</label>
                        <span id="config-personality">-</span>
                    </div>
                    <div class="config-item">
                        <label>Mod√®le LLM:</label>
                        <span id="config-llm">-</span>
                    </div>
                    <div class="config-item">
                        <label>Mod√®le TTS:</label>
                        <span id="config-tts">-</span>
                    </div>
                    <div class="config-item">
                        <label>P√©riph√©rique Audio:</label>
                        <span id="config-audio">-</span>
                    </div>
                </div>
            </div>
        </aside>
    </main>

    <!-- Modal Param√®tres -->
    <div class="modal" id="settings-modal">
        <div class="modal-content">
            <div class="modal-header">
                <h2>‚öôÔ∏è Param√®tres</h2>
                <button class="modal-close" onclick="closeSettings()">‚úï</button>
            </div>
            
            <div class="modal-body">
                <div class="settings-tabs">
                    <button class="tab-btn active" onclick="switchSettingsTab('voice')">üîä Voix</button>
                    <button class="tab-btn" onclick="switchSettingsTab('audio')">üé§ Audio</button>
                    <button class="tab-btn" onclick="switchSettingsTab('llm')">üß† LLM</button>
                    <button class="tab-btn" onclick="switchSettingsTab('interface')">üé® Interface</button>
                </div>
                
                <div class="settings-content">
                    <!-- Param√®tres Voix -->
                    <div id="settings-voice" class="settings-tab active">
                        <div class="setting-group">
                            <label for="voice-personality">Personnalit√©:</label>
                            <select id="voice-personality">
                                <option value="Jarvis">Jarvis (Masculin)</option>
                                <option value="Samantha">Samantha (F√©minin)</option>
                                <option value="Eloise">Eloise (Jeune)</option>
                            </select>
                        </div>
                        
                        <div class="setting-group">
                            <label for="voice-speed">Vitesse de parole:</label>
                            <input type="range" id="voice-speed" min="0.5" max="2.0" step="0.1" value="1.0">
                            <span id="voice-speed-value">1.0x</span>
                        </div>
                        
                        <div class="setting-group">
                            <label for="voice-volume">Volume:</label>
                            <input type="range" id="voice-volume" min="0" max="100" value="90">
                            <span id="voice-volume-value">90%</span>
                        </div>
                        
                        <button class="test-btn" onclick="testVoice()">üîä Tester la voix</button>
                    </div>
                    
                    <!-- Param√®tres Audio -->
                    <div id="settings-audio" class="settings-tab">
                        <div class="setting-group">
                            <label for="audio-device">Microphone:</label>
                            <select id="audio-device">
                                <option value="default">P√©riph√©rique par d√©faut</option>
                            </select>
                        </div>
                        
                        <div class="setting-group">
                            <label for="audio-sensitivity">Sensibilit√©:</label>
                            <input type="range" id="audio-sensitivity" min="1" max="10" value="5">
                            <span id="audio-sensitivity-value">5</span>
                        </div>
                        
                        <button class="test-btn" onclick="testMicrophone()">üé§ Tester le micro</button>
                    </div>
                    
                    <!-- Param√®tres LLM -->
                    <div id="settings-llm" class="settings-tab">
                        <div class="setting-group">
                            <label for="llm-model">Mod√®le:</label>
                            <select id="llm-model">
                                <option value="llama3.1:8b">Llama 3.1 8B (Recommand√©)</option>
                                <option value="llama3.2:3b">Llama 3.2 3B (Rapide)</option>
                                <option value="mistral:7b">Mistral 7B</option>
                            </select>
                        </div>
                        
                        <div class="setting-group">
                            <label for="llm-temperature">Cr√©ativit√© (Temperature):</label>
                            <input type="range" id="llm-temperature" min="0.1" max="1.0" step="0.1" value="0.7">
                            <span id="llm-temperature-value">0.7</span>
                        </div>
                    </div>
                    
                    <!-- Param√®tres Interface -->
                    <div id="settings-interface" class="settings-tab">
                        <div class="setting-group">
                            <label for="interface-theme">Th√®me:</label>
                            <select id="interface-theme">
                                <option value="light">Light</option>
                                <option value="dark">Gray - blue</option>
                                <option value="jarvis">Dark - green</option>
                            </select>
                        </div>
                        
                        <div class="setting-group">
                            <label for="interface-background">Arri√®re-plan:</label>
                            <select id="interface-background">
                                <option value="default">Par d√©faut</option>
                                <option value="jarvis-lab">Laboratoire Jarvis</option>
                                <option value="space">Espace</option>
                                <option value="matrix">Matrix</option>
                            </select>
                        </div>
                        
                        <div class="setting-group">
                            <input type="checkbox" id="interface-animations">
                            <label for="interface-animations">Animations fluides</label>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="modal-footer">
                <button class="btn-secondary" onclick="resetSettings()">R√©initialiser</button>
                <button class="btn-primary" onclick="saveSettings()">Sauvegarder</button>
            </div>
        </div>
    </div>

    <!-- Modal Aide -->
    <div class="modal" id="help-modal">
        <div class="modal-content">
            <div class="modal-header">
                <h2>‚ùì Aide & Guide d'utilisation</h2>
                <button class="modal-close" onclick="closeHelp()">‚úï</button>
            </div>
            
            <div class="modal-body">
                <div class="help-section">
                    <h3>üé§ Utilisation du microphone</h3>
                    <p>Cliquez sur le bouton microphone pour commencer l'√©coute vocale. Parlez clairement et attendez la transcription.</p>
                </div>
                
                <div class="help-section">
                    <h3>üí¨ Zone de conversation</h3>
                    <p>Tapez vos messages dans la zone de saisie ou utilisez Ctrl+Entr√©e pour envoyer rapidement.</p>
                </div>
                
                <div class="help-section">
                    <h3>üîç Debug et logs</h3>
                    <p>Utilisez la zone debug (droite) pour surveiller les performances et diagnostiquer les probl√®mes.</p>
                </div>
                
                <div class="help-section">
                    <h3>‚öôÔ∏è Param√®tres</h3>
                    <p>Personnalisez la voix, l'audio et l'interface selon vos pr√©f√©rences.</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Scripts -->
    <script src="static/app.js"></script>
</body>
</html>

==================================================
FICHIER: .\.0IAquestions\interface_bridge.py
==================================================

"""
interface_bridge.py - Pont entre interface web et modules Jarvis (Thalamus)
Responsabilit√© : Interface simplifi√©e pour l'acc√®s aux modules
Remplace jarvis_backend.py avec logique thalamique
"""

from pathlib import Path
import sys

# Ajouter le chemin vers les modules Jarvis
sys.path.append(str(Path(__file__).parent.parent))

# Imports des modules existants (r√©utilisation)
from cortex_prefrontal.llm_client import JarvisLLM
from lobes_temporaux.stt import SpeechToText
from lobes_temporaux.tts import TextToSpeech
from hypothalamus.device_manager import DeviceManager
from hypothalamus.voice_manager import VoiceManager
from hypothalamus.logger import log

# Constantes et configurations
DEFAULT_PERSONALITIES = {
    'Jarvis': {
        'display_name': 'Assistant virtuel - Jarvis',
        'tts_model': 'edge-tts',
        'edge_voice': 'fr-FR-HenriNeural',
        'description': 'Assistant masculin, style professionnel'
    },
    'Samantha': {
        'display_name': 'Assistant virtuel - Samantha', 
        'tts_model': 'edge-tts',
        'edge_voice': 'fr-FR-DeniseNeural',
        'description': 'Assistante f√©minine, style chaleureux'
    },
    'Eloise': {
        'display_name': 'Assistant virtuel - Eloise',
        'tts_model': 'edge-tts',
        'edge_voice': 'fr-FR-EloiseNeural',
        'description': 'Assistante jeune et dynamique'
    },
    'Josephine': {
        'display_name': 'Assistant virtuel - Josephine',
        'tts_model': 'edge-tts', 
        'edge_voice': 'fr-FR-JosephineNeural',
        'description': 'Assistante professionnelle et moderne'
    }
}

class InterfaceBridge:
    """Pont simplifi√© entre interface web et modules Jarvis (Thalamus)"""
    
    def __init__(self):
        log.info("Interface Bridge initialis√© (Thalamus)")
    
    @staticmethod
    def get_system_info():
        """Retourne les informations syst√®me pour debug"""
        try:
            import psutil
            import platform
            
            return {
                'platform': platform.system(),
                'python_version': platform.python_version(),
                'cpu_count': psutil.cpu_count(),
                'memory_total': psutil.virtual_memory().total // (1024**3),  # GB
                'memory_available': psutil.virtual_memory().available // (1024**3)  # GB
            }
        except ImportError:
            return {
                'platform': 'Unknown',
                'python_version': 'Unknown',
                'note': 'psutil non install√© - infos limit√©es'
            }

    @staticmethod
    def validate_ollama_connection():
        """V√©rifie la connexion √† Ollama"""
        try:
            import ollama
            
            # Test simple de connexion
            models = ollama.list()
            return {
                'success': True,
                'models_count': len(models.get('models', [])),
                'models': [m['name'] for m in models.get('models', [])]
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'suggestion': 'V√©rifiez qu\'Ollama est d√©marr√©'
            }

    @staticmethod
    def get_available_microphones():
        """Retourne la liste des microphones disponibles (via DeviceManager)"""
        try:
            device_manager = DeviceManager()
            
            import pyaudio
            p = pyaudio.PyAudio()
            devices = []
            
            for i in range(p.get_device_count()):
                try:
                    info = p.get_device_info_by_index(i)
                    if info['maxInputChannels'] > 0:
                        devices.append({
                            'index': i,
                            'name': info['name'],
                            'channels': info['maxInputChannels'],
                            'sample_rate': int(info['defaultSampleRate'])
                        })
                except:
                    continue
            
            p.terminate()
            return devices
            
        except Exception as e:
            log.error(f"Erreur √©num√©ration microphones: {e}")
            return []

    @staticmethod
    def create_jarvis_instance(personality: str = "Samantha"):
        """Cr√©e une instance Jarvis compl√®te (r√©utilise modules existants)"""
        try:
            # Validation de la personnalit√©
            if personality not in DEFAULT_PERSONALITIES:
                personality = "Samantha"
            
            config = DEFAULT_PERSONALITIES[personality]
            
            # Cr√©er les instances en r√©utilisant les modules existants
            llm = JarvisLLM(personality=personality)
            
            # Pour le TTS, utiliser la configuration de la personnalit√©
            tts = TextToSpeech(
                model_name=config['tts_model'],
                personality=personality,
                edge_voice=config['edge_voice']
            )
            
            log.success(f"Instance Jarvis cr√©√©e: {config['display_name']}")
            
            return {
                'llm': llm,
                'tts': tts,
                'config': config,
                'personality': personality
            }
            
        except Exception as e:
            log.error(f"Erreur cr√©ation instance Jarvis: {e}")
            raise

    @staticmethod
    def test_audio_pipeline(device_index: int = None):
        """Teste la pipeline audio compl√®te (r√©utilise modules existants)"""
        try:
            # Test STT avec module existant
            stt = SpeechToText(device_index=device_index)
            
            # Test TTS avec voix par d√©faut
            tts = TextToSpeech(
                model_name="edge-tts",
                personality="Samantha", 
                edge_voice="fr-FR-DeniseNeural"
            )
            
            return {
                'stt_ready': True,
                'tts_ready': True,
                'device_index': device_index
            }
            
        except Exception as e:
            return {
                'stt_ready': False,
                'tts_ready': False,
                'error': str(e)
            }

    @staticmethod
    def format_display_name(personality: str) -> str:
        """Formate le nom d'affichage selon la spec"""
        if personality in DEFAULT_PERSONALITIES:
            return DEFAULT_PERSONALITIES[personality]['display_name']
        else:
            return f"Assistant virtuel - {personality}"

    @staticmethod
    def get_personality_config(personality: str) -> dict:
        """Retourne la configuration d'une personnalit√©"""
        return DEFAULT_PERSONALITIES.get(personality, DEFAULT_PERSONALITIES['Samantha'])

    @staticmethod
    def log_system_startup():
        """Log les informations de d√©marrage syst√®me"""
        system_info = InterfaceBridge.get_system_info()
        ollama_status = InterfaceBridge.validate_ollama_connection()
        
        log.info("=== D√âMARRAGE JARVIS THALAMUS ===")
        log.info(f"Syst√®me: {system_info.get('platform', 'Unknown')}")
        log.info(f"Python: {system_info.get('python_version', 'Unknown')}")
        log.info(f"RAM: {system_info.get('memory_available', '?')}GB disponible")
        
        if ollama_status['success']:
            log.success(f"Ollama connect√© ({ollama_status['models_count']} mod√®les)")
        else:
            log.warning(f"Ollama: {ollama_status['error']}")
        
        microphones = InterfaceBridge.get_available_microphones()
        log.info(f"Microphones d√©tect√©s: {len(microphones)}")
        
        log.info("=== THALAMUS INITIALIS√â ===")

# Test standalone
if __name__ == "__main__":
    print("üß™ Test Interface Bridge (Thalamus)")
    
    bridge = InterfaceBridge()
    bridge.log_system_startup()
    
    try:
        instance = bridge.create_jarvis_instance("Samantha")
        print(f"‚úÖ Instance cr√©√©e: {instance['config']['display_name']}")
        
        audio_test = bridge.test_audio_pipeline()
        if audio_test['stt_ready'] and audio_test['tts_ready']:
            print("‚úÖ Pipeline audio fonctionnelle")
        else:
            print(f"‚ùå Pipeline audio: {audio_test.get('error', 'Erreur inconnue')}")
            
    except Exception as e:
        print(f"‚ùå Erreur test bridge: {e}")

==================================================
FICHIER: .\.0IAquestions\jarvis.py
==================================================

"""
jarvis.py - Point d'entr√©e unifi√© (Interface Web)
Lance automatiquement l'interface web et ouvre le navigateur
"""

import sys
import time
import webbrowser
import threading
from pathlib import Path
import uvicorn
from colorama import init, Fore, Style

# Initialiser colorama
init()

def print_banner():
    """Banni√®re Jarvis avec info web"""
    print(f"""{Fore.CYAN}
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë         ü§ñ JARVIS v0.2            ‚ïë
‚ïë    Assistant Vocal Intelligent    ‚ïë
‚ïë     Interface Web Unifi√©e         ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
{Style.RESET_ALL}""")

def check_dependencies():
    """V√©rifier que toutes les d√©pendances sont install√©es"""
    missing = []
    
    try:
        import fastapi
    except ImportError:
        missing.append("fastapi")
    
    try:
        import uvicorn
    except ImportError:
        missing.append("uvicorn")
    
    try:
        import ollama
    except ImportError:
        missing.append("ollama")
    
    if missing:
        print(f"{Fore.RED}‚ùå D√©pendances manquantes: {', '.join(missing)}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}üí° Installez avec: pip install {' '.join(missing)}{Style.RESET_ALL}")
        return False
    
    return True

def check_ollama_running():
    """V√©rifier qu'Ollama est d√©marr√©"""
    try:
        import ollama
        models = ollama.list()
        print(f"{Fore.GREEN}‚úÖ Ollama connect√© ({len(models.get('models', []))} mod√®les){Style.RESET_ALL}")
        return True
    except Exception as e:
        print(f"{Fore.RED}‚ùå Ollama non accessible: {e}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}üí° D√©marrez Ollama puis relancez Jarvis{Style.RESET_ALL}")
        return False

def open_browser_delayed(url: str, delay: float = 2.0):
    """Ouvre le navigateur apr√®s un d√©lai"""
    time.sleep(delay)
    try:
        webbrowser.open(url)
        print(f"{Fore.GREEN}üåê Navigateur ouvert sur {url}{Style.RESET_ALL}")
    except Exception as e:
        print(f"{Fore.YELLOW}‚ö†Ô∏è Impossible d'ouvrir le navigateur: {e}{Style.RESET_ALL}")
        print(f"{Fore.BLUE}üí° Ouvrez manuellement: {url}{Style.RESET_ALL}")

def create_web_app():
    """Cr√©e l'application FastAPI"""
    from fastapi import FastAPI, WebSocket, WebSocketDisconnect
    from fastapi.staticfiles import StaticFiles
    from fastapi.responses import FileResponse
    from contextlib import asynccontextmanager
    
    # Variables globales pour les gestionnaires
    websocket_relay = None
    config_coordinator = None
    conversation_flow = None

    @asynccontextmanager
    async def lifespan(app: FastAPI):
        """Gestionnaire de cycle de vie FastAPI"""
        # Startup minimal pour √©viter les blocages
        print(f"{Fore.BLUE}üöÄ D√©marrage FastAPI (initialisation diff√©r√©e)...{Style.RESET_ALL}")
        
        # Variables globales mises √† jour mais pas initialis√©es ici
        nonlocal websocket_relay, config_coordinator, conversation_flow
        
        print(f"{Fore.GREEN}‚úÖ Serveur pr√™t (modules charg√©s √† la premi√®re connexion){Style.RESET_ALL}")
        
        yield  # Application en cours
        
        # Shutdown
        print(f"{Fore.YELLOW}üõë Arr√™t du serveur...{Style.RESET_ALL}")
        if websocket_relay:
            await websocket_relay.shutdown()

    # Fonction d'initialisation diff√©r√©e (appel√©e au premier WebSocket)
    def init_modules_lazy():
        """Initialisation paresseuse des modules"""
        nonlocal websocket_relay, config_coordinator, conversation_flow
        
        if websocket_relay is None:  # Premi√®re fois
            print(f"{Fore.CYAN}üîß Initialisation diff√©r√©e des modules...{Style.RESET_ALL}")
            
            from thalamus.websocket_relay import WebSocketRelay
            from hypothalamus.config_coordinator import ConfigCoordinator
            from lobes_temporaux.conversation_flow import ConversationFlow
            
            websocket_relay = WebSocketRelay()
            config_coordinator = ConfigCoordinator()
            conversation_flow = ConversationFlow()
            
            config_coordinator.set_conversation_flow(conversation_flow)
            
            print(f"{Fore.GREEN}‚úÖ Modules initialis√©s !{Style.RESET_ALL}")
        
        return websocket_relay, config_coordinator, conversation_flow

    # Cr√©er l'application
    app = FastAPI(title="Jarvis Assistant - Architecture Neuroanatomique", lifespan=lifespan)

    # Servir les fichiers statiques
    app.mount("/static", StaticFiles(directory="web_interface"), name="static")

    # Routes principales
    @app.get("/")
    async def root():
        """Page principale"""
        return FileResponse('web_interface/index.html')

    # Routes API - D√©l√©gation selon architecture neuroanatomique
    @app.get("/api/config")
    async def get_config():
        """Configuration actuelle (Hypothalamus)"""
        try:
            _, coordinator, _ = init_modules_lazy()
            if coordinator:
                return coordinator.get_current_config()
            return {"error": "Config coordinator non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.post("/api/config")
    async def update_config(config: dict):
        """Mettre √† jour la configuration (Hypothalamus)"""
        try:
            _, coordinator, _ = init_modules_lazy()
            if coordinator:
                return await coordinator.update_config(config)
            return {"error": "Config coordinator non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.get("/api/conversation")
    async def get_conversation():
        """Historique de conversation (Lobes Temporaux)"""
        try:
            _, _, flow = init_modules_lazy()
            if flow:
                return flow.get_history()
            return {"error": "Conversation flow non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.delete("/api/conversation")
    async def clear_conversation():
        """Effacer l'historique (Lobes Temporaux)"""
        try:
            _, _, flow = init_modules_lazy()
            if flow:
                return flow.clear_history()
            return {"error": "Conversation flow non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.get("/api/voices")
    async def get_available_voices():
        """Voix disponibles (Hypothalamus)"""
        try:
            _, coordinator, _ = init_modules_lazy()
            if coordinator:
                return coordinator.get_available_voices()
            return {"error": "Config coordinator non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    @app.get("/api/devices")
    async def get_available_devices():
        """P√©riph√©riques audio disponibles (Hypothalamus)"""
        try:
            _, coordinator, _ = init_modules_lazy()
            if coordinator:
                return coordinator.get_available_devices()
            return {"error": "Config coordinator non initialis√©"}
        except Exception as e:
            return {"error": f"Erreur: {e}"}

    # WebSocket - Thalamus (Hub communication)
    @app.websocket("/ws")
    async def websocket_endpoint(websocket: WebSocket):
        """WebSocket principal - Thalamus relay avec initialisation diff√©r√©e"""
        try:
            # Initialisation diff√©r√©e des modules
            relay, coordinator, flow = init_modules_lazy()
            
            if not all([relay, coordinator, flow]):
                await websocket.close(code=1011, reason="Modules neuroanatomiques non initialis√©s")
                return
            
            await relay.handle_connection(
                websocket, 
                flow,        # Lobes temporaux
                coordinator  # Hypothalamus
            )
        except WebSocketDisconnect:
            pass  # D√©connexion normale
        except Exception as e:
            print(f"{Fore.RED}‚ùå Erreur Thalamus WebSocket: {e}{Style.RESET_ALL}")

    return app

def main():
    """Point d'entr√©e principal"""
    print_banner()
    
    # V√©rifications pr√©alables
    print(f"{Fore.BLUE}üîç V√©rification des pr√©requis...{Style.RESET_ALL}")
    
    if not check_dependencies():
        return 1
    
    if not check_ollama_running():
        return 1
    
    # Cr√©er l'application web
    app = create_web_app()
    
    # URL de l'interface
    url = "http://localhost:8000"
    
    # Programmer l'ouverture du navigateur
    browser_thread = threading.Thread(
        target=open_browser_delayed, 
        args=(url, 3.0),
        daemon=True
    )
    browser_thread.start()
    
    # D√©marrer le serveur
    print(f"{Fore.BLUE}üåê D√©marrage de l'interface web...{Style.RESET_ALL}")
    print(f"{Fore.GREEN}üìç Interface accessible sur: {url}{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}üí° Le navigateur va s'ouvrir automatiquement{Style.RESET_ALL}")
    print(f"{Fore.CYAN}üîÑ Appuyez Ctrl+C pour arr√™ter{Style.RESET_ALL}\n")
    
    try:
        # Lancer uvicorn
        uvicorn.run(
            app,
            host="127.0.0.1",
            port=8000,
            log_level="error",  # Moins verbeux
            access_log=False    # Pas de logs d'acc√®s
        )
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}üõë Arr√™t demand√© par l'utilisateur{Style.RESET_ALL}")
        print(f"{Fore.GREEN}üëã Au revoir !{Style.RESET_ALL}")
        return 0
    except Exception as e:
        print(f"\n{Fore.RED}‚ùå Erreur fatale: {e}{Style.RESET_ALL}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

==================================================
FICHIER: .\.0IAquestions\llm_client.py
==================================================

"""
Client LLM  pour Jarvis avec streaming natif
Avec support streaming web et CMD
"""

import ollama
import yaml
from pathlib import Path
from hypothalamus.logger import log


class JarvisLLM:
    """LLM Jarvis unifi√© avec support streaming natif"""
    
    def __init__(self, personality="Jarvis"):
        # Charger config
        config_path = Path(__file__).parent.parent / "config" / "settings.yaml"
        if not config_path.exists():
            # Fallback si pas de config projet
            config_path = Path(__file__).parent / "settings.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.model = self.config['llm']['model_default']
        self.personality = personality
        
        log.success(f"LLM pr√™t ({self.model}) - Mode: {personality}", "üß†")

    def estimate_complexity(self, text: str) -> str:
        """Analyse simple de la complexit√© (mots-cl√©s + longueur)"""
        text_lower = text.lower()
        word_count = len(text_lower.split())

        expert_keywords = [
            "analyse", "explique", "pourquoi", "comment", "comparer", "th√©orie",
            "concept", "quantique", "algorithme", "d√©veloppe", "projet", "plan",
            "fonctionnement", "m√©canisme", "histoire", "impact", "diff√©rence"
        ]

        simple_keywords = [
            "salut", "bonjour", "heure", "merci", "date",
            "temp√©rature", "m√©t√©o", "au revoir"
        ]

        # Cas simples : social / commande
        if any(k in text_lower for k in simple_keywords):
            return "Express"

        # Cas complexes : question profonde / notion avanc√©e
        if any(k in text_lower for k in expert_keywords):
            return "Expert"

        # Sinon on se base sur la longueur
        if word_count <= 8:
            return "Express"
        elif word_count <= 30:
            return "Standard"
        else:
            return "Expert"

    def generate_response_stream(self, user_input: str):
        """
        üî• STREAMING NATIF - Yield les tokens un par un depuis Ollama
        Utilis√© par l'interface web pour affichage temps r√©el
        """
        # 1Ô∏è‚É£ Estimation de la complexit√© locale
        complexity = self.estimate_complexity(user_input)

        # 2Ô∏è‚É£ R√©glages dynamiques selon complexit√©
        if complexity == "Express":
            temperature = 0.3
            max_tokens = 500
        elif complexity == "Standard":
            temperature = 0.5
            max_tokens = 1200
        else:  # Expert
            temperature = 0.7
            max_tokens = 3000

        log.info(f"Complexit√© estim√©e : {complexity} ({temperature=}, {max_tokens=})")

        # 3Ô∏è‚É£ Description du ton selon la personnalit√©
        if self.personality == "Jarvis":
            assistant_desc = (
                "Tu es Jarvis, un assistant fran√ßais intelligent, pr√©cis et un peu ironique. "
                "R√©ponds toujours en fran√ßais, de fa√ßon claire, naturelle et concise."
            )
        else:
            assistant_desc = (
                "Tu es Samantha, une assistante fran√ßaise douce, empathique et professionnelle. "
                "R√©ponds toujours en fran√ßais, de fa√ßon fluide, naturelle et concise."
            )

        # 4Ô∏è‚É£ Construire le prompt complet
        prompt = f"""{assistant_desc}

Question ({complexity}): {user_input}

R√©ponse:"""

        # 5Ô∏è‚É£ Appel √† Ollama avec streaming natif
        try:
            log.debug("D√©marrage streaming Ollama...")
            
            # üî• STREAMING NATIF OLLAMA
            stream = ollama.generate(
                model=self.model,
                prompt=prompt,
                stream=True,  # ‚ö° STREAMING ACTIV√â
                options={
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            )
            
            # Yield chaque token re√ßu en temps r√©el
            token_count = 0
            for chunk in stream:
                if 'response' in chunk:
                    token = chunk['response']
                    if token:  # Ignorer les tokens vides
                        token_count += 1
                        yield token
            
            log.debug(f"Streaming termin√©: {token_count} tokens")

        except Exception as e:
            log.error(f"Erreur streaming Ollama: {e}")
            yield "D√©sol√©, une erreur est survenue pendant la r√©ponse."

    def generate_response(self, user_input: str) -> str:
        """
        M√©thode de compatibilit√© (non-streaming)
        R√©cup√®re tout le stream et le joint pour retourner une string compl√®te
        Utilis√© pour compatibilit√© avec ancien code ou usage simple
        """
        # R√©cup√©rer tout le stream et le joindre
        tokens = list(self.generate_response_stream(user_input))
        return ''.join(tokens)

    def ask(self, user_input: str) -> str:
        """M√©thode courte (compatibilit√©)"""
        return self.generate_response(user_input)

# Test standalone
if __name__ == "__main__":
    print("üß™ Test LLM Unifi√©")
    
    try:
        llm = JarvisLLM("Samantha")
        
        print("\nüî• Test streaming:")
        print("Question: Raconte-moi une blague")
        print("R√©ponse: ", end="", flush=True)
        
        for token in llm.generate_response_stream("Raconte-moi une blague"):
            print(token, end="", flush=True)
        
        print("\n\n‚úÖ Test termin√©")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")

==================================================
FICHIER: .\.0IAquestions\logger.py
==================================================

"""
Syst√®me de logs pour Jarvis
"""

from colorama import Fore, Style
import yaml
from pathlib import Path

# Charger config
config_path = Path("config/settings.yaml")
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

LOG_LEVEL = config['system'].get('log_level', 'STANDARD').upper()
LEVELS = {"STANDARD": 0, "INFO": 1, "DEBUG": 2}

class JarvisLogger:
    """Logger avec niveaux DEBUG/INFO/STANDARD"""
    
    @staticmethod
    def debug(message, prefix="üîç"):
        """Affiche uniquement en mode DEBUG"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["DEBUG"]:
            print(f"{Fore.CYAN}{prefix} [DEBUG] {message}{Style.RESET_ALL}")
    
    @staticmethod
    def info(message, prefix="‚ÑπÔ∏è"):
        """Affiche si niveau >= STANDARD pour les information de base"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["STANDARD"]:
            print(f"{Fore.WHITE}{prefix} {message}{Style.RESET_ALL}")
    
    @staticmethod
    def success(message, prefix="‚úÖ"):
        """Message de succ√®s"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["INFO"]:
            print(f"{Fore.GREEN}{prefix} {message}{Style.RESET_ALL}")
    
    @staticmethod
    def warning(message, prefix="‚ö†Ô∏è"):
        """Avertissement"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["INFO"]:
            print(f"{Fore.YELLOW}{prefix} {message}{Style.RESET_ALL}")
    
    @staticmethod
    def error(message, prefix="‚ùå"):
        """Erreur"""
        # Toujours afficher les erreurs, m√™me en mode STANDARD
        print(f"{Fore.RED}{prefix} {message}{Style.RESET_ALL}")
    
    @staticmethod
    def user(message):
        """Message utilisateur"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["STANDARD"]:
            print(f"{Fore.BLUE}üë§ Vous: {message}{Style.RESET_ALL}")
    
    @staticmethod
    def jarvis(message):
        """Message Jarvis"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["STANDARD"]:
            print(f"{Fore.MAGENTA}ü§ñ Jarvis: {message}{Style.RESET_ALL}")
    
    @staticmethod
    def thinking(message):
        """R√©flexion"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["DEBUG"]:
            print(f"{Fore.CYAN}üß† {message}{Style.RESET_ALL}")
    
    @staticmethod
    def separator():
        """S√©parateur visuel"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["DEBUG"]:
            print(f"{Fore.CYAN}{'='*50}{Style.RESET_ALL}")

# Instance globale
log = JarvisLogger()


==================================================
FICHIER: .\.0IAquestions\message_router.py
==================================================

"""
message_router.py - Routage intelligent des messages (Thalamus)
Responsabilit√© : Router les messages vers les bons modules selon leur type
"""

from pathlib import Path
import sys

# Import logger depuis hypothalamus
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

class MessageRouter:
    """Routeur intelligent de messages (Gare de triage thalamique)"""
    
    def __init__(self):
        # Mapping des types de messages vers les modules cibles
        self.route_map = {
            # Messages de conversation ‚Üí lobes_temporaux
            'text_message': 'conversation_flow',
            'voice_input': 'conversation_flow',
            'transcription': 'conversation_flow',
            
            # Messages de configuration ‚Üí hypothalamus
            'config_update': 'config_coordinator',
            'voice_change': 'config_coordinator',
            'device_change': 'config_coordinator',
            
            # Messages syst√®me ‚Üí hypothalamus
            'system_status': 'system_monitor',
            'health_check': 'system_monitor',
            
            # Messages de contr√¥le ‚Üí thalamus
            'ping': 'local',
            'connection_test': 'local'
        }
        
        log.info("MessageRouter initialis√© (Thalamus)")
    
    def get_target_module(self, message_type: str) -> str:
        """D√©termine le module cible pour un type de message"""
        target = self.route_map.get(message_type, 'unknown')
        
        if target == 'unknown':
            log.warning(f"Type de message non rout√©: {message_type}")
        else:
            log.debug(f"Route: {message_type} ‚Üí {target}")
        
        return target
    
    def is_local_message(self, message_type: str) -> bool:
        """V√©rifie si le message doit √™tre trait√© localement par le thalamus"""
        return self.route_map.get(message_type) == 'local'
    
    def get_module_priority(self, message_type: str) -> int:
        """Retourne la priorit√© de traitement (1=urgent, 3=normal, 5=diff√©r√©)"""
        priority_map = {
            # Messages urgents
            'voice_input': 1,
            'transcription': 1,
            'connection_test': 1,
            
            # Messages normaux
            'text_message': 3,
            'config_update': 3,
            'ping': 3,
            
            # Messages diff√©r√©s
            'health_check': 5,
            'system_status': 5
        }
        
        return priority_map.get(message_type, 3)  # Normal par d√©faut
    
    def validate_message(self, message: dict) -> tuple[bool, str]:
        """Valide la structure d'un message"""
        if not isinstance(message, dict):
            return False, "Message doit √™tre un dictionnaire"
        
        if 'type' not in message:
            return False, "Champ 'type' manquant"
        
        message_type = message['type']
        
        # Validation sp√©cifique par type
        if message_type in ['text_message', 'transcription']:
            if 'content' not in message or not message['content'].strip():
                return False, "Champ 'content' manquant ou vide"
        
        elif message_type == 'config_update':
            if 'config' not in message:
                return False, "Champ 'config' manquant"
        
        return True, "Message valide"
    
    def get_routing_stats(self) -> dict:
        """Retourne les statistiques de routage"""
        module_counts = {}
        for target in self.route_map.values():
            module_counts[target] = module_counts.get(target, 0) + 1
        
        return {
            'total_routes': len(self.route_map),
            'modules_count': len(set(self.route_map.values())),
            'distribution': module_counts
        }

# Test standalone
if __name__ == "__main__":
    router = MessageRouter()
    
    # Test routing
    test_messages = [
        {'type': 'text_message', 'content': 'Hello'},
        {'type': 'config_update', 'config': {'voice': 'Jarvis'}},
        {'type': 'ping'},
        {'type': 'unknown_type'}
    ]
    
    for msg in test_messages:
        valid, reason = router.validate_message(msg)
        if valid:
            target = router.get_target_module(msg['type'])
            priority = router.get_module_priority(msg['type'])
            print(f"‚úÖ {msg['type']} ‚Üí {target} (priorit√©: {priority})")
        else:
            print(f"‚ùå {msg}: {reason}")
    
    print(f"\nüìä Stats: {router.get_routing_stats()}")

==================================================
FICHIER: .\.0IAquestions\stt.py
==================================================

"""
Speech-to-Text avec Whisper + VAD (Voice Activity Detection)
"""

import whisper
import pyaudio
import wave
import os
from pathlib import Path
import yaml
from datetime import datetime
import numpy as np
import webrtcvad
from collections import deque
import time

# Import logger
import sys
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

# Configuration FFmpeg
import imageio_ffmpeg
os.environ["PATH"] = os.path.dirname(imageio_ffmpeg.get_ffmpeg_exe()) + os.pathsep + os.environ.get("PATH", "")

class SpeechToText:
    def __init__(self, device_index=None):
        # Charger config
        config_path = Path(__file__).parent.parent / "config" / "settings.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        model_size = config['audio']['input']['whisper_model']
        self.language = config['audio']['input']['language']
        self.device_index = device_index
        
        # Cr√©er dossier pour les enregistrements
        self.recordings_dir = Path("recordings")
        self.recordings_dir.mkdir(exist_ok=True)
        
        log.info(f"Chargement mod√®le Whisper '{model_size}'...", "üé§")
        self.model = whisper.load_model(model_size)
        log.success("Whisper pr√™t !", "üé§")
        
        # Initialiser VAD (Voice Activity Detection)
        self.vad = webrtcvad.Vad(2)  # Agressivit√© 0-3 (2 = √©quilibr√©)
        log.success("VAD initialis√© !", "üéôÔ∏è")
        
        if self.device_index is not None:
            p = pyaudio.PyAudio()
            device_name = p.get_device_info_by_index(self.device_index)['name']
            p.terminate()
            log.info(f"Utilise : {device_name}", "üéôÔ∏è")
    
    def listen_with_vad(self, timeout: int = 30, silence_duration: float = 1.5):
        """
        Enregistre avec d√©tection de voix automatique
        
        timeout: dur√©e max d'attente/enregistrement (secondes)
        silence_duration: dur√©e de silence pour arr√™ter (secondes)
        """
        
        # Param√®tres audio pour VAD
        RATE = 16000
        CHUNK = 480  # 30ms √† 16kHz (requis par webrtcvad)
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        
        # Buffers
        pre_speech_buffer = deque(maxlen=20)  # 600ms avant d√©tection
        speech_frames = []
        
        # √âtats
        is_speaking = False
        silence_frames = 0
        silence_threshold = int(silence_duration * RATE / CHUNK)
        
        log.info("üéôÔ∏è  Micro actif, parle quand tu veux...", "")
        
        p = pyaudio.PyAudio()
        
        stream_params = {
            'format': FORMAT,
            'channels': CHANNELS,
            'rate': RATE,
            'input': True,
            'frames_per_buffer': CHUNK
        }
        
        if self.device_index is not None:
            stream_params['input_device_index'] = self.device_index
        
        stream = p.open(**stream_params)
        
        start_time = time.time()
        
        try:
            while time.time() - start_time < timeout:
                frame = stream.read(CHUNK, exception_on_overflow=False)
                
                # D√©tection de voix avec VAD
                is_speech = self.vad.is_speech(frame, RATE)
                
                if not is_speaking:
                    # Avant de parler : garder un buffer
                    pre_speech_buffer.append(frame)
                    
                    if is_speech:
                        # D√©but de parole d√©tect√© !
                        log.info("üó£Ô∏è  Parole d√©tect√©e, enregistrement...", "")
                        is_speaking = True
                        # Ajouter le buffer pr√©-parole
                        speech_frames.extend(pre_speech_buffer)
                        speech_frames.append(frame)
                else:
                    # En train de parler
                    speech_frames.append(frame)
                    
                    if not is_speech:
                        # Silence d√©tect√©
                        silence_frames += 1
                        
                        if silence_frames > silence_threshold:
                            # Assez de silence, arr√™ter
                            log.success("‚úÖ Fin de parole d√©tect√©e", "")
                            break
                    else:
                        # R√©initialiser le compteur de silence
                        silence_frames = 0
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            
            if not speech_frames:
                log.warning("Aucune parole d√©tect√©e")
                return ""
            
            log.success(f"Enregistr√© {len(speech_frames)} frames")
            
            # Sauvegarder le fichier
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            audio_filename = f"recording_{timestamp}.wav"
            tmp_path = self.recordings_dir / audio_filename
            
            log.debug(f"Sauvegarde : {tmp_path}", "üíæ")
            
            wf = wave.open(str(tmp_path), 'wb')
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(p.get_sample_size(FORMAT))
            wf.setframerate(RATE)
            wf.writeframes(b''.join(speech_frames))
            wf.close()
            
            file_size = tmp_path.stat().st_size
            log.debug(f"Fichier : {tmp_path.name} ({file_size} bytes)")
            
            # Analyser volume
            import audioop
            volumes = [audioop.rms(frame, 2) for frame in speech_frames]
            volume_max = max(volumes)
            volume_avg = sum(volumes) / len(volumes)
            log.debug(f"Volume max: {volume_max}, moyen: {volume_avg:.0f}", "üìä")
            
            # Transcription
            return self._transcribe(tmp_path)
            
        except Exception as e:
            log.error(f"Erreur enregistrement VAD: {e}")
            stream.stop_stream()
            stream.close()
            p.terminate()
            return ""
    
    def _transcribe(self, audio_path):
        """Transcription interne"""
        log.info("Transcription...")
        log.debug(f"Fichier : {audio_path.absolute()}")
        
        try:
            # Lecture manuelle WAV
            with wave.open(str(audio_path.absolute()), 'rb') as wf:
                n_channels = wf.getnchannels()
                framerate = wf.getframerate()
                n_frames = wf.getnframes()
                
                log.debug(f"Format : {n_channels} canal, {framerate}Hz, {n_frames} frames")
                
                audio_data = wf.readframes(n_frames)
            
            # Convertir en numpy
            audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            log.debug(f"Audio charg√© : {len(audio_array)} samples")
            
            # Transcription
            result = self.model.transcribe(
                audio_array,
                language=self.language, 
                fp16=False
            )
            text = result["text"].strip()
            
            if text:
                log.success(f"Transcription: '{text}'")
            else:
                log.warning("Transcription vide")
            
            return text
            
        except Exception as e:
            log.error(f"Erreur transcription: {e}")
            import traceback
            log.debug(traceback.format_exc())
            return ""

==================================================
FICHIER: .\.0IAquestions\styles.css
==================================================

/* Variables CSS pour les th√®mes */
:root {
    /* Th√®me Light */
    --bg-primary: #ffffff;
    --bg-secondary: #f8f9fa;
    --bg-accent: #e9ecef;
    --text-primary: #212529;
    --text-secondary: #6c757d;
    --text-accent: #0d6efd;
    --border: #dee2e6;
    --shadow: rgba(0,0,0,0.1);
    --success: #198754;
    --warning: #ffc107;
    --danger: #dc3545;
    --info: #0dcaf0;
}

.theme-dark {
    /* Th√®me Gray - blue */
    --bg-primary: #1a1a1a;
    --bg-secondary: #2d2d2d;
    --bg-accent: #3d3d3d;
    --text-primary: #ffffff;
    --text-secondary: #b0b0b0;
    --text-accent: #4dabf7;
    --border: #4d4d4d;
    --shadow: rgba(0,0,0,0.3);
    --success: #51cf66;
    --warning: #ffd43b;
    --danger: #ff6b6b;
    --info: #74c0fc;
}

.theme-jarvis {
    /* Th√®me Dark - Green*/
    --bg-primary: #0a0e1a;
    --bg-secondary: #1a2332;
    --bg-accent: #2a3441;
    --text-primary: #00d4ff;
    --text-secondary: #7fb8d3;
    --text-accent: #00ff88;
    --border: #3a4451;
    --shadow: rgba(0,212,255,0.2);
    --success: #00ff88;
    --warning: #ffb347;
    --danger: #ff6b6b;
    --info: #00d4ff;
}

/* Reset et base */
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    background-color: var(--bg-primary);
    color: var(--text-primary);
    transition: all 0.3s ease;
    height: 100vh;
    overflow: hidden;
}

/* Header */
.header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1rem 2rem;
    background-color: var(--bg-secondary);
    border-bottom: 1px solid var(--border);
    box-shadow: 0 2px 4px var(--shadow);
    z-index: 1000;
}

.header-left {
    display: flex;
    align-items: center;
}

.logo {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    font-size: 1.5rem;
    font-weight: bold;
    color: var(--text-accent);
}

.logo-icon {
    font-size: 2rem;
    animation: pulse 2s infinite;
}

.status-indicator {
    font-size: 0.8rem;
    margin-left: 0.5rem;
    color: var(--success);
    animation: blink 1.5s infinite;
}

.status-indicator.offline {
    color: var(--danger);
}

.header-nav {
    display: flex;
    gap: 1rem;
}

.nav-btn {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    padding: 0.5rem 1rem;
    background: var(--bg-accent);
    border: 1px solid var(--border);
    border-radius: 8px;
    color: var(--text-primary);
    cursor: pointer;
    transition: all 0.2s ease;
    font-size: 0.9rem;
}

.nav-btn:hover {
    background: var(--text-accent);
    color: var(--bg-primary);
    transform: translateY(-1px);
}

.icon {
    font-size: 1.1rem;
}

/* Layout principal */
.main-content {
    display: grid;
    grid-template-columns: 1fr 350px;
    grid-template-rows: 1fr auto;
    height: calc(100vh - 80px);
    gap: 1rem;
    padding: 1rem;
}

.main-content.debug-hidden {
    grid-template-columns: 1fr;
}

/* Section dialogue */
.dialogue-section {
    grid-row: 1;
    display: flex;
    flex-direction: column;
    background: var(--bg-secondary);
    border-radius: 12px;
    border: 1px solid var(--border);
    overflow: hidden;
}

.dialogue-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1rem;
    background: var(--bg-accent);
    border-bottom: 1px solid var(--border);
}

.dialogue-header h2 {
    font-size: 1.2rem;
    color: var(--text-accent);
}

.dialogue-controls {
    display: flex;
    gap: 0.5rem;
}

.control-btn {
    padding: 0.5rem;
    background: transparent;
    border: 1px solid var(--border);
    border-radius: 6px;
    color: var(--text-secondary);
    cursor: pointer;
    transition: all 0.2s ease;
}

.control-btn:hover {
    background: var(--bg-primary);
    color: var(--text-primary);
}

.dialogue-container {
    flex: 1;
    padding: 1rem;
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    gap: 1rem;
}

/* Messages */
.message-bubble {
    max-width: 80%;
    padding: 0.75rem 1rem;
    border-radius: 18px;
    animation: slideIn 0.3s ease;
}

.message-bubble.user {
    align-self: flex-end;
    background: var(--text-accent);
    color: var(--bg-primary);
    border-bottom-right-radius: 6px;
}

.message-bubble.assistant {
    align-self: flex-start;
    background: var(--bg-accent);
    color: var(--text-primary);
    border-bottom-left-radius: 6px;
}

.message-bubble.system {
    align-self: center;
    background: var(--bg-accent);
    color: var(--text-secondary);
    font-style: italic;
    text-align: center;
    border-radius: 12px;
}

.message-content {
    line-height: 1.4;
}

.message-time {
    font-size: 0.7rem;
    opacity: 0.6;
    margin-top: 0.25rem;
}

/* Section saisie */
.input-section {
    grid-row: 2;
    background: var(--bg-secondary);
    border-radius: 12px;
    border: 1px solid var(--border);
    padding: 1rem;
}

.input-container {
    display: flex;
    flex-direction: column;
}

.input-row {
    display: flex;
    gap: 1rem;
    align-items: flex-end;
}

#message-input {
    flex: 1;
    padding: 0.75rem;
    border: 1px solid var(--border);
    border-radius: 8px;
    background: var(--bg-primary);
    color: var(--text-primary);
    resize: vertical;
    font-family: inherit;
    font-size: 0.9rem;
    transition: border-color 0.2s ease;
}

#message-input:focus {
    outline: none;
    border-color: var(--text-accent);
    box-shadow: 0 0 0 2px rgba(13, 110, 253, 0.25);
}

.input-buttons {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}

.mic-button {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    padding: 0.75rem 1rem;
    background: var(--success);
    color: white;
    border: none;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.2s ease;
    font-weight: 500;
}

.mic-button:hover {
    background: #157347;
    transform: scale(1.02);
}

.mic-button.active {
    background: var(--danger);
    animation: pulse 1s infinite;
}

.send-button {
    padding: 0.75rem 1rem;
    background: var(--text-accent);
    color: white;
    border: none;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.2s ease;
    font-size: 1.2rem;
    font-weight: bold;
}

.send-button:hover {
    background: #0b5ed7;
    transform: scale(1.02);
}

/* Section debug */
.debug-section {
    grid-row: 1 / -1;
    background: var(--bg-secondary);
    border-radius: 12px;
    border: 1px solid var(--border);
    display: flex;
    flex-direction: column;
    overflow: hidden;
    transition: all 0.3s ease;
}

.debug-section.hidden {
    display: none;
}

.debug-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1rem;
    background: var(--bg-accent);
    border-bottom: 1px solid var(--border);
}

.debug-header h3 {
    font-size: 1.1rem;
    color: var(--text-accent);
}

.close-debug {
    background: transparent;
    border: none;
    color: var(--text-secondary);
    cursor: pointer;
    font-size: 1.2rem;
    padding: 0.25rem;
    border-radius: 4px;
    transition: all 0.2s ease;
}

.close-debug:hover {
    background: var(--bg-primary);
    color: var(--text-primary);
}

.debug-tabs {
    display: flex;
    background: var(--bg-primary);
    border-bottom: 1px solid var(--border);
}

.tab-btn {
    flex: 1;
    padding: 0.75rem;
    background: transparent;
    border: none;
    color: var(--text-secondary);
    cursor: pointer;
    transition: all 0.2s ease;
    font-size: 0.9rem;
}

.tab-btn.active {
    background: var(--bg-accent);
    color: var(--text-accent);
    border-bottom: 2px solid var(--text-accent);
}

.tab-btn:hover:not(.active) {
    background: var(--bg-secondary);
    color: var(--text-primary);
}

.debug-content {
    flex: 1;
    overflow: hidden;
}

.debug-tab {
    height: 100%;
    overflow-y: auto;
    padding: 1rem;
    display: none;
}

.debug-tab.active {
    display: block;
}

/* Logs */
.log-container {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}

.log-entry {
    display: flex;
    align-items: flex-start;
    gap: 0.5rem;
    padding: 0.5rem;
    border-radius: 6px;
    font-size: 0.8rem;
    line-height: 1.3;
}

.log-entry.info {
    background: rgba(13, 202, 240, 0.1);
    border-left: 3px solid var(--info);
}

.log-entry.success {
    background: rgba(25, 135, 84, 0.1);
    border-left: 3px solid var(--success);
}

.log-entry.warning {
    background: rgba(255, 193, 7, 0.1);
    border-left: 3px solid var(--warning);
}

.log-entry.error {
    background: rgba(220, 53, 69, 0.1);
    border-left: 3px solid var(--danger);
}

.log-time {
    color: var(--text-secondary);
    font-weight: 500;
    min-width: 60px;
}

/* Stats */
.stats-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 1rem;
}

.stat-card {
    background: var(--bg-accent);
    padding: 1rem;
    border-radius: 8px;
    text-align: center;
    border: 1px solid var(--border);
}

.stat-value {
    font-size: 1.5rem;
    font-weight: bold;
    color: var(--text-accent);
    margin-bottom: 0.25rem;
}

.stat-label {
    font-size: 0.8rem;
    color: var(--text-secondary);
}

/* Config */
.config-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0.75rem 0;
    border-bottom: 1px solid var(--border);
}

.config-item:last-child {
    border-bottom: none;
}

.config-item label {
    font-weight: 500;
    color: var(--text-secondary);
}

.config-item span {
    color: var(--text-primary);
    background: var(--bg-accent);
    padding: 0.25rem 0.5rem;
    border-radius: 4px;
    font-size: 0.9rem;
}

/* Modals */
.modal {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0, 0, 0, 0.5);
    display: none;
    justify-content: center;
    align-items: center;
    z-index: 2000;
    animation: fadeIn 0.3s ease;
}

.modal.show {
    display: flex;
}

.modal-content {
    background: var(--bg-primary);
    border-radius: 12px;
    border: 1px solid var(--border);
    width: 90%;
    max-width: 600px;
    max-height: 80vh;
    display: flex;
    flex-direction: column;
    animation: slideInScale 0.3s ease;
}

.modal-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1.5rem;
    border-bottom: 1px solid var(--border);
    background: var(--bg-secondary);
    border-radius: 12px 12px 0 0;
}

.modal-header h2 {
    color: var(--text-accent);
    font-size: 1.3rem;
}

.modal-close {
    background: transparent;
    border: none;
    color: var(--text-secondary);
    cursor: pointer;
    font-size: 1.5rem;
    padding: 0.25rem;
    border-radius: 4px;
    transition: all 0.2s ease;
}

.modal-close:hover {
    background: var(--bg-accent);
    color: var(--text-primary);
}

.modal-body {
    flex: 1;
    overflow-y: auto;
    padding: 1.5rem;
}

.modal-footer {
    display: flex;
    justify-content: flex-end;
    gap: 1rem;
    padding: 1.5rem;
    border-top: 1px solid var(--border);
    background: var(--bg-secondary);
}

/* Settings */
.settings-tabs {
    display: flex;
    margin-bottom: 1.5rem;
    background: var(--bg-secondary);
    border-radius: 8px;
    padding: 0.25rem;
}

.settings-tab {
    display: none;
}

.settings-tab.active {
    display: block;
}

.setting-group {
    margin-bottom: 1.5rem;
}

.setting-group label {
    display: block;
    margin-bottom: 0.5rem;
    font-weight: 500;
    color: var(--text-primary);
}

.setting-group input,
.setting-group select {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid var(--border);
    border-radius: 6px;
    background: var(--bg-secondary);
    color: var(--text-primary);
    font-size: 0.9rem;
}

.setting-group input[type="range"] {
    width: calc(100% - 60px);
    margin-right: 10px;
}

.setting-group input[type="checkbox"] {
    width: auto;
    margin-right: 0.5rem;
}

/* Boutons */
.btn-primary {
    background: var(--text-accent);
    color: white;
    border: none;
    padding: 0.75rem 1.5rem;
    border-radius: 6px;
    cursor: pointer;
    font-weight: 500;
    transition: all 0.2s ease;
}

.btn-primary:hover {
    background: #0b5ed7;
    transform: translateY(-1px);
}

.btn-secondary {
    background: var(--bg-accent);
    color: var(--text-primary);
    border: 1px solid var(--border);
    padding: 0.75rem 1.5rem;
    border-radius: 6px;
    cursor: pointer;
    font-weight: 500;
    transition: all 0.2s ease;
}

.btn-secondary:hover {
    background: var(--bg-secondary);
}

.test-btn {
    background: var(--success);
    color: white;
    border: none;
    padding: 0.5rem 1rem;
    border-radius: 6px;
    cursor: pointer;
    font-size: 0.9rem;
    transition: all 0.2s ease;
}

.test-btn:hover {
    background: #157347;
}

/* Help */
.help-section {
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid var(--border);
}

.help-section:last-child {
    border-bottom: none;
}

.help-section h3 {
    color: var(--text-accent);
    margin-bottom: 0.5rem;
}

.help-section p {
    color: var(--text-secondary);
    line-height: 1.5;
}

/* Animations */
@keyframes pulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.5; }
}

@keyframes blink {
    0%, 50% { opacity: 1; }
    51%, 100% { opacity: 0.3; }
}

@keyframes slideIn {
    from {
        opacity: 0;
        transform: translateY(10px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

@keyframes fadeIn {
    from { opacity: 0; }
    to { opacity: 1; }
}

@keyframes slideInScale {
    from {
        opacity: 0;
        transform: scale(0.9) translateY(-20px);
    }
    to {
        opacity: 1;
        transform: scale(1) translateY(0);
    }
}

/* Responsive */
@media (max-width: 768px) {
    .main-content {
        grid-template-columns: 1fr;
        grid-template-rows: 1fr auto 300px;
    }
    
    .debug-section {
        grid-row: 3;
    }
    
    .header {
        padding: 0.75rem 1rem;
    }
    
    .header-nav {
        gap: 0.5rem;
    }
    
    .nav-btn {
        padding: 0.5rem;
        font-size: 0.8rem;
    }
    
    .nav-btn span:not(.icon) {
        display: none;
    }
    
    .modal-content {
        width: 95%;
        margin: 1rem;
    }
}

/* Scrollbars personnalis√©es */
::-webkit-scrollbar {
    width: 8px;
}

::-webkit-scrollbar-track {
    background: var(--bg-secondary);
}

::-webkit-scrollbar-thumb {
    background: var(--border);
    border-radius: 4px;
}

::-webkit-scrollbar-thumb:hover {
    background: var(--text-secondary);
}

==================================================
FICHIER: .\.0IAquestions\system_monitor.py
==================================================

"""
system_monitor.py - Surveillance syst√®me (Hypothalamus)
Responsabilit√© : Monitoring sant√©, m√©triques, √©tat du syst√®me
Nouveau module pour l'hypothalamus √©tendu
"""

import time
import psutil
import threading
from pathlib import Path
from typing import Dict, Any, Optional
import sys

# Import logger depuis hypothalamus
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

class SystemMonitor:
    """Moniteur syst√®me centralis√© (Hypothalamus)"""
    
    def __init__(self, monitoring_interval: float = 5.0):
        self.monitoring_interval = monitoring_interval
        self.is_monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
        
        # M√©triques syst√®me
        self.metrics = {
            'cpu_percent': 0.0,
            'memory_percent': 0.0,
            'memory_available_gb': 0.0,
            'disk_percent': 0.0,
            'temperature': None,
            'last_update': 0
        }
        
        # Seuils d'alerte
        self.thresholds = {
            'cpu_warning': 80.0,
            'cpu_critical': 95.0,
            'memory_warning': 85.0,
            'memory_critical': 95.0,
            'disk_warning': 90.0,
            'disk_critical': 95.0
        }
        
        # Historique pour tendances
        self.history = {
            'cpu': [],
            'memory': [],
            'timestamps': []
        }
        self.history_max_size = 100
        
        log.info("SystemMonitor initialis√© (Hypothalamus)")
    
    def start_monitoring(self):
        """D√©marre le monitoring en arri√®re-plan"""
        if self.is_monitoring:
            log.warning("Monitoring d√©j√† actif")
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        log.success("Monitoring syst√®me d√©marr√©")
    
    def stop_monitoring(self):
        """Arr√™te le monitoring"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        
        log.info("Monitoring syst√®me arr√™t√©")
    
    def _monitoring_loop(self):
        """Boucle principale de monitoring"""
        while self.is_monitoring:
            try:
                self._update_metrics()
                self._check_thresholds()
                self._update_history()
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                log.error(f"Erreur monitoring: {e}")
                time.sleep(self.monitoring_interval * 2)  # Pause plus longue en cas d'erreur
    
    def _update_metrics(self):
        """Met √† jour les m√©triques syst√®me"""
        try:
            # CPU
            self.metrics['cpu_percent'] = psutil.cpu_percent(interval=1)
            
            # M√©moire
            memory = psutil.virtual_memory()
            self.metrics['memory_percent'] = memory.percent
            self.metrics['memory_available_gb'] = memory.available / (1024**3)
            
            # Disque (partition racine)
            disk = psutil.disk_usage('/')
            self.metrics['disk_percent'] = disk.percent
            
            # Temp√©rature (si disponible)
            try:
                temps = psutil.sensors_temperatures()
                if temps:
                    # Prendre la premi√®re temp√©rature disponible
                    first_sensor = list(temps.values())[0]
                    if first_sensor:
                        self.metrics['temperature'] = first_sensor[0].current
            except:
                self.metrics['temperature'] = None
            
            self.metrics['last_update'] = time.time()
            
        except Exception as e:
            log.error(f"Erreur mise √† jour m√©triques: {e}")
    
    def _check_thresholds(self):
        """V√©rifie les seuils et g√©n√®re des alertes"""
        current_time = time.time()
        
        # CPU
        cpu = self.metrics['cpu_percent']
        if cpu > self.thresholds['cpu_critical']:
            log.warning(f"üö® CPU critique: {cpu:.1f}%")
        elif cpu > self.thresholds['cpu_warning']:
            log.warning(f"‚ö†Ô∏è  CPU √©lev√©: {cpu:.1f}%")
        
        # M√©moire
        memory = self.metrics['memory_percent']
        if memory > self.thresholds['memory_critical']:
            log.warning(f"üö® M√©moire critique: {memory:.1f}%")
        elif memory > self.thresholds['memory_warning']:
            log.warning(f"‚ö†Ô∏è  M√©moire √©lev√©e: {memory:.1f}%")
        
        # Disque
        disk = self.metrics['disk_percent']
        if disk > self.thresholds['disk_critical']:
            log.warning(f"üö® Disque critique: {disk:.1f}%")
        elif disk > self.thresholds['disk_warning']:
            log.warning(f"‚ö†Ô∏è  Disque √©lev√©: {disk:.1f}%")
    
    def _update_history(self):
        """Met √† jour l'historique des m√©triques"""
        current_time = time.time()
        
        self.history['cpu'].append(self.metrics['cpu_percent'])
        self.history['memory'].append(self.metrics['memory_percent'])
        self.history['timestamps'].append(current_time)
        
        # Limiter la taille de l'historique
        if len(self.history['cpu']) > self.history_max_size:
            self.history['cpu'].pop(0)
            self.history['memory'].pop(0)
            self.history['timestamps'].pop(0)
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """Retourne les m√©triques actuelles"""
        return {
            'success': True,
            'metrics': self.metrics.copy(),
            'thresholds': self.thresholds.copy(),
            'monitoring_active': self.is_monitoring
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Retourne les informations syst√®me d√©taill√©es"""
        try:
            import platform
            
            # Informations de base
            uname = platform.uname()
            
            # Processeur
            cpu_info = {
                'physical_cores': psutil.cpu_count(logical=False),
                'logical_cores': psutil.cpu_count(logical=True),
                'max_frequency': f"{psutil.cpu_freq().max:.2f}Mhz" if psutil.cpu_freq() else "N/A",
                'current_frequency': f"{psutil.cpu_freq().current:.2f}Mhz" if psutil.cpu_freq() else "N/A"
            }
            
            # M√©moire
            memory = psutil.virtual_memory()
            memory_info = {
                'total_gb': round(memory.total / (1024**3), 2),
                'available_gb': round(memory.available / (1024**3), 2),
                'used_gb': round(memory.used / (1024**3), 2),
                'percentage': memory.percent
            }
            
            # Disque
            disk = psutil.disk_usage('/')
            disk_info = {
                'total_gb': round(disk.total / (1024**3), 2),
                'free_gb': round(disk.free / (1024**3), 2),
                'used_gb': round(disk.used / (1024**3), 2),
                'percentage': disk.percent
            }
            
            return {
                'success': True,
                'system': {
                    'platform': uname.system,
                    'platform_release': uname.release,
                    'platform_version': uname.version,
                    'architecture': uname.machine,
                    'hostname': uname.node,
                    'processor': uname.processor,
                    'python_version': platform.python_version()
                },
                'cpu': cpu_info,
                'memory': memory_info,
                'disk': disk_info,
                'boot_time': psutil.boot_time()
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_process_info(self, process_name: str = "python") -> Dict[str, Any]:
        """Retourne les informations sur les processus Jarvis/Python"""
        try:
            processes = []
            
            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent', 'cmdline']):
                try:
                    if process_name.lower() in proc.info['name'].lower():
                        # V√©rifier si c'est un processus Jarvis
                        cmdline = ' '.join(proc.info['cmdline'] or [])
                        if 'jarvis' in cmdline.lower():
                            processes.append({
                                'pid': proc.info['pid'],
                                'name': proc.info['name'],
                                'cpu_percent': proc.info['cpu_percent'],
                                'memory_percent': proc.info['memory_percent'],
                                'cmdline': cmdline[:100] + '...' if len(cmdline) > 100 else cmdline
                            })
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            return {
                'success': True,
                'processes': processes,
                'count': len(processes)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_health_status(self) -> Dict[str, Any]:
        """Retourne le statut de sant√© global"""
        metrics = self.metrics
        
        # Calculer le score de sant√© (0-100)
        health_score = 100
        
        # P√©nalit√©s selon les m√©triques
        if metrics['cpu_percent'] > self.thresholds['cpu_warning']:
            health_score -= 20
        if metrics['memory_percent'] > self.thresholds['memory_warning']:
            health_score -= 20
        if metrics['disk_percent'] > self.thresholds['disk_warning']:
            health_score -= 10
        
        # Statut global
        if health_score >= 80:
            status = "Excellent"
            color = "green"
        elif health_score >= 60:
            status = "Bon"
            color = "yellow"
        elif health_score >= 40:
            status = "Moyen"
            color = "orange"
        else:
            status = "Probl√®me"
            color = "red"
        
        return {
            'success': True,
            'health_score': health_score,
            'status': status,
            'color': color,
            'metrics': metrics,
            'last_update': metrics['last_update']
        }

# Test standalone
if __name__ == "__main__":
    print("üß™ Test SystemMonitor (Hypothalamus)")
    
    monitor = SystemMonitor(monitoring_interval=2.0)
    
    try:
        # Test des m√©triques
        monitor._update_metrics()
        print(f"‚úÖ M√©triques: {monitor.get_current_metrics()}")
        
        # Test info syst√®me
        system_info = monitor.get_system_info()
        if system_info['success']:
            print(f"‚úÖ Syst√®me: {system_info['system']['platform']} {system_info['system']['architecture']}")
            print(f"‚úÖ CPU: {system_info['cpu']['logical_cores']} cores")
            print(f"‚úÖ RAM: {system_info['memory']['total_gb']}GB")
        
        # Test health
        health = monitor.get_health_status()
        print(f"‚úÖ Sant√©: {health['status']} ({health['health_score']}/100)")
        
        # Test monitoring (court)
        print("üîÑ Test monitoring 5s...")
        monitor.start_monitoring()
        time.sleep(5)
        monitor.stop_monitoring()
        
        print("‚úÖ Test SystemMonitor termin√©")
        
    except Exception as e:
        print(f"‚ùå Erreur test: {e}")

==================================================
FICHIER: .\.0IAquestions\tts.py
==================================================

""" Text-to-Speech avec Coqui TTS et Edge-TTS FIXED """
from pathlib import Path
import numpy as np
import sounddevice as sd
import asyncio
import tempfile
import os

# Import logger
import sys
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

class TextToSpeech:
    def __init__(self, model_name, personality="Jarvis", edge_voice=None):
        self.personality = personality
        self.model_name = model_name
        
        log.info(f"Chargement mod√®le TTS {personality}...", "üîä")
        
        # D√©tection du mode Edge-TTS
        if model_name == "edge-tts":
            try:
                import edge_tts
                self.is_edge = True
                self.edge_voice = edge_voice or "fr-FR-DeniseNeural"
                self.tts = None
                self.sample_rate = 24000  # Edge utilise 24kHz par d√©faut
                log.success(f"Edge-TTS pr√™t ! Voix: {self.edge_voice} - {personality}", "üîä")
            except ImportError:
                log.error("edge-tts non install√© ! pip install edge-tts")
                raise
        else:
            # Mode Coqui normal
            self.is_edge = False
            try:
                from TTS.api import TTS
                self.tts = TTS(model_name=model_name)
                self.sample_rate = self.tts.synthesizer.output_sample_rate
                log.success(f"Coqui TTS pr√™t ! ({self.sample_rate}Hz) - {personality}", "üîä")
            except Exception as e:
                log.error(f"Erreur chargement TTS: {e}")
                raise
    
    def speak(self, text: str):
        """M√©thode principale qui route vers la bonne impl√©mentation"""
        if self.is_edge:
            # Edge est async, on doit cr√©er un event loop si n√©cessaire
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            loop.run_until_complete(self._speak_edge(text))
        else:
            self._speak_coqui(text)
    
    async def _speak_edge(self, text: str):
        """Version Edge-TTS (async) FIXED"""
        log.jarvis(f"{self.personality}: {text}")
        
        try:
            import edge_tts
            import pygame
            
            log.debug("G√©n√©ration audio avec Edge-TTS...", "üîä")
            
            # Cr√©er un fichier temporaire
            with tempfile.NamedTemporaryFile(delete=False, suffix='.mp3') as tmp_file:
                tmp_path = tmp_file.name
            
            # G√©n√©rer l'audio
            communicate = edge_tts.Communicate(text, self.edge_voice)
            await communicate.save(tmp_path)
            
            # üîß FIX pygame mixer robuste
            try:
                # V√©rifier si pygame mixer est d√©j√† initialis√©
                if not pygame.mixer.get_init():
                    pygame.mixer.init(frequency=22050, size=-16, channels=2, buffer=512)
                    log.debug("Pygame mixer initialis√©", "üîä")
                
                # Charger et jouer
                pygame.mixer.music.load(tmp_path)
                pygame.mixer.music.play()
                
                # Attendre la fin avec timeout de s√©curit√©
                timeout_counter = 0
                max_timeout = 30  # 30 secondes max
                
                while pygame.mixer.music.get_busy() and timeout_counter < max_timeout:
                    pygame.time.Clock().tick(10)  # 100ms
                    timeout_counter += 0.1
                
                if timeout_counter >= max_timeout:
                    log.warning("Timeout lecture Edge-TTS, arr√™t forc√©")
                    pygame.mixer.music.stop()
                
                # ‚úÖ Arr√™ter la musique mais GARDER le mixer initialis√©
                pygame.mixer.music.stop()
                
            except pygame.error as pe:
                log.error(f"Erreur pygame: {pe}")
                # R√©initialiser pygame en cas d'erreur
                try:
                    pygame.mixer.quit()
                    pygame.mixer.init(frequency=22050, size=-16, channels=2, buffer=512)
                    pygame.mixer.music.load(tmp_path)
                    pygame.mixer.music.play()
                    
                    while pygame.mixer.music.get_busy():
                        pygame.time.Clock().tick(10)
                    
                    pygame.mixer.music.stop()
                    
                except Exception as retry_error:
                    log.error(f"Impossible de r√©cup√©rer pygame: {retry_error}")
            
            # Nettoyer le fichier temporaire
            try:
                os.unlink(tmp_path)
            except:
                pass  # Pas grave si on ne peut pas supprimer
            
            log.debug("Lecture Edge-TTS termin√©e", "üîä")
            
        except Exception as e:
            log.error(f"Erreur Edge-TTS: {e}")
            import traceback
            log.debug(traceback.format_exc())
    
    def _speak_coqui(self, text: str):
        """Version Coqui TTS avec streaming"""
        log.jarvis(f"{self.personality}: {text}")
        
        try:
            log.debug("G√©n√©ration audio avec Coqui...", "üîä")
            
            # Synth√©tiser en m√©moire
            wav = self.tts.tts(text=text)
            
            if not isinstance(wav, np.ndarray):
                wav = np.array(wav)
            
            log.debug(f"Audio g√©n√©r√© : {len(wav)} samples", "üîä")
            
            # Streaming par chunks pour fluidit√©
            stream = sd.OutputStream(
                samplerate=self.sample_rate,
                channels=1,
                dtype='float32',
                blocksize=2048
            )
            
            chunk_size = int(self.sample_rate * 0.05)  # 50ms chunks
            
            with stream:
                for i in range(0, len(wav), chunk_size):
                    chunk = wav[i:i+chunk_size]
                    if len(chunk) > 0:
                        stream.write(chunk.astype('float32'))
            
            log.debug("Lecture Coqui termin√©e", "üîä")
            
        except Exception as e:
            log.error(f"Erreur Coqui TTS: {e}")
            import traceback
            log.debug(traceback.format_exc())

==================================================
FICHIER: .\.0IAquestions\voice_manager.py
==================================================

"""
Voice Manager - Gestion des voix TTS
Permet de choisir la voix et la personnalit√© associ√©e
"""

import json
from pathlib import Path
from TTS.api import TTS

class VoiceManager:
    def __init__(self):
        self.config_file = Path("config/voice_config.json")
        self.config_file.parent.mkdir(exist_ok=True)
        
        # Voix disponibles avec m√©tadonn√©es
        self.available_voices = {
            "1": {
                "name": "Jarvis (Homme - Fran√ßais)",
                "model": "tts_models/fr/css10/vits",
                "personality": "Jarvis",
                "gender": "male",
                "description": "Voix masculine fran√ßaise, style assistant"
            },
            "2": {
                "name": "Samantha (Femme - Fran√ßais)",
                "model": "edge-tts",                # Marqueur sp√©cial POUR EDGE TTS (API Microsoft)
                "voice": "fr-FR-DeniseNeural",      # ou fr-FR-JosephineNeural
                "personality": "Samantha",
                "gender": "female",
                "description": "Voix f√©minine fran√ßaise, chaleureuse"
            },
            "3": {
                "name": "Eloise (jeune fille- Edge)",
                "model": "edge-tts",
                "edge_voice": "fr-FR-EloiseNeural", 
                "personality": "Eloise",
                "gender": "female", 
                "description": "Voix f√©minine jeune et dynamique"
            },
            "4": {
                "name": "Josephine (jeune femme - Edge)",
                "model": "edge-tts",
                "edge_voice": "fr-FR-JosephineNeural", 
                "personality": "Josephine",
                "gender": "female", 
                "description": "Voix f√©minine jeune et dynamique"
            }
        }
    
    def load_saved_voice(self):
        """Charge la voix sauvegard√©e"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get('voice_id'), config.get('personality'), config.get('model'), config.get('edge_voice')
        return None, None, None, None
    
    def save_voice(self, voice_id, personality, model, edge_voice):
        """Sauvegarde le choix de voix"""
        config = {
            'voice_id': voice_id,
            'personality': personality,
            'model': model,
            'edge_voice': edge_voice  
        }
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Voix sauvegard√©e : {personality}")
    
    def select_voice(self):
        """
        S√©lection de la voix par l'utilisateur
        Retourne (personality, model)
        """
        print("\n" + "="*60)
        print("üé§ CONFIGURATION VOIX")
        print("="*60)
        
        # V√©rifier si une voix est d√©j√† sauvegard√©e
        saved_id, saved_personality, saved_model, edge_voice = self.load_saved_voice()
        
        if saved_id and saved_id in self.available_voices:
            voice_info = self.available_voices[saved_id]
            print(f"\nüìÅ Voix sauvegard√©e : {voice_info['name']}")
            print(f"   Personnalit√© : {saved_personality}")
            
            choice = input("Utiliser cette voix ? (O/n) : ").strip().lower()
            if choice in ['', 'o', 'oui', 'y', 'yes']:
                return saved_personality, saved_model
        
        # Afficher les voix disponibles
        print("\nüéôÔ∏è  Voix disponibles :\n")
        for voice_id, info in self.available_voices.items():
            gender_icon = "üë®" if info['gender'] == 'male' else "üë©"
            print(f"{voice_id}. {gender_icon} {info['name']}")
            print(f"   {info['description']}")
            print()
        
        # Choix utilisateur
        while True:
            try:
                choice = input(f"Choisis une voix (1-{len(self.available_voices)}) : ").strip()
                
                if choice in self.available_voices:
                    voice_info = self.available_voices[choice]
                    personality = voice_info['personality']
                    model = voice_info['model']
                    edge_voice = voice_info.get('edge_voice')               # Retourne None si absent
                    
                    print(f"\n‚úÖ Voix s√©lectionn√©e : {voice_info['name']}")
                    print(f"   Personnalit√© : {personality}")
                    print(f"   T√©l√©chargement du mod√®le si n√©cessaire...")
                    
                    # Sauvegarder
                    self.save_voice(choice, personality, model, edge_voice)
                    
                    return personality, model, edge_voice
                else:
                    print(f"‚ùå Choix invalide (1-{len(self.available_voices)})")
                    
            except (ValueError, KeyboardInterrupt):
                print("\n‚ùå Annul√©")
                return None, None, None

==================================================
FICHIER: .\.0IAquestions\websocket_relay.py
==================================================

"""
websocket_relay.py - Relais WebSocket central (Thalamus)
Responsabilit√© : Communication temps r√©el client/serveur
Migr√© depuis web_modules/websocket_handler.py
"""

import json
import asyncio
from typing import List
from fastapi import WebSocket
from pathlib import Path
import sys

# Import logger depuis hypothalamus
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

class WebSocketRelay:
    """Relais centralis√© des connexions WebSocket (Thalamus)"""
    
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.is_initialized = False
    
    async def handle_connection(self, websocket: WebSocket, conversation_flow, config_coordinator):
        """G√®re une nouvelle connexion WebSocket"""
        await websocket.accept()
        self.active_connections.append(websocket)
        
        try:
            # Initialisation automatique
            if not self.is_initialized:
                await self.send_to_client(websocket, {
                    'type': 'status',
                    'content': 'Initialisation automatique...'
                })
                
                success = await conversation_flow.auto_initialize()
                
                if success:
                    personality = conversation_flow.get_personality()
                    await self.send_to_client(websocket, {
                        'type': 'status',
                        'content': 'Pr√™t !',
                        'personality': f'Assistant virtuel - {personality}'
                    })
                    self.is_initialized = True
                else:
                    await self.send_to_client(websocket, {
                        'type': 'error',
                        'content': '√âchec de l\'initialisation automatique'
                    })
                    return
            else:
                # D√©j√† initialis√©, envoyer le statut
                personality = conversation_flow.get_personality()
                await self.send_to_client(websocket, {
                    'type': 'status',
                    'content': 'Connexion √©tablie',
                    'personality': f'Assistant virtuel - {personality}'
                })
            
            # Connecter les √©v√©nements de conversation
            conversation_flow.set_websocket_callback(self.broadcast_to_all)
            
            # Boucle de r√©ception des messages
            await self._message_loop(websocket, conversation_flow, config_coordinator)
            
        except Exception as e:
            log.error(f"Erreur WebSocket: {e}")
        finally:
            if websocket in self.active_connections:
                self.active_connections.remove(websocket)
    
    async def _message_loop(self, websocket: WebSocket, conversation_flow, config_coordinator):
        """Boucle de traitement des messages"""
        while True:
            try:
                data = await websocket.receive_text()
                message = json.loads(data)
                
                # Router les messages vers les bons modules
                await self._route_message(message, conversation_flow, config_coordinator)
                
            except Exception as e:
                log.error(f"Erreur message loop: {e}")
                break
    
    async def _route_message(self, message: dict, conversation_flow, config_coordinator):
        """Route les messages vers les bons modules (Thalamus routing)"""
        
        message_type = message.get('type')
        
        if message_type == 'text_message':
            # Message texte ‚Üí ConversationFlow (lobes_temporaux)
            await conversation_flow.process_text_message(message['content'])
            
        elif message_type == 'voice_input':
            # Entr√©e vocale ‚Üí ConversationFlow (lobes_temporaux)
            await conversation_flow.process_voice_input()
            
        elif message_type == 'config_update':
            # Mise √† jour config ‚Üí ConfigCoordinator (hypothalamus)
            result = await config_coordinator.apply_config(message['config'])
            await self.broadcast_to_all({
                'type': 'config_updated',
                'success': result['success'],
                'message': result.get('message', '')
            })
            
        elif message_type == 'ping':
            # Keep-alive
            await self.broadcast_to_all({'type': 'pong'})
        
        else:
            log.warning(f"Type de message inconnu: {message_type}")
    
    async def send_to_client(self, websocket: WebSocket, message: dict):
        """Envoie un message √† un client sp√©cifique"""
        try:
            await websocket.send_text(json.dumps(message))
        except Exception as e:
            log.error(f"Erreur envoi message: {e}")
            if websocket in self.active_connections:
                self.active_connections.remove(websocket)
    
    async def broadcast_to_all(self, message: dict):
        """Diffuse un message √† tous les clients connect√©s"""
        if not self.active_connections:
            return
        
        # Envoyer √† tous les clients connect√©s
        disconnected = []
        for websocket in self.active_connections:
            try:
                await websocket.send_text(json.dumps(message))
            except:
                disconnected.append(websocket)
        
        # Nettoyer les connexions ferm√©es
        for websocket in disconnected:
            if websocket in self.active_connections:
                self.active_connections.remove(websocket)
    
    def get_connection_count(self) -> int:
        """Retourne le nombre de connexions actives"""
        return len(self.active_connections)
    
    async def shutdown(self):
        """Ferme toutes les connexions proprement"""
        for websocket in self.active_connections[:]:
            try:
                await websocket.close()
            except:
                pass
        self.active_connections.clear()
        log.info("WebSocket Relay ferm√© proprement")

==================================================
FICHIER: .\config\audio_device.json
==================================================

{
  "device_index": 2,
  "device_name": "Headset (OpenComm2 by Shokz)"
}

==================================================
FICHIER: .\config\backgrounds.json
==================================================

{
    "backgrounds": {
      "none": {
        "id": "none",
        "name": "Transparent",
        "type": "solid",
        "value": "transparent"
      },
      "white": {
        "id": "white", 
        "name": "Blanc",
        "type": "solid",
        "value": "#ffffff"
      },
      "dark": {
        "id": "dark",
        "name": "Sombre", 
        "type": "solid",
        "value": "#1a1a1a"
      }
    },
    "default_background": "none",
    "image_folder": "web_interface/images",
    "default_opacity": 0.3,
    "supported_formats": [".jpg", ".jpeg", ".png", ".gif", ".webp"]
}

==================================================
FICHIER: .\config\models.json
==================================================

{
    "llm_models": {
        "llama3.1:8b": {
            "id": "llama3.1:8b",
            "name": "Llama 3.1 8B",
            "display_name": "Llama 3.1 8B (Recommand√©)",
            "description": "Mod√®le √©quilibr√© entre vitesse et qualit√©",
            "size": "8B param√®tres",
            "ram_required": "8GB",
            "speed": "moyenne (5-15 tok/s)",
            "quality": "haute",
            "available": true,
            "default": true,
            "provider": "ollama",
            "complexity_support": {
                "express": {
                    "max_tokens": 150,
                    "temperature": 0.3,
                    "typical_time": "1-2s"
                },
                "standard": {
                    "max_tokens": 500,
                    "temperature": 0.5,
                    "typical_time": "3-5s"
                },
                "expert": {
                    "max_tokens": 2000,
                    "temperature": 0.7,
                    "typical_time": "10-15s"
                }
            }
        },
        "llama3.2:3b": {
            "id": "llama3.2:3b",
            "name": "Llama 3.2 3B",
            "display_name": "Llama 3.2 3B (Rapide)",
            "description": "Mod√®le rapide pour r√©ponses simples",
            "size": "3B param√®tres",
            "ram_required": "4GB",
            "speed": "rapide (15-30 tok/s)",
            "quality": "moyenne",
            "available": false,
            "default": false,
            "provider": "ollama",
            "install_command": "ollama pull llama3.2:3b",
            "install_note": "T√©l√©chargement ~2GB requis",
            "complexity_support": {
                "express": {
                    "max_tokens": 100,
                    "temperature": 0.2,
                    "typical_time": "0.5-1s"
                },
                "standard": {
                    "max_tokens": 300,
                    "temperature": 0.4,
                    "typical_time": "1-2s"
                },
                "expert": {
                    "max_tokens": 800,
                    "temperature": 0.6,
                    "typical_time": "3-5s"
                }
            }
        },
        "mistral:7b": {
            "id": "mistral:7b",
            "name": "Mistral 7B",
            "display_name": "Mistral 7B (Polyvalent)",
            "description": "Mod√®le polyvalent pour usages avanc√©s",
            "size": "7B param√®tres",
            "ram_required": "7GB",
            "speed": "moyenne (8-18 tok/s)",
            "quality": "haute",
            "available": false,
            "default": false,
            "provider": "ollama",
            "install_command": "ollama pull mistral:7b",
            "install_note": "T√©l√©chargement ~4GB requis",
            "complexity_support": {
                "express": {
                    "max_tokens": 150,
                    "temperature": 0.3,
                    "typical_time": "1-2s"
                },
                "standard": {
                    "max_tokens": 600,
                    "temperature": 0.5,
                    "typical_time": "4-6s"
                },
                "expert": {
                    "max_tokens": 2500,
                    "temperature": 0.8,
                    "typical_time": "15-20s"
                }
            }
        },
        "qwen2.5:7b": {
            "id": "qwen2.5:7b",
            "name": "Qwen 2.5 7B",
            "display_name": "Qwen 2.5 7B (Multilingue)",
            "description": "Excellent mod√®le pour le fran√ßais et les langues mixtes",
            "size": "7B param√®tres",
            "ram_required": "7GB",
            "speed": "moyenne (10-20 tok/s)",
            "quality": "tr√®s haute",
            "available": false,
            "default": false,
            "provider": "ollama",
            "install_command": "ollama pull qwen2.5:7b",
            "install_note": "T√©l√©chargement ~4GB requis - Excellent en fran√ßais",
            "complexity_support": {
                "express": {
                    "max_tokens": 150,
                    "temperature": 0.3,
                    "typical_time": "1-2s"
                },
                "standard": {
                    "max_tokens": 600,
                    "temperature": 0.5,
                    "typical_time": "3-5s"
                },
                "expert": {
                    "max_tokens": 2500,
                    "temperature": 0.7,
                    "typical_time": "12-18s"
                }
            }
        }
    },
    "config": {
        "default_model": "llama3.1:8b",
        "fallback_model": "llama3.1:8b",
        "auto_install": false,
        "complexity_detection": {
            "enabled": true,
            "keywords": {
                "express": ["salut", "bonjour", "heure", "merci", "date", "oui", "non"],
                "expert": ["analyse", "explique", "comprend", "pourquoi", "comment", "compare", "projet", "plan"]
            }
        }
    }
}

==================================================
FICHIER: .\config\roles.json
==================================================

{
    "roles": {
      "assistant_general": {
        "id": "assistant_general",
        "name": "Assistant G√©n√©ral",
        "description": "Assistant polyvalent pour toutes t√¢ches",
        "system_prompt": "Tu es un assistant personnel intelligent et bienveillant",
        "temperature": 0.7,
        "max_tokens": 500
      },
      "prof_maths": {
        "id": "prof_maths",
        "name": "Professeur de Math√©matiques",
        "description": "Expert en math√©matiques et p√©dagogie",
        "system_prompt": "Tu es un professeur de math√©matiques exp√©riment√©. Tu expliques les concepts de mani√®re claire et progressive",
        "temperature": 0.5,
        "max_tokens": 800
      },
      "prof_anglais": {
        "id": "prof_anglais",
        "name": "Professeur d'Anglais",
        "description": "Expert en langue anglaise",
        "system_prompt": "Tu es un professeur d'anglais natif. Tu aides √† am√©liorer la grammaire, le vocabulaire et la prononciation",
        "temperature": 0.6,
        "max_tokens": 600
      },
      "chef_projet": {
        "id": "chef_projet",
        "name": "Chef de Projet",
        "description": "Expert en gestion de projet et m√©thodologie",
        "system_prompt": "Tu es un chef de projet senior avec 15 ans d'exp√©rience. Tu donnes des conseils sur la planification, l'organisation et la gestion d'√©quipe",
        "temperature": 0.7,
        "max_tokens": 700
      }
    },
    "default_role": "assistant_general"
  }

==================================================
FICHIER: .\config\themes.json
==================================================

{
    "themes": {
        "light": {
            "id": "light",
            "current_name": "Mode Clair",
            "next_name": "Mode Sombre",
            "next_icon": "üåô",
            "next_theme": "dark",
            "description": "Passer en mode sombre",
            "css_class": "theme-light",
            "primary_color": "#ffffff",
            "text_color": "#212529",
            "accent_color": "#0d6efd",
            "compatible_backgrounds": ["default", "office"]
        },
        "dark": {
            "id": "dark",
            "current_name": "Mode Sombre",
            "next_name": "Mode Jarvis",
            "next_icon": "ü§ñ",
            "next_theme": "jarvis",
            "description": "Passer en mode Jarvis",
            "css_class": "theme-dark",
            "primary_color": "#1a1a1a",
            "text_color": "#ffffff",
            "accent_color": "#4dabf7",
            "compatible_backgrounds": ["default", "jarvis_lab", "space", "office"]
        },
        "jarvis": {
            "id": "jarvis",
            "current_name": "Mode Jarvis",
            "next_name": "Mode Clair",
            "next_icon": "‚òÄÔ∏è",
            "next_theme": "light",
            "description": "Passer en mode clair",
            "css_class": "theme-jarvis",
            "primary_color": "#0a0e1a",
            "text_color": "#00d4ff",
            "accent_color": "#00ff88",
            "compatible_backgrounds": ["default", "jarvis_lab", "space", "matrix"]
        }
    },
    "config": {
        "default_theme": "light",
        "cycle_order": ["light", "dark", "jarvis"],
        "enable_transitions": true,
        "transition_duration": "0.3s"
    }
}

==================================================
FICHIER: .\config\voices.json
==================================================

{
  "voices": {
    "Jarvis": {
      "id": "Jarvis",
      "name": "Jarvis",
      "display_name": "Jarvis (Masculin - Russe)",
      "gender": "male",
      "model": "tts_models/fr/css10/vits",
      "description": "Voix masculine russe, style professeur",
      "voice_type": "adult_male_russian",
      "personality_config": {
        "voice_speed": 1.0,
        "voice_volume": 90
      },
      "personality": "Jarvis",
      "type": "standard",
      "voice_id": "Jarvis"
    },
    "Samantha": {
      "id": "Samantha",
      "name": "Samantha",
      "display_name": "Samantha (F√©minin)",
      "gender": "female",
      "model": "edge-tts",
      "edge_voice": "fr-FR-DeniseNeural",
      "description": "Voix f√©minine chaleureuse et empathique",
      "voice_type": "adult_female",
      "personality_config": {
        "voice_speed": 1.1,
        "voice_volume": 85
      },
      "personality": "Samantha",
      "type": "standard",
      "voice_id": "Samantha"
    },
    "Eloise": {
      "id": "Eloise",
      "name": "Eloise",
      "display_name": "Eloise (Petite fille)",
      "gender": "female",
      "model": "edge-tts",
      "edge_voice": "fr-FR-EloiseNeural",
      "description": "Voix de petite fille, dynamique et enjou√©e",
      "voice_type": "child_female",
      "personality_config": {
        "voice_speed": 1.2,
        "voice_volume": 95
      },
      "personality": "Eloise",
      "type": "standard",
      "voice_id": "Eloise"
    },
    "Josephine": {
      "id": "Josephine",
      "name": "Josephine",
      "display_name": "Josephine (Professionnelle)",
      "gender": "female",
      "model": "edge-tts",
      "edge_voice": "fr-FR-JosephineNeural",
      "description": "Voix f√©minine professionnelle et moderne",
      "voice_type": "adult_female_professional",
      "personality_config": {
        "voice_speed": 1.0,
        "voice_volume": 88
      },
      "personality": "Josephine",
      "type": "standard",
      "voice_id": "Josephine"
    }
  },
  "default_voice": "Jarvis",
  "demo_text": "Bonjour, je suis votre assistant personnel. Comment puis-je vous aider aujourd'hui ?",
  "cloned_voices": {
    "cloned_ffe046c7": {
      "id": "cloned_ffe046c7",
      "name": "Lectrice - Calme et clair",
      "display_name": "üé≠ Lectrice - Calme et clair",
      "gender": "unknown",
      "model": "xtts-v2",
      "sample_path": "cloned_voices\\samples\\cloned_ffe046c7.wav",
      "description": "Voix clon√©e le 10/11/2025",
      "voice_type": "cloned",
      "duration": 19.423900226757368,
      "sample_rate": 22050,
      "created_at": 1762787010.9642298,
      "personality_config": {
        "voice_speed": 1.0,
        "voice_volume": 90
      },
      "processing_status": "ready",
      "embedding_path": "cloned_voices\\samples\\cloned_ffe046c7.pt"
    },
    "cloned_ebf120b2": {
      "id": "cloned_ebf120b2",
      "name": "premier test2",
      "display_name": "üé≠ premier test2",
      "gender": "unknown",
      "model": "xtts-v2",
      "sample_path": "cloned_voices\\samples\\cloned_ebf120b2.wav",
      "description": "Voix clon√©e le 13/11/2025",
      "voice_type": "cloned",
      "duration": 40.06965986394558,
      "sample_rate": 22050,
      "created_at": 1763029038.138143,
      "personality_config": {
        "voice_speed": 1.0,
        "voice_volume": 90
      },
      "processing_status": "ready",
      "embedding_path": "cloned_voices\\samples\\cloned_ebf120b2.pt"
    }
  }
}

==================================================
FICHIER: .\config\voice_config.json
==================================================

{
  "voice_id": "2",
  "personality": "cloned_ffe046c7",
  "model": "xtts-v2",
  "edge_voice": null,
  "sample_path": "cloned_voices/samples/cloned_ffe046c7.wav",
  "embedding_path": null
}

==================================================
FICHIER: .\config\whisper_config.json
==================================================

{
    "model": {
      "name": "small",
      "device": "cpu",
      "compute_type": "int8",
      "cpu_threads": 0,
      "num_workers": 1
    },
    "transcription": {
      "language": "fr",
      "beam_size": 5,
      "best_of": 5,
      "patience": 1.0,
      "length_penalty": 1.0,
      "repetition_penalty": 1.0,
      "no_repeat_ngram_size": 0,
      "temperature": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
      "compression_ratio_threshold": 2.4,
      "log_prob_threshold": -1.0,
      "no_speech_threshold": 0.6,
      "condition_on_previous_text": true,
      "prompt_reset_on_temperature": 0.5,
      "initial_prompt": "Bonjour, voici ce que je veux dire :",
      "prefix": null,
      "suppress_blank": true,
      "suppress_tokens": [-1],
      "without_timestamps": false,
      "max_initial_timestamp": 0.0,
      "word_timestamps": false,
      "prepend_punctuations": "\"'¬ø([{-",
      "append_punctuations": "\"'.„ÄÇÔºå!?::)]}„ÄÅ"
    },
    "vad": {
        "enabled": true,
        "aggressiveness": 2,
        "min_speech_duration": 0.2,
        "silence_duration": 1.3,
        "timeout": 30
    },
    "vad_whisper": {
        "enabled": true,
        "onset": 0.4,
        "offset": 0.3,
        "min_speech_duration": 0.1,
        "max_speech_duration": 30.0,
        "min_silence_duration": 0.5,
        "speech_pad": 0.3
      },
    "audio": {
      "sample_rate": 16000,
      "chunk_size": 480,
      "channels": 1,
      "format": "int16"
    },
    "performance": {
      "fp16": false,
      "enable_chunking": false,
      "chunk_length": 30,
      "hotwords": null,
      "language_detection_threshold": 0.5,
      "language_detection_segments": 1
    },
    "engine": {
        "prefer_faster_whisper": true,
        "auto_fallback": true,
        "performance_mode": "balanced"
    },
    "debug": {
      "save_recordings": false,
      "recordings_path": "./recordings",
      "log_transcription_details": true,
      "log_performance_stats": true
    }
  }

==================================================
FICHIER: .\cortex_prefrontal\llm_client.py
==================================================

"""
Client LLM  pour Jarvis avec streaming natif
Avec support streaming web et CMD
"""

import ollama
import yaml
from pathlib import Path
from hypothalamus.logger import log


class JarvisLLM:
    """LLM Jarvis unifi√© avec support streaming natif"""
    
    def __init__(self, personality="Jarvis"):
        # Charger config
        config_path = Path(__file__).parent.parent / "config\settings.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.model = self.config['llm']['model']
        self.personality = personality
        
        log.success(f"LLM pr√™t ({self.model}) - Mode: {personality}", "üß†")

    def estimate_complexity(self, text: str) -> str:
        """Analyse simple de la complexit√© (mots-cl√©s + longueur)"""
        text_lower = text.lower()
        word_count = len(text_lower.split())

        expert_keywords = [
            "analyse", "explique", "pourquoi", "comment", "comparer", "th√©orie",
            "concept", "quantique", "algorithme", "d√©veloppe", "projet", "plan",
            "fonctionnement", "m√©canisme", "histoire", "impact", "diff√©rence"
        ]

        simple_keywords = [
            "salut", "bonjour", "heure", "merci", "date",
            "temp√©rature", "m√©t√©o", "au revoir"
        ]

        # Cas simples : social / commande
        if any(k in text_lower for k in simple_keywords):
            return "Express"

        # Cas complexes : question profonde / notion avanc√©e
        if any(k in text_lower for k in expert_keywords):
            return "Expert"

        # Sinon on se base sur la longueur
        if word_count <= 8:
            return "Express"
        elif word_count <= 30:
            return "Standard"
        else:
            return "Expert"

    def generate_response_stream(self, user_input: str):
        """
        üî• STREAMING NATIF - Yield les tokens un par un depuis Ollama
        Utilis√© par l'interface web pour affichage temps r√©el
        """
        # 1Ô∏è‚É£ Estimation de la complexit√© locale
        complexity = self.estimate_complexity(user_input)

        # 2Ô∏è‚É£ R√©glages dynamiques selon complexit√©
        if complexity == "Express":
            temperature = 0.3
            max_tokens = 500
        elif complexity == "Standard":
            temperature = 0.5
            max_tokens = 1200
        else:  # Expert
            temperature = 0.7
            max_tokens = 3000

        log.info(f"Complexit√© estim√©e : {complexity} ({temperature=}, {max_tokens=})")

        # 3Ô∏è‚É£ Description du ton selon la personnalit√©
        if self.personality == "Jarvis":
            assistant_desc = (
                "Tu es Jarvis, un assistant fran√ßais intelligent, pr√©cis et un peu ironique. "
                "R√©ponds toujours en fran√ßais, de fa√ßon claire, naturelle et concise."
            )
        else:
            assistant_desc = (
                "Tu es Samantha, une assistante fran√ßaise douce, empathique et professionnelle. "
                "R√©ponds toujours en fran√ßais, de fa√ßon fluide, naturelle et concise."
            )

        # 4Ô∏è‚É£ Construire le prompt complet
        prompt = f"""{assistant_desc}

Question ({complexity}): {user_input}

R√©ponse:"""

        # 5Ô∏è‚É£ Appel √† Ollama avec streaming natif
        try:
            log.debug("D√©marrage streaming Ollama...")
            
            # üî• STREAMING NATIF OLLAMA
            stream = ollama.generate(
                model=self.model,
                prompt=prompt,
                stream=True,  # ‚ö° STREAMING ACTIV√â
                options={
                    "temperature": temperature,
                    "num_predict": max_tokens
                }
            )
            
            # Yield chaque token re√ßu en temps r√©el
            token_count = 0
            for chunk in stream:
                if 'response' in chunk:
                    token = chunk['response']
                    if token:  # Ignorer les tokens vides
                        token_count += 1
                        yield token
            
            log.debug(f"Streaming termin√©: {token_count} tokens")

        except Exception as e:
            log.error(f"Erreur streaming Ollama: {e}")
            yield "D√©sol√©, une erreur est survenue pendant la r√©ponse."

    def generate_response(self, user_input: str) -> str:
        """
        M√©thode de compatibilit√© (non-streaming)
        R√©cup√®re tout le stream et le joint pour retourner une string compl√®te
        Utilis√© pour compatibilit√© avec ancien code ou usage simple
        """
        # R√©cup√©rer tout le stream et le joindre
        tokens = list(self.generate_response_stream(user_input))
        return ''.join(tokens)

    def ask(self, user_input: str) -> str:
        """M√©thode courte (compatibilit√©)"""
        return self.generate_response(user_input)

    def change_model(self, new_model: str):
        """Change le mod√®le LLM √† la vol√©e"""
        old_model = self.model
        self.model = new_model
        log.info(f"üîÑ Mod√®le chang√©: {old_model} ‚Üí {new_model}")
        return True

    def get_current_model(self) -> str:
        """Retourne le mod√®le actuellement utilis√©"""
        return self.model

# Test standalone
if __name__ == "__main__":
    print("üß™ Test LLM Unifi√©")
    
    try:
        llm = JarvisLLM("Samantha")
        
        print("\nüî• Test streaming:")
        print("Question: Raconte-moi une blague")
        print("R√©ponse: ", end="", flush=True)
        
        for token in llm.generate_response_stream("Raconte-moi une blague"):
            print(token, end="", flush=True)
        
        print("\n\n‚úÖ Test termin√©")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")

==================================================
FICHIER: .\cortex_prefrontal\model_manager.py
==================================================

"""
model_manager.py - Gestionnaire des mod√®les LLM avec Ollama
Responsabilit√© : Installation, changement, et v√©rification des mod√®les
Adapt√© pour l'architecture plate de Jarvis
"""

import ollama
import json
import asyncio
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Callable
from hypothalamus.logger import log


class ModelManager:
    """Gestionnaire des mod√®les LLM pour Ollama"""
    
    def __init__(self, models_config_path: str = "models.json"):
        self.config_path = Path(__file__).parent.parent / "config/models.json"
        self.current_model = None
        self.download_callbacks = {}  # Pour les callbacks de progression
        
    def load_available_models(self) -> Dict:
        """Charge la liste des mod√®les disponibles depuis la config"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            log.error(f"Erreur chargement config mod√®les: {e}")
            return {"llm_models": {}, "config": {"default_model": "llama3.1:8b"}}
    
    def get_installed_models(self) -> List[str]:
        """R√©cup√®re la liste des mod√®les install√©s dans Ollama"""
        try:
            models = ollama.list()
            result = []
            for model in models.get('models', []):
                # Essayer plusieurs champs possibles
                model_name = model.get('name') or model.get('model') or str(model)
                if model_name:
                    result.append(model_name)
            return result
        except Exception as e:
            log.error(f"Erreur r√©cup√©ration mod√®les install√©s: {e}")
            return []
    
    def get_model_status(self) -> Dict:
        """Retourne le statut de tous les mod√®les (install√©/non install√©)"""
        config = self.load_available_models()
        installed = self.get_installed_models()
        
        # Si pas de mod√®le actuel d√©fini, prendre celui par d√©faut s'il est install√©
        if not self.current_model and installed:
            default_model = config.get('config', {}).get('default_model')
            if default_model and default_model in installed:
                self.current_model = default_model
        
        status = {}
        for model_id, model_info in config['llm_models'].items():
            status[model_id] = {
                **model_info,
                'installed': model_id in installed,
                'current': model_id == self.current_model
            }
        
        return {
            'models': status,
            'current_model': self.current_model,
            'installed_count': len(installed)
        }
    
    def is_model_available(self, model_id: str) -> bool:
        """V√©rifie si un mod√®le est install√© dans Ollama"""
        installed = self.get_installed_models()
        return model_id in installed
    
    async def download_model(self, model_id: str, progress_callback: Optional[Callable] = None) -> bool:
        """
        T√©l√©charge un mod√®le avec Ollama (asynchrone)
        
        Args:
            model_id: ID du mod√®le √† t√©l√©charger
            progress_callback: Fonction appel√©e pour les updates de progression
        
        Returns:
            bool: Succ√®s du t√©l√©chargement
        """
        try:
            config = self.load_available_models()
            if model_id not in config['llm_models']:
                log.error(f"Mod√®le {model_id} non trouv√© dans la config")
                return False
            
            install_command = config['llm_models'][model_id].get('install_command')
            if not install_command:
                log.error(f"Commande d'installation manquante pour {model_id}")
                return False
            
            log.info(f"üì• D√©but t√©l√©chargement {model_id}...")
            if progress_callback:
                progress_callback({"status": "starting", "model": model_id})
            
            # Lancer ollama pull de mani√®re asynchrone
            process = await asyncio.create_subprocess_shell(
                install_command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            # Monitorer la progression (basique)
            while True:
                line = await process.stdout.readline()
                if not line:
                    break
                
                line_str = line.decode().strip()
                log.debug(f"Ollama pull: {line_str}")
                
                if progress_callback:
                    # Parsing basique de la progression d'Ollama
                    if "pulling" in line_str.lower():
                        progress_callback({
                            "status": "downloading", 
                            "model": model_id, 
                            "message": line_str
                        })
                    elif "verifying" in line_str.lower():
                        progress_callback({
                            "status": "verifying", 
                            "model": model_id, 
                            "message": line_str
                        })
            
            # Attendre la fin du processus
            await process.wait()
            
            if process.returncode == 0:
                log.success(f"‚úÖ Mod√®le {model_id} t√©l√©charg√© avec succ√®s")
                if progress_callback:
                    progress_callback({"status": "completed", "model": model_id})
                return True
            else:
                error_output = await process.stderr.read()
                log.error(f"‚ùå √âchec t√©l√©chargement {model_id}: {error_output.decode()}")
                if progress_callback:
                    progress_callback({
                        "status": "error", 
                        "model": model_id, 
                        "error": error_output.decode()
                    })
                return False
                
        except Exception as e:
            log.error(f"Erreur t√©l√©chargement {model_id}: {e}")
            if progress_callback:
                progress_callback({"status": "error", "model": model_id, "error": str(e)})
            return False
    
    def set_current_model(self, model_id: str) -> bool:
        """
        Change le mod√®le actuel (sans red√©marrer tout le syst√®me)
        
        Args:
            model_id: ID du nouveau mod√®le
            
        Returns:
            bool: Succ√®s du changement
        """
        try:
            # V√©rifier que le mod√®le est install√©
            if not self.is_model_available(model_id):
                log.warning(f"Mod√®le {model_id} non install√©, impossible de basculer")
                return False
            
            # Test rapide du mod√®le
            try:
                test_response = ollama.generate(
                    model=model_id,
                    prompt="Test",
                    options={"num_predict": 1}
                )
                if not test_response:
                    log.error(f"Test du mod√®le {model_id} √©chou√©")
                    return False
            except Exception as e:
                log.error(f"Erreur test mod√®le {model_id}: {e}")
                return False
            
            # Mettre √† jour le mod√®le actuel
            self.current_model = model_id
            log.success(f"‚úÖ Mod√®le bascul√© vers {model_id}")
            return True
            
        except Exception as e:
            log.error(f"Erreur changement mod√®le vers {model_id}: {e}")
            return False
    
    def get_current_model(self) -> Optional[str]:
        """Retourne le mod√®le actuellement utilis√©"""
        return self.current_model
    
    def update_llm_client_model(self, llm_client, model_id: str) -> bool:
        """
        Met √† jour le mod√®le d'un client LLM existant
        
        Args:
            llm_client: Instance de JarvisLLM
            model_id: Nouveau mod√®le √† utiliser
            
        Returns:
            bool: Succ√®s de la mise √† jour
        """
        try:
            if not self.is_model_available(model_id):
                log.warning(f"Mod√®le {model_id} non disponible")
                return False
            
            # Mettre √† jour le mod√®le dans le client LLM
            llm_client.model = model_id
            self.current_model = model_id
            
            log.success(f"‚úÖ Client LLM mis √† jour avec {model_id}")
            return True
            
        except Exception as e:
            log.error(f"Erreur mise √† jour client LLM: {e}")
            return False


# Test standalone
if __name__ == "__main__":
    import asyncio
    
    async def test_manager():
        manager = ModelManager()
        
        print("üìã Statut des mod√®les:")
        status = manager.get_model_status()
        for model_id, info in status['models'].items():
            installed = "‚úÖ" if info['installed'] else "‚ùå"
            print(f"  {installed} {model_id} - {info['display_name']}")
        
        print(f"\nüéØ Mod√®le actuel: {manager.get_current_model()}")
        
    asyncio.run(test_manager())

==================================================
FICHIER: .\cortex_prefrontal\__init__.py
==================================================

"""
cortex_prefrontal - Module de traitement LLM
"""

from .llm_client import JarvisLLM

__all__ = ['JarvisLLM']

==================================================
FICHIER: .\hypothalamus\config_api.py
==================================================

"""
config_api.py - API REST unifi√©e pour la configuration
üéØ Remplace tous les endpoints config s√©par√©s
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Dict, Any, Optional
from config_manager import config, get_config, update_config

# Router API
router = APIRouter(prefix="/api/config", tags=["config"])

# Mod√®les Pydantic
class ConfigUpdate(BaseModel):
    config: Dict[str, Any]

class VoiceConfig(BaseModel):
    personality: str
    tts_model: str
    edge_voice: Optional[str] = None
    sample_path: Optional[str] = None
    embedding_path: Optional[str] = None

class InterfaceConfig(BaseModel):
    theme: Optional[str] = None
    background: Optional[str] = None
    background_opacity: Optional[int] = None

class LLMConfig(BaseModel):
    model: Optional[str] = None
    temperature: Optional[float] = None
    role: Optional[str] = None

# === ENDPOINTS LECTURE ===

@router.get("/")
async def get_full_config():
    """Retourne la configuration compl√®te (remplace /api/config ancien)"""
    try:
        config = get_config()
        return {
            "success": True,
            "config": config
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lecture config: {e}")

@router.get("/voice")
async def get_voice_config():
    """Configuration voix (remplace /api/voice/current)"""
    try:
        voice_config = config.get_voice_config()
        return {
            "success": True,
            "voice_id": voice_config.get('personality'),
            "personality": voice_config.get('personality'),
            "tts_model": voice_config.get('tts_model'),
            "voice_config": voice_config
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur config voix: {e}")

@router.get("/interface")
async def get_interface_config():
    """Configuration interface"""
    try:
        interface_config = config.get_interface_config()
        return {
            "success": True,
            "config": interface_config
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur config interface: {e}")

@router.get("/llm")
async def get_llm_config():
    """Configuration LLM"""
    try:
        llm_config = config.get_llm_config()
        return {
            "success": True,
            "config": llm_config
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur config LLM: {e}")

# === ENDPOINTS √âCRITURE ===

@router.post("/update")
async def update_full_config(request: ConfigUpdate):
    """
    Mise √† jour g√©n√©rique (remplace WebSocket config_update)
    """
    try:
        success = update_config(request.config)
        if success:
            return {
                "success": True,
                "message": f"Configuration mise √† jour: {list(request.config.keys())}",
                "config": get_config()
            }
        else:
            raise HTTPException(status_code=400, detail="Erreur mise √† jour config")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur: {e}")

@router.post("/voice")
async def update_voice_config(voice_config: VoiceConfig):
    """Mise √† jour voix (remplace /api/voice/set-default)"""
    try:
        updates = {
            "voice": {
                "personality": voice_config.personality,
                "display_name": voice_config.personality,
                "tts_model": voice_config.tts_model,
                "edge_voice": voice_config.edge_voice,
                "sample_path": voice_config.sample_path,
                "embedding_path": voice_config.embedding_path
            }
        }
        
        success = update_config(updates)
        if success:
            return {
                "success": True,
                "message": f"Voix mise √† jour: {voice_config.personality}",
                "voice_name": voice_config.personality
            }
        else:
            raise HTTPException(status_code=400, detail="Erreur mise √† jour voix")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur: {e}")

@router.post("/interface")
async def update_interface_config(interface_config: InterfaceConfig):
    """Mise √† jour interface"""
    try:
        updates = {"interface": {}}
        
        if interface_config.theme:
            updates["interface"]["theme"] = interface_config.theme
        if interface_config.background:
            updates["interface"]["background"] = interface_config.background
        if interface_config.background_opacity is not None:
            updates["interface"]["background_opacity"] = interface_config.background_opacity
        
        success = update_config(updates)
        if success:
            return {
                "success": True,
                "message": "Interface mise √† jour",
                "config": config.get_interface_config()
            }
        else:
            raise HTTPException(status_code=400, detail="Erreur mise √† jour interface")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur: {e}")

@router.post("/llm")
async def update_llm_config(llm_config: LLMConfig):
    """Mise √† jour LLM"""
    try:
        updates = {"llm": {}}
        
        if llm_config.model:
            updates["llm"]["model"] = llm_config.model
        if llm_config.temperature is not None:
            updates["llm"]["temperature"] = llm_config.temperature
        if llm_config.role:
            updates["llm"]["role"] = llm_config.role
        
        success = update_config(updates)
        if success:
            return {
                "success": True,
                "message": "LLM mis √† jour",
                "config": config.get_llm_config()
            }
        else:
            raise HTTPException(status_code=400, detail="Erreur mise √† jour LLM")
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur: {e}")

# === ENDPOINTS DE COMPATIBILIT√â ===

@router.get("/backgrounds") 
async def get_backgrounds():
    """Compatibilit√© avec ancien /api/backgrounds"""
    # TODO: Scanner le dossier images/ ou utiliser une liste fixe
    return {
        "success": True,
        "backgrounds": [
            {"name": "Par d√©faut", "path": "default", "filename": None},
            {"name": "Jarvis", "path": "images/Jarvis.jpeg", "filename": "Jarvis.jpeg"},
            {"name": "One Piece", "path": "images/One_piece.jpg", "filename": "One_piece.jpg"},
            {"name": "Samatha", "path": "images/Samatha.jpeg", "filename": "Samatha.jpeg"}
        ]
    }

@router.get("/models")
async def get_models():
    """Compatibilit√© - retourne mod√®les disponibles"""
    # TODO: Scanner Ollama ou utiliser liste fixe
    return {
        "success": True,
        "models": ["llama3.1:8b", "qwen2.5:7b", "mistral:7b"]
    }

# Fonction d'initialisation
def register_config_api(app):
    """Enregistre l'API unifi√©e dans FastAPI"""
    app.include_router(router)
    print("‚úÖ API configuration unifi√©e enregistr√©e")

if __name__ == "__main__":
    print("‚úÖ API config unifi√©e pr√™te")


==================================================
FICHIER: .\hypothalamus\config_coordinator.py
==================================================

"""
config_coordinator.py - Coordinateur simplifi√©
üéØ Utilise SEULEMENT settings.yaml via ConfigManager
"""

import asyncio
from typing import Dict, Any
from hypothalamus.logger import log
from hypothalamus.config_manager import ConfigManager

config = ConfigManager()

class ConfigCoordinator:
    """
    Coordinateur de configuration simplifi√©
    Utilise SEULEMENT settings.yaml - AUCUN autre fichier
    """
    
    def __init__(self, conversation_flow=None):
        self.conversation_flow = conversation_flow
        log.info("üéØ ConfigCoordinator unifi√© initialis√©")
    
    async def update_config(self, new_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Mise √† jour de configuration - VERSION SIMPLIFI√âE
        """
        try:
            # 1. Validation simple
            validated_config = self._validate_config(new_config)
            
            # 2. Application voix si n√©cessaire
            if 'personality' in validated_config:
                await self._apply_voice_changes(validated_config)
            
            # 3. üöÄ SAUVEGARDE UNIFI√âE - Une seule ligne !
            success = config.update_config(validated_config)
            
            if success:
                return {
                    'success': True,
                    'message': f'Configuration mise √† jour: {list(validated_config.keys())}',
                    'config': config.get_config()
                }
            else:
                return {
                    'success': False,
                    'message': 'Erreur sauvegarde',
                    'config': config.get_config()
                }
                
        except Exception as e:
            log.error(f"‚ùå Erreur update_config: {e}")
            return {
                'success': False,
                'message': f'Erreur: {e}',
                'config': config.get_config()
            }
    
    def _validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Validation simple des param√®tres"""
        validated = {}
        
        # Mappage direct - plus de conversion compliqu√©e
        if 'personality' in config:
            validated['voice'] = {
                'personality': config['personality']
            }
        
        if 'voice_speed' in config:
            validated['audio'] = validated.get('audio', {})
            validated['audio']['output'] = validated['audio'].get('output', {})
            validated['audio']['output']['speed'] = float(config['voice_speed'])
        
        if 'voice_volume' in config:
            validated['audio'] = validated.get('audio', {})
            validated['audio']['output'] = validated['audio'].get('output', {})
            validated['audio']['output']['volume'] = int(config['voice_volume'])
        
        if 'llm_model' in config:
            validated['llm'] = validated.get('llm', {})
            validated['llm']['model'] = config['llm_model']
        
        if 'llm_temperature' in config:
            validated['llm'] = validated.get('llm', {})
            validated['llm']['temperature'] = float(config['llm_temperature'])
        
        if 'theme' in config:
            validated['interface'] = validated.get('interface', {})
            validated['interface']['theme'] = config['theme']
        
        if 'background' in config:
            validated['interface'] = validated.get('interface', {})
            validated['interface']['background'] = config['background']
        
        if 'background_opacity' in config:
            validated['interface'] = validated.get('interface', {})
            validated['interface']['background_opacity'] = int(config['background_opacity'])
        
        return validated
    
    async def _apply_voice_changes(self, config: Dict[str, Any]):
        """Application des changements de voix"""
        try:
            if not self.conversation_flow:
                log.warning("ConversationFlow non disponible")
                return
            
            voice_config = config.get('voice', {})
            personality = voice_config.get('personality')
            
            if personality:
                # Charger la nouvelle voix
                result = await self.conversation_flow.change_voice(personality)
                log.success(f"üîä Voix appliqu√©e: {result}")
                
        except Exception as e:
            log.error(f"‚ùå Erreur application voix: {e}")
    
    def get_current_config(self) -> Dict[str, Any]:
        """Retourne la configuration actuelle"""
        return config.get_config()
    
    def get_available_voices(self) -> Dict[str, Any]:
        """Retourne les voix disponibles"""
        # TODO: Interface avec voice_manager ou liste fixe
        return {
            "standard": {
                "Jarvis": {"name": "Jarvis", "model": "edge-tts"},
                "Samantha": {"name": "Samantha", "model": "edge-tts"},
                "Eloise": {"name": "Eloise", "model": "edge-tts"}
            },
            "cloned": {}  # TODO: Scanner dossier cloned_voices
        }

# Factory function pour remplacer l'ancienne classe
def create_config_coordinator(conversation_flow=None):
    """Cr√©e une instance du coordinateur unifi√©"""
    return ConfigCoordinator(conversation_flow)

if __name__ == "__main__":
    # Test
    coordinator = ConfigCoordinator()
    print("‚úÖ ConfigCoordinator unifi√© cr√©√©")

==================================================
FICHIER: .\hypothalamus\config_manager.py
==================================================

"""
config_manager.py - Gestionnaire unifi√© de configuration
üéØ Source unique de v√©rit√© : settings.yaml SEULEMENT
"""

import yaml
from pathlib import Path
from typing import Dict, Any, Optional
from hypothalamus.logger import log

class ConfigManager:
    """
    Gestionnaire unifi√© pour TOUTES les configurations
    """
    
    def __init__(self):
        self.settings_path = Path(__file__).parent.parent / "config/settings.yaml"
        self.config = {}
        self._load_config()
    
    def _load_config(self):
        """Charge la configuration depuis settings.yaml"""
        try:
            if self.settings_path.exists():
                with open(self.settings_path, 'r', encoding='utf-8') as f:
                    self.config = yaml.safe_load(f) or {}
                log.success("üìÑ Configuration unifi√©e charg√©e")
            else:
                self.config = self._get_default_config()
                self._save_config()
                log.info("üìÑ Configuration par d√©faut cr√©√©e")
                
        except Exception as e:
            log.error(f"‚ùå Erreur chargement config: {e}")
            self.config = self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Configuration par d√©faut"""
        return {
            'voice': {
                'personality': 'Samantha',
                'display_name': 'Samantha',
                'tts_model': 'edge-tts',
                'edge_voice': 'fr-FR-DeniseNeural',
                'sample_path': None,
                'embedding_path': None
            },
            'audio': {
                'input': {
                    'whisper_model': 'small',
                    'language': 'fr',
                    'vad_aggressiveness': 2,
                    'silence_duration': 1.5,
                    'timeout': 30,
                    'sensitivity': 5
                },
                'output': {
                    'speed': 1.0,
                    'volume': 90,
                    'device_index': None
                }
            },
            'llm': {
                'model': 'llama3.1:8b',
                'temperature': 0.7,
                'role': 'assistant_general'
            },
            'interface': {
                'theme': 'light',
                'background': 'default',
                'background_opacity': 30,
                'panels': {
                    'voice_lab_visible': False,
                    'camera_visible': False,
                    'debug_visible': False
                }
            },
            'system': {
                'language': 'fr',
                'log_level': 'DEBUG'
            }
        }
    
    def _save_config(self):
        """Sauvegarde la configuration"""
        try:
            with open(self.settings_path, 'w', encoding='utf-8') as f:
                yaml.dump(self.config, f, default_flow_style=False, allow_unicode=True)
            log.success("üíæ Configuration sauvegard√©e")
        except Exception as e:
            log.error(f"‚ùå Erreur sauvegarde: {e}")
    
    # === LECTURE ===
    def get_config(self) -> Dict[str, Any]:
        """Retourne la configuration compl√®te"""
        return self.config.copy()
    
    def get_voice_config(self) -> Dict[str, Any]:
        """Retourne la config voix"""
        return self.config.get('voice', {})
    
    def get_audio_config(self) -> Dict[str, Any]:
        """Retourne la config audio"""
        return self.config.get('audio', {})
    
    def get_llm_config(self) -> Dict[str, Any]:
        """Retourne la config LLM"""
        return self.config.get('llm', {})
    
    def get_interface_config(self) -> Dict[str, Any]:
        """Retourne la config interface"""
        return self.config.get('interface', {})
    
    def get(self, key_path: str, default=None):
        """
        R√©cup√®re une valeur par chemin (ex: 'voice.personality')
        """
        keys = key_path.split('.')
        value = self.config
        
        for key in keys:
            if isinstance(value, dict) and key in value:
                value = value[key]
            else:
                return default
        
        return value
    
    # === √âCRITURE ===
    def update_config(self, updates: Dict[str, Any]) -> bool:
        """
        Met √† jour la configuration avec les changements
        """
        try:
            self._deep_update(self.config, updates)
            self._save_config()
            log.info(f"‚úÖ Config mise √† jour: {list(updates.keys())}")
            return True
        except Exception as e:
            log.error(f"‚ùå Erreur mise √† jour: {e}")
            return False
    
    def set(self, key_path: str, value: Any) -> bool:
        """
        D√©finit une valeur par chemin (ex: 'voice.personality', 'Jarvis')
        """
        try:
            keys = key_path.split('.')
            config_ref = self.config
            
            # Naviguer jusqu'√† l'avant-derni√®re cl√©
            for key in keys[:-1]:
                if key not in config_ref:
                    config_ref[key] = {}
                config_ref = config_ref[key]
            
            # D√©finir la valeur finale
            config_ref[keys[-1]] = value
            self._save_config()
            log.debug(f"‚úÖ {key_path} = {value}")
            return True
            
        except Exception as e:
            log.error(f"‚ùå Erreur set {key_path}: {e}")
            return False
    
    def _deep_update(self, base_dict: dict, updates: dict):
        """Mise √† jour r√©cursive des dictionnaires"""
        for key, value in updates.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._deep_update(base_dict[key], value)
            else:
                base_dict[key] = value

# Instance globale
config = ConfigManager()

# API simplifi√©e pour compatibilit√©
def get_config() -> Dict[str, Any]:
    return config.get_config()

def update_config(updates: Dict[str, Any]) -> bool:
    return config.update_config(updates)

def get_voice_config() -> Dict[str, Any]:
    return config.get_voice_config()

def get_current_personality() -> str:
    return config.get('voice.personality', 'Samantha')

def save_voice_config(personality: str, tts_model: str, **kwargs):
    """Migration : remplace voice_manager.save_voice()"""
    voice_config = {
        'voice': {
            'personality': personality,
            'display_name': kwargs.get('display_name', personality),
            'tts_model': tts_model,
            'edge_voice': kwargs.get('edge_voice'),
            'sample_path': kwargs.get('sample_path'),
            'embedding_path': kwargs.get('embedding_path')
        }
    }
    return config.update_config(voice_config)

if __name__ == "__main__":
    # Test
    config_manager = ConfigManager()
    print("‚úÖ Configuration charg√©e:", config_manager.get_config())


==================================================
FICHIER: .\hypothalamus\device_manager.py
==================================================

"""
Device Manager - Gestion des p√©riph√©riques audio
D√©tecte et sauvegarde le meilleur microphone
"""

import pyaudio
import wave
import json
from pathlib import Path
import audioop

class DeviceManager:
    def __init__(self):
        self.config_file = Path("config/audio_device.json")
        self.config_file.parent.mkdir(exist_ok=True)
    
    def load_saved_device(self):
        """Charge le device sauvegard√©"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return config.get('device_index'), config.get('device_name')
        return None, None
    
    def save_device(self, device_index, device_name):
        """Sauvegarde le device choisi"""
        config = {
            'device_index': device_index,
            'device_name': device_name
        }
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Micro sauvegard√© : {device_name} (index {device_index})")
    
    def verify_device(self, device_index):
        """V√©rifie qu'un device existe toujours"""
        try:
            p = pyaudio.PyAudio()
            info = p.get_device_info_by_index(device_index)
            p.terminate()
            
            if info['maxInputChannels'] > 0:
                return True, info['name']
            return False, None
        except:
            return False, None
    
    def test_device(self, device_index, device_name):
        """
        Teste un device en enregistrant 3 secondes
        Retourne (success, volume_max)
        """
        CHUNK = 1024
        FORMAT = pyaudio.paInt16
        CHANNELS = 1
        RATE = 16000
        DURATION = 3
        
        print(f"\nüé§ Test : {device_name}")
        print(f"   Parle FORT pendant 3 secondes...")
        
        p = pyaudio.PyAudio()
        
        try:
            stream = p.open(
                format=FORMAT,
                channels=CHANNELS,
                rate=RATE,
                input=True,
                input_device_index=device_index,
                frames_per_buffer=CHUNK
            )
            
            frames = []
            for i in range(0, int(RATE / CHUNK * DURATION)):
                data = stream.read(CHUNK, exception_on_overflow=False)
                frames.append(data)
            
            stream.stop_stream()
            stream.close()
            
            # Analyser le volume
            volumes = [audioop.rms(frame, 2) for frame in frames]
            volume_max = max(volumes)
            volume_avg = sum(volumes) / len(volumes)
            
            print(f"   üìä Volume max: {volume_max}, moyen: {volume_avg:.0f}")
            
            # Consid√©rer comme fonctionnel si volume > 100
            if volume_max > 100:
                print(f"   ‚úÖ FONCTIONNE (volume suffisant)")
                return True, volume_max
            else:
                print(f"   ‚ö†Ô∏è  Trop silencieux")
                return False, volume_max
                
        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
            return False, 0
        finally:
            p.terminate()
    
    def find_best_microphone(self):
        """
        Scanne tous les devices et trouve le meilleur micro
        Retourne (device_index, device_name)
        """
        print("\n" + "="*60)
        print("üîç Recherche du meilleur microphone...")
        print("="*60)
        
        p = pyaudio.PyAudio()
        
        # Lister tous les devices d'entr√©e
        input_devices = []
        for i in range(p.get_device_count()):
            try:
                info = p.get_device_info_by_index(i)
                if info['maxInputChannels'] > 0:
                    input_devices.append((i, info['name']))
            except:
                pass
        
        p.terminate()
        
        if not input_devices:
            print("‚ùå Aucun microphone d√©tect√© !")
            return None, None
        
        print(f"\nüìã {len(input_devices)} microphone(s) d√©tect√©(s)")
        
        # Tester chaque device
        working_devices = []
        for device_index, device_name in input_devices:
            success, volume = self.test_device(device_index, device_name)
            if success:
                working_devices.append((device_index, device_name, volume))
        
        if not working_devices:
            print("\n‚ùå Aucun microphone ne fonctionne correctement !")
            print("V√©rifiez :")
            print("  - Que le micro est branch√©")
            print("  - Les permissions Windows")
            print("  - Que vous parlez assez fort")
            return None, None
        
        # Si un seul fonctionne, le choisir automatiquement
        if len(working_devices) == 1:
            device_index, device_name, volume = working_devices[0]
            print(f"\n‚úÖ Micro s√©lectionn√© automatiquement : {device_name}")
            return device_index, device_name
        
        # Si plusieurs fonctionnent, demander √† l'utilisateur
        print(f"\n‚úÖ {len(working_devices)} microphone(s) fonctionnel(s) :")
        for i, (idx, name, volume) in enumerate(working_devices, 1):
            print(f"   {i}. {name} (volume: {volume})")
        
        while True:
            try:
                choice = input(f"\nChoisis un micro (1-{len(working_devices)}) : ").strip()
                choice_idx = int(choice) - 1
                if 0 <= choice_idx < len(working_devices):
                    device_index, device_name, _ = working_devices[choice_idx]
                    return device_index, device_name
                else:
                    print(f"‚ùå Choix invalide (1-{len(working_devices)})")
            except (ValueError, KeyboardInterrupt):
                print("\n‚ùå Annul√©")
                return None, None
    
    def setup_microphone(self):
        """
        Configuration compl√®te du microphone
        Retourne device_index ou None
        """
        print("\nüéôÔ∏è  CONFIGURATION MICROPHONE")
        
        # V√©rifier si un device est d√©j√† sauvegard√©
        saved_index, saved_name = self.load_saved_device()
        
        if saved_index is not None:
            print(f"\nüìÅ Micro sauvegard√© : {saved_name} (index {saved_index})")
            
            # V√©rifier qu'il existe toujours
            exists, current_name = self.verify_device(saved_index)
            
            if exists:
                print(f"‚úÖ Le micro est toujours disponible")
                
                # Demander si on veut le garder ou refaire le test
                choice = input("Utiliser ce micro ? (O/n) : ").strip().lower()
                if choice in ['', 'o', 'oui', 'y', 'yes']:
                    return saved_index
            else:
                print(f"‚ö†Ô∏è  Le micro n'est plus disponible !")
        
        # Rechercher un nouveau micro
        device_index, device_name = self.find_best_microphone()
        
        if device_index is not None:
            self.save_device(device_index, device_name)
            return device_index
        
        return None

==================================================
FICHIER: .\hypothalamus\logger.py
==================================================

"""
Syst√®me de logs pour Jarvis
"""

from colorama import Fore, Style
import yaml
from pathlib import Path

# Charger config
config_path = Path("config/settings.yaml")
with open(config_path, 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

LOG_LEVEL = config['system'].get('log_level', 'STANDARD').upper()
LEVELS = {"STANDARD": 0, "INFO": 1, "DEBUG": 2}

class JarvisLogger:
    """Logger avec niveaux DEBUG/INFO/STANDARD"""
    
    @staticmethod
    def debug(message, prefix="üîç"):
        """Affiche uniquement en mode DEBUG"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["DEBUG"]:
            print(f"{Fore.CYAN}{prefix} [DEBUG] {message}{Style.RESET_ALL}")
    
    @staticmethod
    def info(message, prefix="‚ÑπÔ∏è"):
        """Affiche si niveau >= STANDARD pour les information de base"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["STANDARD"]:
            print(f"{Fore.WHITE}{prefix} {message}{Style.RESET_ALL}")
    
    @staticmethod
    def success(message, prefix="‚úÖ"):
        """Message de succ√®s"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["INFO"]:
            print(f"{Fore.GREEN}{prefix} {message}{Style.RESET_ALL}")
    
    @staticmethod
    def warning(message, prefix="‚ö†Ô∏è"):
        """Avertissement"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["INFO"]:
            print(f"{Fore.YELLOW}{prefix} {message}{Style.RESET_ALL}")
    
    @staticmethod
    def error(message, prefix="‚ùå"):
        """Erreur"""
        # Toujours afficher les erreurs, m√™me en mode STANDARD
        print(f"{Fore.RED}{prefix} {message}{Style.RESET_ALL}")
    
    @staticmethod
    def user(message):
        """Message utilisateur"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["STANDARD"]:
            print(f"{Fore.BLUE}üë§ Vous: {message}{Style.RESET_ALL}")
    
    @staticmethod
    def jarvis(message):
        """Message Jarvis"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["STANDARD"]:
            print(f"{Fore.MAGENTA}ü§ñ Jarvis: {message}{Style.RESET_ALL}")
    
    @staticmethod
    def thinking(message):
        """R√©flexion"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["DEBUG"]:
            print(f"{Fore.CYAN}üß† {message}{Style.RESET_ALL}")
    
    @staticmethod
    def separator():
        """S√©parateur visuel"""
        if LEVELS.get(LOG_LEVEL, 0) >= LEVELS["DEBUG"]:
            print(f"{Fore.CYAN}{'='*50}{Style.RESET_ALL}")

# Instance globale
log = JarvisLogger()


==================================================
FICHIER: .\hypothalamus\system_monitor.py
==================================================

"""
system_monitor.py - Surveillance syst√®me (Hypothalamus)
Responsabilit√© : Monitoring sant√©, m√©triques, √©tat du syst√®me
Nouveau module pour l'hypothalamus √©tendu
"""

import time
import psutil
import threading
from pathlib import Path
from typing import Dict, Any, Optional
import sys

# Import logger depuis hypothalamus
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

class SystemMonitor:
    """Moniteur syst√®me centralis√© (Hypothalamus)"""
    
    def __init__(self, monitoring_interval: float = 5.0):
        self.monitoring_interval = monitoring_interval
        self.is_monitoring = False
        self.monitor_thread: Optional[threading.Thread] = None
        
        # M√©triques syst√®me
        self.metrics = {
            'cpu_percent': 0.0,
            'memory_percent': 0.0,
            'memory_available_gb': 0.0,
            'disk_percent': 0.0,
            'temperature': None,
            'last_update': 0
        }
        
        # Seuils d'alerte
        self.thresholds = {
            'cpu_warning': 80.0,
            'cpu_critical': 95.0,
            'memory_warning': 85.0,
            'memory_critical': 95.0,
            'disk_warning': 90.0,
            'disk_critical': 95.0
        }
        
        # Historique pour tendances
        self.history = {
            'cpu': [],
            'memory': [],
            'timestamps': []
        }
        self.history_max_size = 100
        
        log.info("SystemMonitor initialis√© (Hypothalamus)")
    
    def start_monitoring(self):
        """D√©marre le monitoring en arri√®re-plan"""
        if self.is_monitoring:
            log.warning("Monitoring d√©j√† actif")
            return
        
        self.is_monitoring = True
        self.monitor_thread = threading.Thread(target=self._monitoring_loop, daemon=True)
        self.monitor_thread.start()
        
        log.success("Monitoring syst√®me d√©marr√©")
    
    def stop_monitoring(self):
        """Arr√™te le monitoring"""
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        
        log.info("Monitoring syst√®me arr√™t√©")
    
    def _monitoring_loop(self):
        """Boucle principale de monitoring"""
        while self.is_monitoring:
            try:
                self._update_metrics()
                self._check_thresholds()
                self._update_history()
                
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                log.error(f"Erreur monitoring: {e}")
                time.sleep(self.monitoring_interval * 2)  # Pause plus longue en cas d'erreur
    
    def _update_metrics(self):
        """Met √† jour les m√©triques syst√®me"""
        try:
            # CPU
            self.metrics['cpu_percent'] = psutil.cpu_percent(interval=1)
            
            # M√©moire
            memory = psutil.virtual_memory()
            self.metrics['memory_percent'] = memory.percent
            self.metrics['memory_available_gb'] = memory.available / (1024**3)
            
            # Disque (partition racine)
            disk = psutil.disk_usage('/')
            self.metrics['disk_percent'] = disk.percent
            
            # Temp√©rature (si disponible)
            try:
                temps = psutil.sensors_temperatures()
                if temps:
                    # Prendre la premi√®re temp√©rature disponible
                    first_sensor = list(temps.values())[0]
                    if first_sensor:
                        self.metrics['temperature'] = first_sensor[0].current
            except:
                self.metrics['temperature'] = None
            
            self.metrics['last_update'] = time.time()
            
        except Exception as e:
            log.error(f"Erreur mise √† jour m√©triques: {e}")
    
    def _check_thresholds(self):
        """V√©rifie les seuils et g√©n√®re des alertes"""
        current_time = time.time()
        
        # CPU
        cpu = self.metrics['cpu_percent']
        if cpu > self.thresholds['cpu_critical']:
            log.warning(f"üö® CPU critique: {cpu:.1f}%")
        elif cpu > self.thresholds['cpu_warning']:
            log.warning(f"‚ö†Ô∏è  CPU √©lev√©: {cpu:.1f}%")
        
        # M√©moire
        memory = self.metrics['memory_percent']
        if memory > self.thresholds['memory_critical']:
            log.warning(f"üö® M√©moire critique: {memory:.1f}%")
        elif memory > self.thresholds['memory_warning']:
            log.warning(f"‚ö†Ô∏è  M√©moire √©lev√©e: {memory:.1f}%")
        
        # Disque
        disk = self.metrics['disk_percent']
        if disk > self.thresholds['disk_critical']:
            log.warning(f"üö® Disque critique: {disk:.1f}%")
        elif disk > self.thresholds['disk_warning']:
            log.warning(f"‚ö†Ô∏è  Disque √©lev√©: {disk:.1f}%")
    
    def _update_history(self):
        """Met √† jour l'historique des m√©triques"""
        current_time = time.time()
        
        self.history['cpu'].append(self.metrics['cpu_percent'])
        self.history['memory'].append(self.metrics['memory_percent'])
        self.history['timestamps'].append(current_time)
        
        # Limiter la taille de l'historique
        if len(self.history['cpu']) > self.history_max_size:
            self.history['cpu'].pop(0)
            self.history['memory'].pop(0)
            self.history['timestamps'].pop(0)
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """Retourne les m√©triques actuelles"""
        return {
            'success': True,
            'metrics': self.metrics.copy(),
            'thresholds': self.thresholds.copy(),
            'monitoring_active': self.is_monitoring
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Retourne les informations syst√®me d√©taill√©es"""
        try:
            import platform
            
            # Informations de base
            uname = platform.uname()
            
            # Processeur
            cpu_info = {
                'physical_cores': psutil.cpu_count(logical=False),
                'logical_cores': psutil.cpu_count(logical=True),
                'max_frequency': f"{psutil.cpu_freq().max:.2f}Mhz" if psutil.cpu_freq() else "N/A",
                'current_frequency': f"{psutil.cpu_freq().current:.2f}Mhz" if psutil.cpu_freq() else "N/A"
            }
            
            # M√©moire
            memory = psutil.virtual_memory()
            memory_info = {
                'total_gb': round(memory.total / (1024**3), 2),
                'available_gb': round(memory.available / (1024**3), 2),
                'used_gb': round(memory.used / (1024**3), 2),
                'percentage': memory.percent
            }
            
            # Disque
            disk = psutil.disk_usage('/')
            disk_info = {
                'total_gb': round(disk.total / (1024**3), 2),
                'free_gb': round(disk.free / (1024**3), 2),
                'used_gb': round(disk.used / (1024**3), 2),
                'percentage': disk.percent
            }
            
            return {
                'success': True,
                'system': {
                    'platform': uname.system,
                    'platform_release': uname.release,
                    'platform_version': uname.version,
                    'architecture': uname.machine,
                    'hostname': uname.node,
                    'processor': uname.processor,
                    'python_version': platform.python_version()
                },
                'cpu': cpu_info,
                'memory': memory_info,
                'disk': disk_info,
                'boot_time': psutil.boot_time()
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_process_info(self, process_name: str = "python") -> Dict[str, Any]:
        """Retourne les informations sur les processus Jarvis/Python"""
        try:
            processes = []
            
            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent', 'cmdline']):
                try:
                    if process_name.lower() in proc.info['name'].lower():
                        # V√©rifier si c'est un processus Jarvis
                        cmdline = ' '.join(proc.info['cmdline'] or [])
                        if 'jarvis' in cmdline.lower():
                            processes.append({
                                'pid': proc.info['pid'],
                                'name': proc.info['name'],
                                'cpu_percent': proc.info['cpu_percent'],
                                'memory_percent': proc.info['memory_percent'],
                                'cmdline': cmdline[:100] + '...' if len(cmdline) > 100 else cmdline
                            })
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
            
            return {
                'success': True,
                'processes': processes,
                'count': len(processes)
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }
    
    def get_health_status(self) -> Dict[str, Any]:
        """Retourne le statut de sant√© global"""
        metrics = self.metrics
        
        # Calculer le score de sant√© (0-100)
        health_score = 100
        
        # P√©nalit√©s selon les m√©triques
        if metrics['cpu_percent'] > self.thresholds['cpu_warning']:
            health_score -= 20
        if metrics['memory_percent'] > self.thresholds['memory_warning']:
            health_score -= 20
        if metrics['disk_percent'] > self.thresholds['disk_warning']:
            health_score -= 10
        
        # Statut global
        if health_score >= 80:
            status = "Excellent"
            color = "green"
        elif health_score >= 60:
            status = "Bon"
            color = "yellow"
        elif health_score >= 40:
            status = "Moyen"
            color = "orange"
        else:
            status = "Probl√®me"
            color = "red"
        
        return {
            'success': True,
            'health_score': health_score,
            'status': status,
            'color': color,
            'metrics': metrics,
            'last_update': metrics['last_update']
        }

# Test standalone
if __name__ == "__main__":
    print("üß™ Test SystemMonitor (Hypothalamus)")
    
    monitor = SystemMonitor(monitoring_interval=2.0)
    
    try:
        # Test des m√©triques
        monitor._update_metrics()
        print(f"‚úÖ M√©triques: {monitor.get_current_metrics()}")
        
        # Test info syst√®me
        system_info = monitor.get_system_info()
        if system_info['success']:
            print(f"‚úÖ Syst√®me: {system_info['system']['platform']} {system_info['system']['architecture']}")
            print(f"‚úÖ CPU: {system_info['cpu']['logical_cores']} cores")
            print(f"‚úÖ RAM: {system_info['memory']['total_gb']}GB")
        
        # Test health
        health = monitor.get_health_status()
        print(f"‚úÖ Sant√©: {health['status']} ({health['health_score']}/100)")
        
        # Test monitoring (court)
        print("üîÑ Test monitoring 5s...")
        monitor.start_monitoring()
        time.sleep(5)
        monitor.stop_monitoring()
        
        print("‚úÖ Test SystemMonitor termin√©")
        
    except Exception as e:
        print(f"‚ùå Erreur test: {e}")

==================================================
FICHIER: .\hypothalamus\voice_manager.py
==================================================

"""
Voice Manager - Gestion des voix TTS
VERSION CORRIG√âE ET COMPL√àTE
"""

import json
from pathlib import Path

import warnings
# Petit probl√®me de futur incompatibilit√©. On va enlever le warning qui sert √† rien (vu qu'on est en librairie fixe)
warnings.filterwarnings("ignore", category=UserWarning, module='jieba')

from TTS.api import TTS

# Ajoutez ces deux imports
import torch.serialization

class VoiceManager:
    def __init__(self):
        self.config_file = Path("config/voice_config.json")
        self.voices_json = Path("config/voices.json")
        self.config_file.parent.mkdir(exist_ok=True)
        
        # Charger toutes les voix depuis voices.json
        self.available_voices = self._load_all_voices()
        
    def _load_all_voices(self):
        """Charge les voix standard + clon√©es depuis voices.json"""
        voices = {}
        
        try:
            if self.voices_json.exists():
                with open(self.voices_json, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Voix standard (Edge-TTS + Coqui)
                idx = 1
                for voice_id, voice_data in data.get('voices', {}).items():
                    voices[str(idx)] = {
                        "name": voice_data.get('display_name', voice_data['name']),
                        "model": voice_data['model'],
                        "voice": voice_data.get('edge_voice'),
                        "edge_voice": voice_data.get('edge_voice'),
                        "personality": voice_data['name'],
                        "gender": voice_data.get('gender', 'unknown'),
                        "description": voice_data.get('description', ''),
                        "voice_id": voice_id,
                        "type": "standard"
                    }
                    idx += 1
                
                # Voix clon√©es (XTTS) - AVEC NORMALISATION DES CHEMINS
                for voice_id, voice_data in data.get('cloned_voices', {}).items():
                    if voice_data.get('processing_status') == 'ready':
                        voices[str(idx)] = {
                            "name": voice_data.get('display_name', voice_data['name']),
                            "model": "xtts-v2",
                            # ‚ö° NORMALISATION des chemins en format Unix
                            "sample_path": Path(voice_data['sample_path']).as_posix(),
                            "embedding_path": Path(voice_data.get('embedding_path', '')).as_posix() if voice_data.get('embedding_path') else None,
                            "personality": voice_data['name'],
                            "gender": voice_data.get('gender', 'unknown'),
                            "description": voice_data.get('description', ''),
                            "voice_id": voice_id,
                            "type": "cloned"
                        }
                        idx += 1
            
            # Fallback si voices.json n'existe pas
            if not voices:
                voices = self._get_default_voices()
                
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur chargement voices.json: {e}")
            voices = self._get_default_voices()
        
        return voices

    def _get_default_voices(self):
        """Voix par d√©faut si voices.json absent"""
        return {
            "1": {
                "name": "Jarvis (Homme - Fran√ßais)",
                "model": "tts_models/fr/css10/vits",
                "personality": "Jarvis",
                "gender": "male",
                "description": "Voix masculine fran√ßaise, style assistant",
                "type": "standard"
            },
            "2": {
                "name": "Samantha (Femme - Fran√ßais)",
                "model": "edge-tts",
                "voice": "fr-FR-DeniseNeural",
                "edge_voice": "fr-FR-DeniseNeural",
                "personality": "Samantha",
                "gender": "female",
                "description": "Voix f√©minine fran√ßaise, chaleureuse",
                "type": "standard"
            },
            "3": {
                "name": "Eloise (jeune fille- Edge)",
                "model": "edge-tts",
                "edge_voice": "fr-FR-EloiseNeural", 
                "personality": "Eloise",
                "gender": "female", 
                "description": "Voix f√©minine jeune et dynamique"
            },
            "4": {
                "name": "Josephine (jeune femme - Edge)",
                "model": "edge-tts",
                "edge_voice": "fr-FR-JosephineNeural", 
                "personality": "Josephine",
                "gender": "female", 
                "description": "Voix f√©minine jeune et dynamique"
            }
        }
    
    def load_saved_voice(self):
        """Charge la voix sauvegard√©e"""
        if self.config_file.exists():
            with open(self.config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                return (
                    config.get('voice_id'), 
                    config.get('personality'), 
                    config.get('model'), 
                    config.get('edge_voice'),
                    config.get('sample_path'),
                    config.get('embedding_path')  # AJOUT du embedding_path
                )
        return None, None, None, None, None, None
    
    def save_voice(self, voice_id, personality, model, edge_voice, sample_path=None, embedding_path=None):
        """Sauvegarde le choix de voix avec normalisation des chemins"""
        config = {
            'voice_id': voice_id,
            'personality': personality,
            'model': model,
            'edge_voice': edge_voice,
            # ‚ö° NORMALISATION des chemins en format Unix
            'sample_path': Path(sample_path).as_posix() if sample_path else None,
            'embedding_path': Path(embedding_path).as_posix() if embedding_path else None
        }
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Voix sauvegard√©e : {personality}")
    
    def get_current_personality(self):
        """Retourne la personnalit√© actuellement sauvegard√©e"""
        _, personality, _, _, _, _ = self.load_saved_voice()
        return personality or "Jarvis"
    
    def select_voice(self):
        """
        S√©lection de la voix par l'utilisateur
        Retourne (personality, model, edge_voice, sample_path, embedding_path)
        """
        print("\n" + "="*60)
        print("üé§ CONFIGURATION VOIX")
        print("="*60)
        
        # V√©rifier si une voix est d√©j√† sauvegard√©e
        saved_id, saved_personality, saved_model, edge_voice, sample_path, embedding_path = self.load_saved_voice()
        
        if saved_id and saved_id in self.available_voices:
            voice_info = self.available_voices[saved_id]
            print(f"\nüîç Voix sauvegard√©e : {voice_info['name']}")
            print(f"   Personnalit√© : {saved_personality}")
            
            choice = input("Utiliser cette voix ? (O/n) : ").strip().lower()
            if choice in ['', 'o', 'oui', 'y', 'yes']:
                # Retourner avec les paths des embeddings si disponibles
                return (saved_personality, saved_model, edge_voice, sample_path, embedding_path)
        
        # Afficher les voix disponibles
        print("\nüéôÔ∏è  Voix disponibles :\n")
        for voice_id, info in self.available_voices.items():
            gender_icon = "üë®" if info['gender'] == 'male' else "üë©"
            type_icon = "üé≠" if info['type'] == 'cloned' else "üé§"
            print(f"{voice_id}. {gender_icon} {type_icon} {info['name']}")
            print(f"   {info['description']}")
            if info.get('embedding_path'):
                print(f"   ‚ö° Embeddings optimis√©s disponibles")
            print()
        
        # Choix utilisateur
        while True:
            try:
                choice = input(f"Choisis une voix (1-{len(self.available_voices)}) : ").strip()
                
                if choice in self.available_voices:
                    voice_info = self.available_voices[choice]
                    personality = voice_info['personality']
                    model = voice_info['model']
                    edge_voice = voice_info.get('edge_voice')
                    sample_path = voice_info.get('sample_path')
                    embedding_path = voice_info.get('embedding_path')
                    
                    print(f"\n‚úÖ Voix s√©lectionn√©e : {voice_info['name']}")
                    print(f"   Personnalit√© : {personality}")
                    if embedding_path:
                        print(f"   ‚ö° Avec embeddings optimis√©s")
                    print(f"   T√©l√©chargement du mod√®le si n√©cessaire...")
                    
                    # Sauvegarder avec embedding_path
                    self.save_voice(choice, personality, model, edge_voice, sample_path, embedding_path)
                    
                    return personality, model, edge_voice, sample_path, embedding_path
                else:
                    print(f"‚ùå Choix invalide (1-{len(self.available_voices)})")
                    
            except (ValueError, KeyboardInterrupt):
                print("\n‚ùå Annul√©")
                return None, None, None, None, None
    
    def get_voice_by_id(self, voice_id):
        """Retourne les infos d'une voix par son ID"""
        return self.available_voices.get(voice_id)
    
    def get_voice_by_personality(self, personality):
        """Retourne les infos d'une voix par sa personnalit√©"""
        for voice_id, voice_info in self.available_voices.items():
            if voice_info['personality'] == personality:
                return voice_info
        return None

==================================================
FICHIER: .\hypothalamus\__init__.py
==================================================

"""
hypothalamus - Centre de contr√¥le syst√®me Jarvis √âTENDU
Gestion syst√®me, configuration, monitoring, voix et p√©riph√©riques
"""

from .config_coordinator import ConfigCoordinator
from .device_manager import DeviceManager
from .logger import log
from .system_monitor import SystemMonitor
from .voice_manager import VoiceManager

__all__ = ['log', 'DeviceManager', 'VoiceManager', 'ConfigCoordinator', 'SystemMonitor']

==================================================
FICHIER: .\lobes_temporaux\audio_generator.py
==================================================

"""
audio_generator.py - Moteurs de synth√®se audio purs
Optimisation embeddings XTTS pr√©-calcul√©s
VERSION CORRIG√âE ET COMPL√àTE
"""

import os
import tempfile
import asyncio
import time
import traceback
from pathlib import Path
from typing import Optional, Dict, Any, Union

# Import logger
import sys
sys.path.append(str(Path(__file__).parent.parent))

try:
    from hypothalamus.logger import log
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO)
    log = logging.getLogger(__name__)


class AudioGenerator:
    """
    Moteurs de synth√®se audio purs - Interface unifi√©e
    Optimisation embeddings XTTS
    """
    
    def __init__(self):
        """Initialise les moteurs disponibles"""
        self.xtts_model = None
        self.xtts_loaded = False
        self.coqui_models = {}  # Cache des mod√®les Coqui
    
        # üöÄ Cache des embeddings optimis√©s
        self.xtts_embeddings_cache = {
            'gpt_cond_latent': None,
            'speaker_embedding': None,
            'sample_path': None  # Pour v√©rifier si on a les bons embeddings
        }

        log.info("AudioGenerator initialis√©")
    
    async def generate_audio(
        self, 
        text: str, 
        voice_config: Dict[str, Any]
    ) -> Optional[bytes]:
        """
        Interface unifi√©e de g√©n√©ration audio
        
        Args:
            text: Texte √† synth√©tiser
            voice_config: Configuration voix avec model, params, etc.
            
        Returns:
            bytes: Donn√©es audio WAV ou None si √©chec
        """
        model = voice_config.get('model', 'edge-tts')
        
        try:
            if model == 'edge-tts':
                return await self._generate_edge_tts(text, voice_config)
            elif model == 'xtts-v2':
                return await self._generate_xtts(text, voice_config)
            elif model.startswith('tts_models/'):  # Coqui
                return await self._generate_coqui(text, voice_config)
            else:
                log.error(f"Mod√®le non support√©: {model}")
                return None
                
        except Exception as e:
            log.error(f"Erreur g√©n√©ration audio ({model}): {e}")
            return None
    
    async def _generate_edge_tts(
        self, 
        text: str, 
        voice_config: Dict[str, Any]
    ) -> Optional[bytes]:
        """G√©n√©ration Edge-TTS directe en m√©moire (sans fichier temporaire)"""
        try:
            import edge_tts
            
            # Configuration voix
            edge_voice = voice_config.get('edge_voice', 'fr-FR-DeniseNeural')
            
            # Calcul vitesse (de voice_speed vers format Edge-TTS)
            voice_speed = voice_config.get('personality_config', {}).get('voice_speed', 1.0)
            rate = f"{int((voice_speed - 1) * 100):+d}%"
            
            log.debug(f"Edge-TTS: {edge_voice}, rate: {rate}")
            
            # Cr√©er communication
            communicate = edge_tts.Communicate(text, edge_voice, rate=rate)
            
            # Stream directement en m√©moire (AUCUN fichier temporaire)
            audio_data = b""
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    audio_data += chunk["data"]
            
            log.debug(f"‚úÖ Edge-TTS g√©n√©r√©: {len(audio_data)} bytes (direct)")
            return audio_data
            
        except Exception as e:
            log.error(f"Erreur Edge-TTS: {e}")
            return None

    async def preload_xtts_embeddings(self, voice_config: Dict[str, Any]):
        """Pr√©-charge les embeddings XTTS pour optimisation"""
        try:
            sample_path = voice_config.get('sample_path')
            embedding_path = voice_config.get('embedding_path')
            
            if not embedding_path or not sample_path:
                log.debug("üîß Pas d'embeddings √† pr√©-charger")
                return
            
            # ‚ö° NORMALISER les chemins (fix Windows/Linux)
            sample_path = Path(sample_path).as_posix()
            embedding_path = Path(embedding_path).as_posix()
            
            # Construire chemins absolus
            config_dir = Path(__file__).parent.parent / "config"
            
            if not Path(embedding_path).is_absolute():
                embedding_abs = config_dir / embedding_path
            else:
                embedding_abs = Path(embedding_path)
            
            if not Path(sample_path).is_absolute():
                sample_abs = config_dir / sample_path
            else:
                sample_abs = Path(sample_path)
            
            if not embedding_abs.exists():
                log.warning(f"üîß Embeddings non trouv√©s: {embedding_abs}")
                return
            
            # Charger les embeddings
            import torch
            embedding_data = torch.load(str(embedding_abs))
            
            # üöÄ Stocker dans le cache avec chemins normalis√©s
            self.xtts_embeddings_cache = {
                'gpt_cond_latent': embedding_data.get('gpt_cond_latent'),
                'speaker_embedding': embedding_data.get('speaker_embedding'), 
                'sample_path': sample_path  # Chemin normalis√©
            }
            
            log.success(f"‚ö° Embeddings XTTS pr√©-charg√©s: {embedding_abs.name}")
            log.debug(f"   Cache sample_path: {sample_path}")
            
        except Exception as e:
            log.warning(f"Erreur pr√©-chargement embeddings: {e}")
            # Reset cache en cas d'erreur
            self.xtts_embeddings_cache = {
                'gpt_cond_latent': None, 
                'speaker_embedding': None, 
                'sample_path': None
            }
            
    async def _generate_xtts(self, text: str, voice_config: Dict[str, Any]) -> Optional[bytes]:
        """G√©n√©ration XTTS directe en m√©moire avec embeddings optimis√©s"""
        try:
            # ‚úÖ Initialisation XTTS si n√©cessaire
            if not await self._init_xtts():
                log.error("XTTS non disponible")
                return None

            # ‚úÖ R√©cup√©rer et normaliser les chemins
            sample_path = voice_config.get("sample_path")
            if not sample_path:
                log.error("sample_path manquant pour voix XTTS")
                return None

            # Normaliser le chemin
            sample_path = Path(sample_path).as_posix()
            
            # ‚úÖ Construire chemin complet si relatif
            if not Path(sample_path).is_absolute():
                config_dir = Path(__file__).parent.parent / "config"
                sample_path = config_dir / sample_path
            
            if not Path(sample_path).exists():
                log.error(f"√âchantillon audio non trouv√©: {sample_path}")
                return None

            log.debug(f"üé§ G√©n√©ration XTTS avec {sample_path}")

            # üöÄ Utiliser embeddings du cache si disponibles
            cache = self.xtts_embeddings_cache
            sample_path_normalized = Path(sample_path).as_posix()

            if (cache['gpt_cond_latent'] is not None and 
                cache['speaker_embedding'] is not None and 
                cache['sample_path'] == sample_path_normalized):
                
                log.debug("‚ö° Utilisation embeddings cach√©s")
                
                # G√©n√©ration avec embeddings pr√©-calcul√©s
                try:
                    # Cr√©er fichier temporaire pour l'audio
                    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
                        tmp_path = tmp.name
                    
                    # G√©n√©rer avec embeddings
                    wav = self.xtts_model.synthesizer.tts(
                        text=text.strip(),
                        language_name="fr",
                        gpt_cond_latent=cache['gpt_cond_latent'],
                        speaker_embedding=cache['speaker_embedding'],
                        temperature=0.7,
                        length_penalty=1.0,
                        repetition_penalty=2.0,
                        top_k=50,
                        top_p=0.85
                    )
                    
                    # Convertir et sauver
                    import numpy as np
                    from scipy.io import wavfile
                    
                    if hasattr(wav, 'cpu'):
                        wav = wav.cpu().numpy()
                    
                    if isinstance(wav, (list, np.ndarray)):
                        wav = np.array(wav)
                        if wav.dtype == np.float32 or wav.dtype == np.float64:
                            wav = (wav * 32767).astype(np.int16)
                    
                    wavfile.write(tmp_path, 22050, wav)
                    
                    # Lire en bytes
                    with open(tmp_path, 'rb') as f:
                        audio_data = f.read()
                    
                    # Nettoyer
                    os.unlink(tmp_path)
                    
                except Exception as e:
                    log.warning(f"√âchec m√©thode embeddings optimis√©e: {e}")
                    # Fallback sur m√©thode standard
                    audio_data = await self._generate_xtts_standard(text, sample_path)
            else:
                log.debug("üêå XTTS standard (pas d'embeddings cach√©s)")
                audio_data = await self._generate_xtts_standard(text, sample_path)
            
            if audio_data:
                log.debug(f"‚úÖ XTTS g√©n√©r√© ({len(audio_data)} bytes)")
            
            return audio_data
            
        except Exception as e:
            log.error(f"Erreur XTTS: {e}")
            import traceback
            log.debug(traceback.format_exc())
            return None
    
    async def _generate_xtts_standard(self, text: str, sample_path) -> Optional[bytes]:
        """G√©n√©ration XTTS standard avec fichier audio"""
        try:
            # G√©n√©ration standard avec fichier temporaire
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
                tmp_path = tmp.name
            
            self.xtts_model.tts_to_file(
                text=text.strip(),
                language="fr",
                speaker_wav=str(sample_path),
                file_path=tmp_path
            )
            
            # Lire le fichier g√©n√©r√©
            with open(tmp_path, 'rb') as f:
                audio_data = f.read()
            
            # Nettoyer
            os.unlink(tmp_path)
            
            return audio_data
            
        except Exception as e:
            log.error(f"Erreur g√©n√©ration XTTS standard: {e}")
            return None
    
    async def _generate_coqui(
        self, 
        text: str, 
        voice_config: Dict[str, Any]
    ) -> Optional[bytes]:
        """G√©n√©ration Coqui TTS locale"""
        try:
            model_name = voice_config.get('model', 'tts_models/fr/css10/vits')
            
            # Charger mod√®le si pas en cache
            if model_name not in self.coqui_models:
                log.debug(f"Chargement mod√®le Coqui: {model_name}")
                from TTS.api import TTS
                self.coqui_models[model_name] = TTS(model_name)
            
            model = self.coqui_models[model_name]
            
            # G√©n√©rer avec fichier temporaire
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp:
                tmp_path = tmp.name
            
            model.tts_to_file(
                text=text,
                file_path=tmp_path
            )
            
            # Lire et retourner les bytes
            with open(tmp_path, 'rb') as f:
                audio_data = f.read()
            
            # Nettoyer
            os.unlink(tmp_path)
            
            log.debug(f"‚úÖ Coqui g√©n√©r√©: {len(audio_data)} bytes")
            return audio_data
            
        except Exception as e:
            log.error(f"Erreur Coqui TTS: {e}")
            return None
    
    async def _init_xtts(self):
        """Initialise XTTS avec optimisations maximales"""
        if self.xtts_loaded:
            return True
            
        try:
            from TTS.api import TTS
            import torch
            
            log.info("‚è≥ Chargement du mod√®le XTTS...")
            
            # D√©tection du device optimal
            if torch.cuda.is_available():
                device = "cuda"
                log.info("üéÆ CUDA disponible - utilisation du GPU")
            else:
                device = "cpu"
                # Sur CPU, limiter les threads pour √©viter la surcharge
                torch.set_num_threads(4)
                log.info("üíª Utilisation du CPU (4 threads)")
            
            # Charger le mod√®le
            self.xtts_model = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
            
            # Optimisations si sur GPU
            if device == "cuda":
                self.xtts_model = self.xtts_model.to(device)
            
            # Mode √©valuation (d√©sactive dropout, batch norm, etc.)
            if hasattr(self.xtts_model, 'synthesizer'):
                if hasattr(self.xtts_model.synthesizer, 'tts_model'):
                    self.xtts_model.synthesizer.tts_model.eval()
            
            # Warm-up du mod√®le
            try:
                log.debug("üî• Warm-up du mod√®le XTTS...")
                # Cr√©er un sample audio temporaire pour le warm-up
                import tempfile
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=True) as tmp:
                    # Cr√©er un fichier WAV minimal
                    import wave
                    import struct
                    
                    with wave.open(tmp.name, 'wb') as wav_file:
                        wav_file.setnchannels(1)  # Mono
                        wav_file.setsampwidth(2)   # 16 bits
                        wav_file.setframerate(22050)  # 22kHz
                        # G√©n√©rer 1 seconde de silence
                        for _ in range(22050):
                            wav_file.writeframes(struct.pack('h', 0))
                    
                    # Warm-up avec ce fichier
                    _ = self.xtts_model.tts(
                        text="Test",
                        language="fr",
                        speaker_wav=tmp.name
                    )
                log.debug("‚úÖ Warm-up termin√©")
            except Exception as e:
                log.warning(f"Warm-up √©chou√© (non critique): {e}")
            
            self.xtts_loaded = True
            log.success(f"‚úÖ XTTS initialis√© sur {device}")
            return True
            
        except Exception as e:
            log.error(f"Impossible de charger XTTS: {e}")
            self.xtts_loaded = False
            return False
    
    def cleanup(self):
        """Nettoyage des ressources"""
        try:
            # Lib√©rer mod√®les Coqui
            self.coqui_models.clear()
            
            # Lib√©rer XTTS
            if self.xtts_model:
                del self.xtts_model
                self.xtts_model = None
                self.xtts_loaded = False
            
            # Vider cache embeddings
            self.xtts_embeddings_cache = {
                'gpt_cond_latent': None,
                'speaker_embedding': None,
                'sample_path': None
            }
            
            log.info("AudioGenerator nettoy√©")
            
        except Exception as e:
            log.warning(f"Erreur nettoyage AudioGenerator: {e}")

==================================================
FICHIER: .\lobes_temporaux\audio_pipeline.py
==================================================

"""
audio_pipeline.py - Pipeline de streaming audio avec workers intelligents
VERSION CORRIG√âE ET COMPL√àTE
"""

import asyncio
import tempfile
import time
import psutil
import threading
from pathlib import Path
from typing import Optional, Dict, Any
from dataclasses import dataclass

# Import logger
import sys
sys.path.append(str(Path(__file__).parent.parent))

try:
    from hypothalamus.logger import log
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO)
    log = logging.getLogger(__name__)


@dataclass
class AudioChunk:
    """Repr√©sente un chunk audio avec m√©tadonn√©es pour le pipeline"""
    text: str
    audio_path: Optional[str] = None
    audio_data: Optional[bytes] = None 
    is_generated: bool = False
    is_played: bool = False
    generation_time: float = 0.0
    play_start_time: float = 0.0
    chunk_id: int = 0


class AudioPipeline:
    """
    Pipeline de streaming audio intelligent avec workers parall√®les
    VERSION CORRIG√âE
    """
    
    def __init__(self, audio_generator, voice_config):
        """
        Initialise le pipeline avec g√©n√©rateur audio et config voix
        
        Args:
            audio_generator: Instance d'AudioGenerator
            voice_config: Configuration voix pour g√©n√©ration
        """
        self.audio_generator = audio_generator
        self.voice_config = voice_config
        
        # Files d'attente pour streaming
        self.text_chunks_queue = asyncio.Queue(maxsize=50)
        self.audio_ready_queue = asyncio.Queue(maxsize=10)
        
        # √âtat du pipeline
        self.pipeline_active = False
        self.chunk_counter = 0
        
        # Statistiques
        self.stats = {
            'chunks_generated': 0,
            'chunks_played': 0,
            'total_generation_time': 0.0,
            'total_playback_time': 0.0,
            'conversations_handled': 0,
            'pipeline_efficiency': 0.0
        }
        
        # √âtat de connectivit√© (pour Edge-TTS)
        self.edge_warmed_up = False
        
        log.info("AudioPipeline initialis√©")
    
    def start_streaming_workers(self):
        """D√©marre les workers de streaming en arri√®re-plan"""
        if self.pipeline_active:
            log.debug("Pipeline d√©j√† actif")
            return
        
        self.pipeline_active = True
        
        # Optimiser les priorit√©s processus si n√©cessaire
        self._optimize_process_priorities()
        
        # Pr√©-initialiser pygame
        self._preinit_pygame()
        
        # ‚ö° PR√â-CHARGER EMBEDDINGS XTTS - CORRECTION ICI
        if self.voice_config.get('model') == 'xtts-v2' and self.voice_config.get('embedding_path'):
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    # Appel CORRECT √† la m√©thode de audio_generator
                    loop.create_task(self.audio_generator.preload_xtts_embeddings(self.voice_config))
                    log.debug("‚ö° Pr√©-chargement embeddings XTTS lanc√©")
            except Exception as e:
                log.warning(f"Pr√©-chargement embeddings impossible: {e}")
        
        # D√©marrer workers
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                loop.create_task(self._generation_worker())
                loop.create_task(self._playback_worker())
                log.success("Workers de streaming d√©marr√©s")
            else:
                raise RuntimeError("Pas de loop active")
        except RuntimeError:
            # D√©marrer dans thread s√©par√©
            def start_workers_thread():
                worker_loop = asyncio.new_event_loop()
                asyncio.set_event_loop(worker_loop)
                worker_loop.create_task(self._generation_worker())
                worker_loop.create_task(self._playback_worker())
                worker_loop.run_forever()
            
            worker_thread = threading.Thread(target=start_workers_thread, daemon=True)
            worker_thread.start()
            log.success("Workers d√©marr√©s en thread s√©par√©")
        
        # Warm-up pour Edge-TTS si applicable
        if self.voice_config.get('model') == 'edge-tts':
            asyncio.create_task(self._warm_up_edge_tts())
    
    async def queue_text_chunk(self, text: str) -> int:
        """
        Ajoute un chunk de texte √† traiter
        
        Returns:
            chunk_id: ID du chunk pour suivi
        """
        if not self.pipeline_active:
            self.start_streaming_workers()
        
        self.chunk_counter += 1
        chunk = AudioChunk(
            text=text,
            chunk_id=self.chunk_counter
        )
        
        await self.text_chunks_queue.put(chunk)
        log.debug(f"Chunk #{chunk.chunk_id} en queue: {text[:30]}...")
        
        return chunk.chunk_id
    
    async def queue_text_chunks(self, texts: list) -> list:
        """
        Queue plusieurs chunks en s√©rie
        
        Returns:
            List des chunk_ids
        """
        chunk_ids = []
        for text in texts:
            chunk_id = await self.queue_text_chunk(text)
            chunk_ids.append(chunk_id)
        
        return chunk_ids
    
    async def _generation_worker(self):
        """Worker de g√©n√©ration audio parall√®le"""
        log.debug("üéµ Worker g√©n√©ration d√©marr√©")
        
        while self.pipeline_active:
            try:
                # Attendre chunk √† traiter
                chunk = await asyncio.wait_for(
                    self.text_chunks_queue.get(),
                    timeout=30.0
                )
                
                if chunk is None:  # Signal d'arr√™t
                    break
                
                # G√©n√©rer audio
                await self._generate_chunk_audio(chunk)
                
                # Envoyer vers lecture si succ√®s
                if chunk.is_generated:
                    await self.audio_ready_queue.put(chunk)
                else:
                    log.warning(f"Chunk #{chunk.chunk_id} ignor√© (g√©n√©ration √©chou√©e)")
                
            except asyncio.TimeoutError:
                # Pas de nouveau chunk depuis 30s
                log.debug("Worker g√©n√©ration en attente...")
            except Exception as e:
                log.error(f"Erreur worker g√©n√©ration: {e}")
        
        log.debug("üéµ Worker g√©n√©ration arr√™t√©")
    
    async def _playback_worker(self):
        """Worker de lecture audio s√©quentielle"""
        log.debug("üîä Worker lecture d√©marr√©")
        
        while self.pipeline_active:
            try:
                # Attendre chunk pr√™t
                chunk = await asyncio.wait_for(
                    self.audio_ready_queue.get(),
                    timeout=30.0
                )
                
                if chunk is None:  # Signal d'arr√™t
                    break
                
                # Lire audio
                await self._play_chunk_audio(chunk)
                
            except asyncio.TimeoutError:
                # Pas de nouveau chunk depuis 30s
                log.debug("Worker lecture en attente...")
            except Exception as e:
                log.error(f"Erreur worker lecture: {e}")
        
        log.debug("üîä Worker lecture arr√™t√©")
    
    async def _generate_chunk_audio(self, chunk: AudioChunk):
        """G√©n√®re l'audio pour un chunk"""
        start_time = time.time()
        
        log.debug(f"üîç [VOICE DEBUG] Config utilis√©e: {self.voice_config.get('model')} - {self.voice_config.get('personality', 'inconnu')}")
        if 'edge_voice' in self.voice_config:
            log.debug(f"üîç [VOICE DEBUG] Edge voice: {self.voice_config.get('edge_voice')}")
        if 'sample_path' in self.voice_config:
            log.debug(f"üîç [VOICE DEBUG] Sample path: {self.voice_config.get('sample_path')}")
        
        # Retry avec d√©lai progressif
        max_retries = 2
        retry_delay = 1.0
        
        for attempt in range(max_retries + 1):
            try:
                log.debug(f"üéµ G√©n√©ration #{chunk.chunk_id}: {chunk.text[:30]}...")
                
                # Utiliser AudioGenerator
                audio_data = await self.audio_generator.generate_audio(
                    chunk.text, 
                    self.voice_config
                )
                
                if audio_data is None:
                    raise RuntimeError("G√©n√©ration audio √©chou√©e")
                
                chunk.audio_data = audio_data  # Stocker les bytes
                chunk.audio_path = None  # Pas de fichier
                chunk.generation_time = time.time() - start_time
                chunk.is_generated = True
                
                # Mise √† jour stats
                self.stats['chunks_generated'] += 1
                self.stats['total_generation_time'] += chunk.generation_time
                
                log.debug(f"‚úÖ G√©n√©ration #{chunk.chunk_id} termin√©e ({chunk.generation_time:.2f}s)")
                return  # Succ√®s
                
            except Exception as e:
                log.warning(f"‚ö†Ô∏è Erreur g√©n√©ration chunk #{chunk.chunk_id} (tentative {attempt + 1}): {e}")
            
            # Retry si pas le dernier essai
            if attempt < max_retries:
                await asyncio.sleep(retry_delay * (attempt + 1))
        
        # √âchec d√©finitif
        log.error(f"‚ùå √âchec g√©n√©ration chunk #{chunk.chunk_id} apr√®s {max_retries + 1} tentatives")
        chunk.is_generated = False
        chunk.audio_path = None
    
    async def _play_chunk_audio(self, chunk: AudioChunk):
        """Lit un chunk audio directement depuis les bytes"""
        if not chunk.is_generated or not hasattr(chunk, 'audio_data') or not chunk.audio_data:
            log.warning(f"‚ö†Ô∏è Chunk #{chunk.chunk_id} ignor√© (g√©n√©ration √©chou√©e)")
            return
        
        try:
            import pygame
            import io
            
            chunk.play_start_time = time.time()
            
            log.debug(f"üîä Lecture #{chunk.chunk_id}: {chunk.text[:30]}...")
            log.jarvis(f"Assistant: {chunk.text}")
            
            # ‚úÖ Lecture directe depuis bytes
            audio_buffer = io.BytesIO(chunk.audio_data)
            pygame.mixer.music.load(audio_buffer)
            pygame.mixer.music.play()
            
            # Attendre fin de lecture
            while pygame.mixer.music.get_busy():
                await asyncio.sleep(0.1)
            
            # Stats
            play_duration = time.time() - chunk.play_start_time
            self.stats['chunks_played'] += 1
            self.stats['total_playback_time'] += play_duration
            chunk.is_played = True
            
            log.debug(f"‚úÖ Lecture #{chunk.chunk_id} termin√©e ({play_duration:.2f}s)")
            
        except Exception as e:
            log.error(f"‚ùå Erreur lecture chunk #{chunk.chunk_id}: {e}")
    
    async def _warm_up_edge_tts(self):
        """Pr√©-chauffe Edge-TTS pour √©liminer la latence du premier chunk"""
        if self.edge_warmed_up or self.voice_config.get('model') != 'edge-tts':
            return
        
        try:
            log.debug("üî• Warm-up Edge-TTS...")
            
            # G√©n√©ration silencieuse pour pr√©chauffage
            warmup_config = self.voice_config.copy()
            warmup_audio = await self.audio_generator.generate_audio(
                "Test", warmup_config
            )
            
            if warmup_audio:
                self.edge_warmed_up = True
                log.success("üî• Edge-TTS pr√©chauff√©")
            else:
                log.warning("‚ö†Ô∏è Warm-up Edge-TTS √©chou√©")
                
        except Exception as e:
            log.warning(f"‚ö†Ô∏è Erreur warm-up Edge-TTS: {e}")
    
    def _optimize_process_priorities(self):
        """Optimise les priorit√©s processus pour audio temps-r√©el"""
        try:
            current_process = psutil.Process()
            
            # Augmenter priorit√© si possible
            if hasattr(psutil, 'HIGH_PRIORITY_CLASS'):
                current_process.nice(psutil.HIGH_PRIORITY_CLASS)
            else:
                current_process.nice(-5)  # Unix
            
            log.debug("üöÄ Priorit√© processus optimis√©e pour audio")
            
        except Exception as e:
            log.debug(f"Impossible d'optimiser priorit√©s: {e}")
    
    def _preinit_pygame(self):
        """Pr√©-initialise pygame pour √©liminer latence d√©marrage"""
        try:
            import pygame
            pygame.mixer.pre_init(frequency=22050, size=-16, channels=2, buffer=512)
            pygame.mixer.init()
            log.debug("üöÄ pygame pr√©-initialis√©")
        except Exception as e:
            log.warning(f"Impossible de pr√©-init pygame: {e}")
    
    def stop_pipeline(self):
        """Arr√™te proprement le pipeline"""
        if not self.pipeline_active:
            return
        
        log.debug("üõë Arr√™t pipeline...")
        self.pipeline_active = False
        
        # Signaler arr√™t aux workers
        try:
            asyncio.create_task(self.text_chunks_queue.put(None))
            asyncio.create_task(self.audio_ready_queue.put(None))
        except:
            pass  # Loop peut √™tre ferm√©e
        
        # Statistiques finales
        self._log_pipeline_stats()
        
        log.success("üõë Pipeline arr√™t√© proprement")
    
    def _log_pipeline_stats(self):
        """Affiche les statistiques du pipeline"""
        stats = self.stats
        
        if stats['chunks_generated'] > 0:
            total_time = max(stats['total_generation_time'], stats['total_playback_time'])
            sequential_time = stats['total_generation_time'] + stats['total_playback_time']
            efficiency = ((sequential_time - total_time) / sequential_time * 100) if sequential_time > 0 else 0
            
            log.success("üìä Pipeline Stats:", "üìà")
            log.info(f"   Chunks g√©n√©r√©s: {stats['chunks_generated']}")
            log.info(f"   Chunks lus: {stats['chunks_played']}")
            log.info(f"   Gain parall√©lisme: {efficiency:.1f}%")
            log.info(f"   Conversations: {stats['conversations_handled']}")
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut du pipeline"""
        return {
            'pipeline_active': self.pipeline_active,
            'chunks_in_generation_queue': self.text_chunks_queue.qsize(),
            'chunks_in_playback_queue': self.audio_ready_queue.qsize(),
            'edge_warmed_up': self.edge_warmed_up,
            'stats': self.stats.copy()
        }
    
    def update_voice_config(self, new_voice_config: Dict[str, Any]):
        """Met √† jour la configuration voix dynamiquement"""
        self.voice_config = new_voice_config
        self.edge_warmed_up = False  # Reset warm-up si changement
        log.info(f"Configuration voix mise √† jour: {new_voice_config.get('model', 'unknown')}")


# ============================================================================
# UTILITAIRES POUR D√âCOUPAGE TEXTE
# ============================================================================

def split_text_for_streaming(text: str, max_length: int = 150) -> list:
    """
    D√©coupe un texte en chunks optimaux pour le streaming
    
    Args:
        text: Texte √† d√©couper
        max_length: Taille maximale d'un chunk
        
    Returns:
        Liste de chunks texte
    """
    if len(text) <= max_length:
        return [text]
    
    chunks = []
    
    # D√©couper par phrases d'abord
    sentences = text.replace('. ', '.|').replace('! ', '!|').replace('? ', '?|').split('|')
    
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # Si ajouter cette phrase d√©passe la limite
        if len(current_chunk) + len(sentence) > max_length:
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = sentence
            else:
                # Phrase trop longue, d√©couper par mots
                words = sentence.split()
                for word in words:
                    if len(current_chunk) + len(word) > max_length:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = word
                    else:
                        current_chunk += " " + word if current_chunk else word
        else:
            current_chunk += " " + sentence if current_chunk else sentence
    
    # Ajouter le dernier chunk
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    
    return chunks

==================================================
FICHIER: .\lobes_temporaux\conversation_flow.py
==================================================

"""
conversation_flow.py - Flux de conversation unifi√© (Lobes Temporaux) - VERSION FINALE
Responsabilit√© : Messages, streaming LLM, STT/TTS
MODIFI√â pour supporter AudioGenerator + AudioPipeline + VoiceCloner
INCLUDES: auto_initialize(), stop(), et toutes les m√©thodes requises
"""

import time
import asyncio
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Callable, Optional
import sys

# Imports des modules - R√©utilisation modules existants
sys.path.append(str(Path(__file__).parent.parent))
from cortex_prefrontal.llm_client import JarvisLLM  # LLM unifi√© avec streaming
from lobes_temporaux.stt import SpeechToText  # Module local
from lobes_temporaux.tts import TextToSpeech  # Module local (maintenant avec NOUVELLE architecture)
from hypothalamus.device_manager import DeviceManager
from hypothalamus.voice_manager import VoiceManager
from hypothalamus.logger import log

class ConversationFlow:
    """Flux de conversation unifi√© avec vrai streaming (Lobes Temporaux)"""
    
    def __init__(self):
        # Instances des modules (R√âUTILISATION maximale)
        self.llm = None  # LLM unifi√©
        self.stt = None  # Module local lobes_temporaux
        self.tts = None  # Module local lobes_temporaux avec NOUVELLE architecture
        
        # Configuration actuelle
        self.personality = None
        self.display_name = None
        self.is_initialized = False
        
        # Queue TTS pour streaming s√©quentiel (LEGACY - pour compatibilit√©)
        self.tts_queue = asyncio.Queue()
        self.tts_worker_running = False

        # Historique de conversation
        self.conversation_history = []

        # Syst√®me anti-duplication
        self.processing_lock = asyncio.Lock()
        self.recent_messages = {}  # Hash -> timestamp

        # Callback WebSocket
        self.websocket_callback: Optional[Callable] = None
        
        # Stats de session (ajout m√©triques pipeline)
        self.session_stats = {
            'messages_count': 0,
            'total_tokens': 0,
            'total_time': 0.0,
            'avg_response_time': 0.0,
            'avg_ttft': 0.0,
            'pipeline_efficiency': 0.0
        }
        
        log.info("ConversationFlow cr√©√© (Lobes Temporaux - Pipeline parall√®le)")
    
    async def auto_initialize(self) -> bool:
        """Initialisation automatique sans interaction utilisateur"""
        try:
            log.info("Initialisation automatique ConversationFlow...")
            
            # 1. Configuration microphone automatique (R√âUTILISE device_manager)
            device_mgr = DeviceManager()
            saved_index, _ = device_mgr.load_saved_device()
            device_index = saved_index if saved_index and device_mgr.verify_device(saved_index)[0] else None
            
            if device_index is None:
                log.error("Aucun microphone disponible")
                return False
            
            # 2. Configuration voix automatique (R√âUTILISE voice_manager)
            voice_mgr = VoiceManager()
            _, personality, tts_model, edge_voice, sample_path, embedding_path = voice_mgr.load_saved_voice()
            if not personality:  # Pas de config sauv√©e
                personality, tts_model, edge_voice, sample_path, embedding_path = "Samantha", "edge-tts", "fr-FR-DeniseNeural", None, None
            
            # 3. Initialiser les modules (R√âUTILISATION COMPL√àTE)
            self.llm = JarvisLLM(personality=personality)  # Cortex pr√©frontal
            self.stt = SpeechToText(device_index=device_index)  # Module local
            
            # NOUVEAU: Initialisation TTS avec nouvelle architecture
            try:
                # PRIORIT√â 1: Nouvelle architecture
                self.tts = TextToSpeech(personality=personality)
                log.success(f"TTS nouvelle architecture initialis√©: {personality}")
            except Exception as tts_error:
                log.warning(f"Fallback ancienne architecture: {tts_error}")
                # PRIORIT√â 2: Ancienne architecture
                if sample_path:
                    from pathlib import Path
                    sample_path_obj = Path(sample_path)
                    if not sample_path_obj.is_absolute():
                        sample_path = str(Path('config') / sample_path)
                self.tts = TextToSpeech(
                    model_name=tts_model, 
                    personality=personality, 
                    edge_voice=edge_voice,
                    sample_path=sample_path,
                    embedding_path=embedding_path
                )
                log.success(f"TTS fallback initialis√©: {personality}")
            
            self.personality = personality
            self.display_name = f"Assistant virtuel - {personality}"
            self.is_initialized = True
            
            log.success(f"ConversationFlow initialis√© - {personality} pr√™t (pipeline actif)")
            return True
            
        except Exception as e:
            log.error(f"Erreur initialisation ConversationFlow: {e}")
            return False
    
    async def initialize(self, personality: str = "Jarvis") -> bool:
        """Initialise tous les modules requis pour la conversation"""
        try:
            self.personality = personality
            
            # 1. Configuration microphone automatique (R√âUTILISE device_manager)
            device_mgr = DeviceManager()
            saved_index, _ = device_mgr.load_saved_device()
            device_index = saved_index if saved_index and device_mgr.verify_device(saved_index)[0] else None
            
            if device_index is None:
                log.error("Aucun microphone disponible")
                return False
            
            # 2. Configuration voix automatique (R√âUTILISE voice_manager)
            voice_mgr = VoiceManager()
            _, personality, tts_model, edge_voice, sample_path, embedding_path = voice_mgr.load_saved_voice()
            if not personality:  # Pas de config sauv√©e
                personality, tts_model, edge_voice, sample_path, embedding_path = "Samantha", "edge-tts", "fr-FR-DeniseNeural", None, None
            
            # 3. Initialisation LLM (Ollama unifi√©)
            self.llm = JarvisLLM()
            if not await self.llm.initialize():
                log.error("√âchec initialisation LLM")
                return False
            
            # 4. Initialisation STT (local)
            self.stt = SpeechToText(device_index=device_index)
            if not self.stt.initialize():
                log.error("√âchec initialisation STT")
                return False
            
            # 5. Initialisation TTS (NOUVELLE ARCHITECTURE)
            try:
                # NOUVEAU: Utiliser factory function ou constructeur direct
                self.tts = TextToSpeech(personality=personality)
                log.success(f"TTS nouvelle architecture initialis√©: {personality}")
            except Exception as tts_error:
                log.warning(f"√âchec nouvelle architecture TTS: {tts_error}")
                # Fallback vers ancienne m√©thode si n√©cessaire
                self.tts = TextToSpeech(tts_model, personality, edge_voice, sample_path, embedding_path)
                log.info(f"TTS fallback initialis√©: {personality}")
            
            self.is_initialized = True
            
            log.success(f"ConversationFlow initialis√© - {personality} pr√™t (pipeline actif)")
            return True
            
        except Exception as e:
            log.error(f"Erreur initialisation ConversationFlow: {e}")
            return False
    
    async def process_voice_input(self):
        """Traite un message vocal complet : √©coute + traitement + r√©ponse"""
        if not self.is_initialized or not self.stt:
            await self._send_error("STT non disponible")
            return
        
        try:
            log.info("D√©but de l'√©coute vocale")
            await self._send_event('listening_start', '')
            
            # √âcouter
            loop = asyncio.get_event_loop()
            transcription = await loop.run_in_executor(
                None, 
                self.stt.listen_with_whisper_vad, 
                15
            )
            
            await self._send_event('listening_end', '')
            
            if transcription and transcription.strip():
                await self._send_event('transcription', transcription)
                
                # TRAITER DIRECTEMENT LE MESSAGE (√©vite la double transcription)
                await self.process_text_message(transcription)
            else:
                log.info("Aucune voix d√©tect√©e")
                
        except Exception as e:
            log.error(f"Erreur STT: {e}")
            await self._send_event('listening_end', '')
            await self._send_error(f"Erreur microphone: {str(e)}")
            
    async def process_text_message(self, message: str):
        """Traite un message texte utilisateur avec VRAI streaming + Pipeline TTS"""
        # Anti-duplication
        message_hash = hashlib.md5(message.encode()).hexdigest()
        current_time = time.time()
        
        # V√©rifier les doublons
        if message_hash in self.recent_messages:
            if current_time - self.recent_messages[message_hash] < 2.0:
                log.warning(f"Message dupliqu√© ignor√©: {message[:30]}...")
                return
        
        self.recent_messages[message_hash] = current_time
        
        # Nettoyer les vieux hashes (>10s)
        self.recent_messages = {
            h: t for h, t in self.recent_messages.items() 
            if current_time - t < 10
        }
        
        # Lock pour √©viter les traitements simultan√©s
        async with self.processing_lock:
                
            if not self.is_initialized:
                await self._send_error("Syst√®me non initialis√©")
                return
            
            try:
                log.info(f"Message texte re√ßu: {message[:50]}...")
                
                # Ajouter √† l'historique
                self._add_to_history('user', message)
                
                # Notifier le d√©but de traitement
                await self._send_event('message_processing_start', message)
                
                # üöÄ NOUVEAU: Pipeline complet LLM + TTS parall√®le
                await self._process_with_parallel_pipeline(message)
                
            except Exception as e:
                log.error(f"Erreur traitement message: {e}")
                await self._send_error(f"Erreur traitement: {str(e)}")
    
    def _supports_pipeline(self) -> bool:
        """D√©termine si le TTS supporte le pipeline parall√®le - ADAPT√â NOUVELLE ARCHITECTURE"""
        # PRIORIT√â 1: V√©rifier si c'est la NOUVELLE architecture
        if hasattr(self.tts, 'pipeline') and hasattr(self.tts.pipeline, 'queue_text_chunk'):
            log.debug("‚úÖ NOUVELLE architecture TTS d√©tect√©e", "üîä")
            return True
        
        # PRIORIT√â 2: Compatibilit√© avec ancienne architecture
        if (hasattr(self.tts, 'is_edge') and 
            self.tts.is_edge and 
            hasattr(self.tts, 'add_text_chunk')):
            log.debug("‚ö†Ô∏è Ancienne architecture TTS d√©tect√©e", "üîä")
            return True
            
        log.debug("‚ùå Aucune architecture pipeline d√©tect√©e", "‚ö†Ô∏è")
        return False

    async def _send_to_tts(self, text: str):
        """Envoie du texte au TTS - ADAPT√â NOUVELLE ARCHITECTURE"""
        
        # PRIORIT√â 1: Nouvelle architecture avec AudioPipeline
        if hasattr(self.tts, 'pipeline') and hasattr(self.tts.pipeline, 'queue_text_chunk'):
            await self.tts.pipeline.queue_text_chunk(text)
            log.debug(f"‚úÖ NOUVEAU pipeline: chunk envoy√©", "üîä")
        
        # PRIORIT√â 2: Ancienne architecture pipeline
        elif (hasattr(self.tts, 'is_edge') and 
              self.tts.is_edge and 
              hasattr(self.tts, 'add_text_chunk')):
            await self.tts.add_text_chunk(text)
            log.debug(f"‚úÖ ANCIEN pipeline: chunk envoy√©", "üîä")
        
        # PRIORIT√â 3: Fallback legacy
        else:
            await self.tts_queue.put(text)
            if not self.tts_worker_running:
                asyncio.create_task(self._tts_worker())
            log.debug(f"‚ö†Ô∏è Legacy: chunk envoy√©", "‚ö†Ô∏è")

    async def _process_with_parallel_pipeline(self, message: str):
        """Pipeline complet LLM streaming + TTS parall√®le ADAPT√â NOUVELLE ARCHITECTURE"""
        session_start = time.time()
        full_response = ""
        token_count = 0
        first_token_time = None
        first_audio_time = None
        sentence_buffer = ""
        
        try:
            log.debug("üöÄ D√©marrage pipeline complet LLM + TTS", "üîä")
            
            # D√©marrer le pipeline TTS si support√©
            if self._supports_pipeline():
                log.debug("üöÄ PIPELINE: D√©marrage workers...", "üîä")
                
                # NOUVEAU: D√©marrage pipeline selon architecture
                if hasattr(self.tts, 'pipeline'):
                    self.tts.pipeline.start_streaming_workers()
                    log.debug("‚úÖ NOUVEAU pipeline TTS d√©marr√©", "üîä")
                elif hasattr(self.tts, '_start_parallel_workers'):
                    await self.tts._start_parallel_workers()
                    log.debug("‚úÖ ANCIEN pipeline TTS d√©marr√©", "üîä")
                
                log.debug("‚úÖ Pipeline TTS d√©marr√©", "üîä")
            else:
                log.debug("‚ö†Ô∏è Utilisation ancien syst√®me TTS", "‚ö†Ô∏è")
            
            # üî• STREAMING depuis Ollama (LLM unifi√©)
            # üß† NOUVEAU: Pr√©chauffer TTS pendant que LLM d√©marre sa r√©flexion
            if self._supports_pipeline():
                # NOUVEAU: Warm-up selon architecture
                if hasattr(self.tts, 'pipeline'):
                    # Le warm-up est automatique dans AudioPipeline
                    log.debug("üî• Warm-up automatique NOUVEAU pipeline", "üîä")
                elif hasattr(self.tts, 'warm_up_during_llm_thinking'):
                    asyncio.create_task(self.tts.warm_up_during_llm_thinking())
                    log.debug("üî• Warm-up ANCIEN pipeline", "üîä")
            
            for token in self.llm.generate_response_stream(message):
                # Premier token - mesurer TTFT
                if first_token_time is None:
                    first_token_time = time.time() - session_start
                    await self._send_event('first_token', token, {
                        'ttft': first_token_time
                    })
                
                # Envoyer chaque token √† l'interface
                await self._send_event('llm_token', token)
                full_response += token
                token_count += 1
                sentence_buffer += token
                
                # üî• OPTIMISATION: D√©tection phrase compl√®te ‚Üí Envoi IMM√âDIAT TTS
                if self._is_sentence_complete(sentence_buffer):
                    clean_sentence = sentence_buffer.strip()
                    
                    if clean_sentence:
                        # Mesurer temps premier audio
                        if first_audio_time is None:
                            first_audio_time = time.time() - session_start
                        
                        # Envoi au TTS (nouvelle architecture compatible)
                        await self._send_to_tts(clean_sentence)
                        log.debug(f"‚úÖ Chunk envoy√©: {clean_sentence[:40]}...", "üîä")
                    
                    sentence_buffer = ""  # Reset buffer
                
                # Petite pause pour √©viter l'inondation WebSocket
                if token_count % 10 == 0:
                    await asyncio.sleep(0.001)
            
            # Traiter le reste du buffer s'il y a du contenu
            if sentence_buffer.strip():
                await self._send_to_tts(sentence_buffer.strip())
                log.debug("‚úÖ Dernier chunk envoy√©", "üîä")
            
            # Finaliser le pipeline si actif avec timeout dynamique
            if self._supports_pipeline():
                # Timeout adaptatif selon la longueur de la r√©ponse
                estimated_time = token_count * 0.3  # 0.3s par token
                dynamic_timeout = max(60.0, estimated_time)  # Minimum 60s
                
                log.debug(f"‚è≥ Attente fin conversation ({dynamic_timeout:.0f}s max)...", "üîä")
                
                # NOUVEAU: Finalisation selon architecture
                if hasattr(self.tts, 'pipeline'):
                    # Attendre que le NOUVEAU pipeline se vide
                    start_wait = time.time()
                    while (time.time() - start_wait) < dynamic_timeout:
                        status = self.tts.pipeline.get_status()
                        if (status['chunks_in_generation_queue'] == 0 and 
                            status['chunks_in_playback_queue'] == 0):
                            break
                        await asyncio.sleep(0.5)
                    log.debug("‚úÖ NOUVEAU pipeline termin√©", "üîä")
                    
                elif hasattr(self.tts, 'finalize_pipeline'):
                    await self.tts.finalize_pipeline(timeout=dynamic_timeout)
                    log.debug("‚úÖ ANCIEN pipeline termin√©", "üîä")
                
                log.debug("‚úÖ Conversation termin√©e", "üîä")
            
            total_time = time.time() - session_start
            tokens_per_second = token_count / max(total_time, 0.001)
            
            # Calculer efficacit√© du pipeline
            pipeline_efficiency = 0.0
            
            # NOUVEAU: Stats selon architecture
            if hasattr(self.tts, 'pipeline'):
                status = self.tts.pipeline.get_status()
                if status.get('stats', {}).get('chunks_generated', 0) > 0:
                    stats = status['stats']
                    if stats['total_generation_time'] > 0 and stats['total_playback_time'] > 0:
                        sequential_time = stats['total_generation_time'] + stats['total_playback_time']
                        parallel_time = max(stats['total_generation_time'], stats['total_playback_time'])
                        pipeline_efficiency = ((sequential_time - parallel_time) / sequential_time * 100)
            
            # ANCIEN: Stats ancienne architecture
            elif hasattr(self.tts, 'is_edge') and self.tts.is_edge and hasattr(self.tts, 'pipeline_stats'):
                stats = self.tts.pipeline_stats
                if stats['total_generation_time'] > 0 and stats['total_playback_time'] > 0:
                    sequential_time = stats['total_generation_time'] + stats['total_playback_time']
                    parallel_time = max(stats['total_generation_time'], stats['total_playback_time'])
                    pipeline_efficiency = ((sequential_time - parallel_time) / sequential_time * 100)
            
            await self._send_event('llm_complete', full_response, {
                'total_time': total_time,
                'token_count': token_count,
                'ttft': first_token_time or 0,
                'first_audio_time': first_audio_time or 0,
                'tokens_per_second': tokens_per_second,
                'pipeline_efficiency': pipeline_efficiency
            })
            
            # Ajouter la r√©ponse compl√®te √† l'historique
            self._add_to_history('assistant', full_response, token_count)
            
            # Mettre √† jour les stats
            self._update_session_stats({
                'total_time': total_time,
                'token_count': token_count,
                'ttft': first_token_time or 0,
                'first_audio_time': first_audio_time or 0,
                'tokens_per_second': tokens_per_second,
                'pipeline_efficiency': pipeline_efficiency
            })
            
            log.success(f"Message trait√©: {token_count} tokens en {total_time:.2f}s ({tokens_per_second:.1f} tok/s, gain: {pipeline_efficiency:.1f}%)")
            
        except Exception as e:
            log.error(f"Erreur pipeline parall√®le: {e}")
            raise
    
    def _is_sentence_complete(self, text: str) -> bool:
        """D√©tecte si une phrase est compl√®te pour envoyer au TTS"""
        if not text.strip():
            return False
        
        # D√©limiteurs de fin de phrase
        sentence_enders = ['.', '!', '?', ':', ';']
        
        # V√©rifier fin de phrase
        if any(text.rstrip().endswith(ender) for ender in sentence_enders):
            return True
        
        # Phrases courtes (questions, exclamations)
        if len(text.split()) >= 4 and text.rstrip().endswith(('?', '!')):
            return True
        
        return False
    
    async def reload_tts(self, model_name, personality, edge_voice=None, sample_path=None, embedding_path=None):
        """Recharge le TTS avec une nouvelle voix"""
        try:
            log.info(f"Rechargement TTS : {personality}")
            
            # Arr√™ter pipeline actuel si n√©cessaire
            if hasattr(self.tts, 'pipeline') and self.tts.pipeline.pipeline_active:
                self.tts.pipeline.stop_pipeline()
                await asyncio.sleep(0.5)  # Laisser temps d'arr√™t
            elif hasattr(self.tts, 'parallel_pipeline_active') and self.tts.parallel_pipeline_active:
                self.tts.parallel_pipeline_active = False
                await asyncio.sleep(0.5)  # Laisser temps d'arr√™t
            
            # NOUVEAU: Cr√©er nouvelle instance TTS avec nouvelle architecture
            try:
                self.tts = TextToSpeech(personality=personality)
                log.success(f"TTS nouvelle architecture recharg√©: {personality}")
            except Exception as e:
                log.warning(f"Fallback ancienne architecture: {e}")
                # Fallback vers ancienne m√©thode
                self.tts = TextToSpeech(
                    model_name=model_name,
                    personality=personality,
                    edge_voice=edge_voice,
                    sample_path=sample_path,
                    embedding_path=embedding_path
                )
                log.success(f"TTS fallback recharg√©: {personality}")
            
            log.success(f"TTS recharg√© avec pipeline : {personality}")
            
        except Exception as e:
            log.error(f"Erreur rechargement TTS: {e}")
            raise
    
    def get_personality(self) -> str:
        """Retourne la personnalit√© actuelle"""
        return self.personality or "Samantha"
    
    def get_display_name(self) -> str:
        """Retourne le nom d'affichage format√©"""
        if hasattr(self, 'display_name') and self.display_name:
            return self.display_name
        else:
            personality = self.get_personality()
            return f"Assistant virtuel - {personality}"
        
    def get_history(self) -> Dict[str, Any]:
        """Retourne l'historique de conversation avec stats pipeline"""
        return {
            'success': True,
            'history': self.conversation_history,
            'stats': self.session_stats
        }
    
    def clear_history(self) -> Dict[str, Any]:
        """Efface l'historique de conversation"""
        self.conversation_history.clear()
        self.session_stats = {
            'messages_count': 0,
            'total_tokens': 0,
            'total_time': 0.0,
            'avg_response_time': 0.0,
            'avg_ttft': 0.0,
            'pipeline_efficiency': 0.0
        }
        
        log.info("Historique de conversation effac√©")
        return {'success': True}
    
    def set_websocket_callback(self, callback: Callable):
        """D√©finit le callback WebSocket pour les √©v√©nements"""
        self.websocket_callback = callback
    
    def _add_to_history(self, sender: str, content: str, token_count: int = 0):
        """Ajoute un message √† l'historique"""
        entry = {
            'sender': sender,
            'content': content,
            'timestamp': time.time(),
            'token_count': token_count if sender == 'assistant' else 0
        }
        
        self.conversation_history.append(entry)
        
        # Limiter l'historique (garder les 100 derniers messages)
        if len(self.conversation_history) > 100:
            self.conversation_history = self.conversation_history[-100:]
    
    def _update_session_stats(self, result: Dict[str, Any]):
        """Met √† jour les statistiques de session avec m√©triques pipeline"""
        self.session_stats['messages_count'] += 1
        self.session_stats['total_tokens'] += result.get('token_count', 0)
        self.session_stats['total_time'] += result.get('total_time', 0)
        
        # TTFT moyen
        if 'ttft' in result:
            current_ttft = self.session_stats.get('avg_ttft', 0)
            count = self.session_stats['messages_count']
            self.session_stats['avg_ttft'] = (current_ttft * (count - 1) + result['ttft']) / count
        
        # Temps de r√©ponse moyen
        if self.session_stats['messages_count'] > 0:
            self.session_stats['avg_response_time'] = (
                self.session_stats['total_time'] / self.session_stats['messages_count']
            )
        
        # NOUVEAU: Efficacit√© pipeline moyenne
        if 'pipeline_efficiency' in result and result['pipeline_efficiency'] > 0:
            current_eff = self.session_stats.get('pipeline_efficiency', 0)
            count = self.session_stats['messages_count']
            self.session_stats['pipeline_efficiency'] = (current_eff * (count - 1) + result['pipeline_efficiency']) / count
    
    async def _send_event(self, event_type: str, content: str, metadata: Dict = None):
        """Envoie un √©v√©nement via WebSocket"""
        if self.websocket_callback:
            event_data = {
                'type': event_type,
                'content': content,
                'timestamp': time.time(),
                'metadata': metadata or {}
            }
            await self.websocket_callback(event_data)
        else:
            log.warning(f"‚ö†Ô∏è WebSocket callback non d√©fini pour: {event_type}")
    
    async def _send_error(self, error_message: str):
        """Envoie une erreur via WebSocket"""
        await self._send_event('error', error_message)
    
    def stop(self):
        """Arr√™te proprement le gestionnaire et le pipeline"""
        # Arr√™ter pipeline TTS si actif
        if hasattr(self.tts, 'pipeline') and hasattr(self.tts.pipeline, 'stop_pipeline'):
            self.tts.pipeline.stop_pipeline()
        elif hasattr(self.tts, 'parallel_pipeline_active') and self.tts.parallel_pipeline_active:
            self.tts.parallel_pipeline_active = False
        
        log.info("ConversationFlow arr√™t√© avec pipeline")
    
    # === M√âTHODES LEGACY (pour compatibilit√©) ===
    
    async def _tts_worker(self):
        """Worker TTS simplifi√© et robuste (LEGACY - pour compatibilit√©)"""
        if self.tts_worker_running:
            return  # D√©j√† actif
            
        self.tts_worker_running = True
        log.debug("üîä TTS worker legacy d√©marr√©")
        
        try:
            while self.tts_worker_running:
                try:
                    # Attendre segment (timeout plus long pour le streaming LLM)
                    segment = await asyncio.wait_for(self.tts_queue.get(), timeout=10.0)
                    
                    log.debug(f"üîä TTS traite (legacy): {segment[:30]}...")
                    
                    # Utiliser l'ancienne m√©thode fiable OU la nouvelle
                    if hasattr(self.tts, 'speak'):
                        # NOUVEAU syst√®me: utiliser speak() async
                        await self.tts.speak(segment)
                    elif hasattr(self.tts, '_speak_response'):
                        # ANCIEN syst√®me: utiliser _speak_response
                        await self.tts._speak_response(segment)
                    else:
                        log.error("Aucune m√©thode TTS disponible")
                    
                    self.tts_queue.task_done()
                    
                except asyncio.TimeoutError:
                    # Plus rien depuis 10s ‚Üí arr√™ter
                    log.debug("üîä TTS timeout - arr√™t propre")
                    break
                    
        except Exception as e:
            log.error(f"‚ùå Erreur TTS worker legacy: {e}")
        finally:
            self.tts_worker_running = False
            log.debug("üîä TTS worker legacy arr√™t√© proprement")

==================================================
FICHIER: .\lobes_temporaux\memory_manager.py
==================================================

import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from datetime import datetime
from typing import List, Dict, Any

class MemoryManager:
    def __init__(self):
        # Client ChromaDB local
        self.client = chromadb.PersistentClient(
            path="./chroma_db",
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Mod√®le d'embeddings fran√ßais
        self.embedder = SentenceTransformer('distiluse-base-multilingual-cased-v2')
        
        # Collections
        self.user_profile = self.client.get_or_create_collection("user_profile")
        self.conversations = self.client.get_or_create_collection("conversations")
        
    async def store_conversation(self, role: str, user_msg: str, assistant_msg: str):
        """Stocke une conversation avec contexte"""
        timestamp = datetime.now().isoformat()
        
        # Cr√©er embedding du contenu
        embedding = self.embedder.encode(f"{user_msg} {assistant_msg}")
        
        self.conversations.add(
            embeddings=[embedding.tolist()],
            documents=[assistant_msg],
            metadatas=[{
                "role": role,
                "user_msg": user_msg,
                "timestamp": timestamp
            }],
            ids=[f"conv_{timestamp}"]
        )
    
    async def get_relevant_context(self, query: str, role: str = None, limit: int = 3):
        """R√©cup√®re le contexte pertinent"""
        embedding = self.embedder.encode(query)
        
        where_clause = {"role": role} if role else None
        
        results = self.conversations.query(
            query_embeddings=[embedding.tolist()],
            n_results=limit,
            where=where_clause
        )
        
        return results

==================================================
FICHIER: .\lobes_temporaux\stt.py
==================================================

"""
Speech-to-Text avec Faster-Whisper + VAD (Voice Activity Detection)
Version nettoy√©e sans redondances
"""

import json
import pyaudio
import os
from pathlib import Path
import yaml
import numpy as np
import time
from typing import Dict, Any, Optional

# Import faster-whisper
from faster_whisper import WhisperModel
import webrtcvad

# Import logger
import sys
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

# Configuration FFmpeg
import imageio_ffmpeg
os.environ["PATH"] = os.path.dirname(imageio_ffmpeg.get_ffmpeg_exe()) + os.pathsep + os.environ.get("PATH", "")

class SpeechToText:
    def __init__(self, device_index=None, config_path: Optional[str] = None):
        """
        Initialise STT avec faster-whisper et configuration JSON
        
        Args:
            device_index: Index du microphone √† utiliser
            config_path: Chemin vers le fichier de config JSON Whisper
        """
        self.device_index = device_index
        
        # Charger configurations
        self._load_yaml_config()
        self._load_whisper_config(config_path)
        
        # Cr√©er dossier pour les enregistrements (optionnel selon config)
        if self.whisper_config['debug']['save_recordings']:
            self.recordings_dir = Path(self.whisper_config['debug']['recordings_path'])
            self.recordings_dir.mkdir(exist_ok=True)
        else:
            self.recordings_dir = None
        
        # Initialiser le mod√®le Whisper
        self._initialize_whisper_model()
        
        # Pr√©-charger les composants audio
        self._preload_audio_components()
        
        if self.device_index is not None:
            self._log_audio_device()
    
    def _load_yaml_config(self):
        """Charge la configuration YAML principale"""
        config_path = Path(__file__).parent.parent / "config/settings.yaml"
        with open(config_path, 'r', encoding='utf-8') as f:
            self.yaml_config = yaml.safe_load(f)
    
    def _load_whisper_config(self, config_path: Optional[str] = None):
        """Charge la configuration JSON Whisper"""
        if config_path is None:
            config_path = Path(__file__).parent.parent / "config/whisper_config.json"
        
        self.whisper_config_path = Path(config_path)
        
        if self.whisper_config_path.exists():
            with open(self.whisper_config_path, 'r', encoding='utf-8') as f:
                self.whisper_config = json.load(f)
            log.success(f"Configuration Whisper charg√©e: {self.whisper_config_path.name}", "‚öôÔ∏è")
        else:
            # Configuration par d√©faut si le fichier n'existe pas
            self.whisper_config = self._get_default_whisper_config()
            self._save_whisper_config()
            log.warning(f"Config Whisper cr√©√©e par d√©faut: {self.whisper_config_path}", "‚öôÔ∏è")
    
    def _get_default_whisper_config(self) -> Dict[str, Any]:
        """Retourne la configuration Whisper par d√©faut (version simplifi√©e)"""
        return {
            "model": {
                "name": "small",
                "device": "cpu",
                "compute_type": "int8"
            },
            "transcription": {
                "language": "fr",
                "beam_size": 5,
                "temperature": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0],
                "no_speech_threshold": 0.6
            },
            "vad": {
                "enabled": True,
                "aggressiveness": 2,
                "min_speech_duration": 0.2,  # Optimis√© pour r√©activit√©
                "silence_duration": 1.5,
                "timeout": 30
            },
            "debug": {
                "save_recordings": False,
                "recordings_path": "./recordings",
                "log_transcription_details": True,
                "log_performance_stats": True
            }
        }
    
    def _save_whisper_config(self):
        """Sauvegarde la configuration Whisper dans le fichier JSON"""
        with open(self.whisper_config_path, 'w', encoding='utf-8') as f:
            json.dump(self.whisper_config, f, indent=2, ensure_ascii=False)
    
    def _initialize_whisper_model(self):
        """Initialise le mod√®le Whisper (version simplifi√©e)"""
        model_config = self.whisper_config['model']
        model_name = model_config['name']
        
        log.info(f"Chargement mod√®le Whisper '{model_name}'...", "üé§")
        
        try:
            self.model = WhisperModel(
                model_name,
                device=model_config.get('device', 'cpu'),
                compute_type=model_config.get('compute_type', 'int8')
            )
            self.use_faster_whisper = True
            log.success("Faster-Whisper pr√™t ! üöÄ", "üé§")
            
        except Exception as e:
            log.error(f"Erreur chargement Whisper: {e}")
            raise
    
    def _preload_audio_components(self):
        """Pr√©-charge VAD et PyAudio une seule fois"""
        try:
            # Pr√©-charger VAD avec config
            vad_config = self.whisper_config['vad']
            self.vad = webrtcvad.Vad(vad_config['aggressiveness'])
            
            # Pr√©-charger PyAudio
            self.pyaudio_instance = pyaudio.PyAudio()
            self.audio_stream = None
            
            log.success("üé§ Composants audio pr√©-charg√©s", "‚ö°")
            
        except Exception as e:
            log.error(f"Erreur pr√©-chargement audio: {e}")
            # Fallback si probl√®me
            self.vad = None
            self.pyaudio_instance = None
            self.audio_stream = None
    
    def _log_audio_device(self):
        """Log info sur le device audio utilis√©"""
        if self.pyaudio_instance and self.device_index is not None:
            try:
                info = self.pyaudio_instance.get_device_info_by_index(self.device_index)
                log.info(f"üé§ Device: {info['name']}", "üéß")
            except:
                log.warning("Impossible de r√©cup√©rer info device")
    
    def listen_with_whisper_vad(self, max_duration: int = 15) -> str:
        """
        M√©thode principale : Enregistrement avec VAD + Transcription
        """
        try:
            log.info("üéôÔ∏è Micro actif, parlez...", "")
            
            # Enregistrement avec VAD
            audio_data = self._record_with_realtime_vad(max_duration)
            
            if audio_data is None or len(audio_data) == 0:
                log.warning("Aucun audio enregistr√©")
                return ""
            
            log.info("üîÑ Transcription...", "")
            
            # Transcription unifi√©e
            result = self._transcribe_audio(audio_data)
            
            if result:
                log.success(f"Transcription: '{result}'")
            else:
                log.warning("‚ö†Ô∏è Transcription vide")
            
            return result
            
        except Exception as e:
            log.error(f"‚ùå Erreur transcription: {e}")
            return ""
    
    def _transcribe_audio(self, audio_data: np.ndarray) -> str:
        """
        Transcription unifi√©e (remplace les anciennes m√©thodes dupliqu√©es)
        """
        try:
            trans_config = self.whisper_config['transcription']
            
            start_time = time.time()
            
            # Transcription avec faster-whisper
            segments, info = self.model.transcribe(
                audio_data,
                language=trans_config['language'],
                beam_size=trans_config['beam_size'],
                temperature=trans_config['temperature'],
                no_speech_threshold=trans_config['no_speech_threshold'],
                # VAD faster-whisper pour nettoyer
                vad_filter=True,
                vad_parameters=dict(
                    min_silence_duration_ms=800,
                    speech_pad_ms=300
                )
            )
            
            # Assembler les segments
            result = " ".join([segment.text for segment in segments]).strip()
            
            # Stats de performance
            if self.whisper_config['debug']['log_performance_stats']:
                transcription_time = time.time() - start_time
                log.debug(f"‚è±Ô∏è Temps transcription: {transcription_time:.2f}s")
            
            return result
            
        except Exception as e:
            log.error(f"‚ùå Erreur lors de la transcription: {e}")
            return ""
    
    def _record_with_realtime_vad(self, max_duration: int) -> Optional[np.ndarray]:
        """
        Enregistrement avec VAD temps r√©el optimis√©
        """
        try:
            # V√©rifier pr√©-chargement
            if self.vad is None or self.pyaudio_instance is None:
                log.warning("Composants non pr√©-charg√©s, impossible d'enregistrer")
                return None
            
            # Configuration audio
            RATE = 16000
            CHUNK = 320  # 20ms chunks pour webrtcvad
            FORMAT = pyaudio.paInt16
            CHANNELS = 1
            
            vad_config = self.whisper_config['vad']
            
            frames = []
            silence_count = 0
            speech_count = 0
            recording_speech = False
            
            # Seuils en chunks (20ms chacun)
            min_speech_chunks = int(vad_config['min_speech_duration'] * 50)
            silence_threshold_chunks = int(vad_config['silence_duration'] * 50)
            
            stream_params = {
                'format': FORMAT,
                'channels': CHANNELS,
                'rate': RATE,
                'input': True,
                'frames_per_buffer': CHUNK
            }
            
            if self.device_index is not None:
                stream_params['input_device_index'] = self.device_index
            
            # Utiliser pyaudio pr√©-charg√©
            stream = self.pyaudio_instance.open(**stream_params)
            
            max_chunks = int(RATE / CHUNK * max_duration)
            
            for i in range(max_chunks):
                try:
                    data = stream.read(CHUNK, exception_on_overflow=False)
                    frames.append(data)
                    
                    # VAD pr√©-charg√©
                    try:
                        is_speech = self.vad.is_speech(data, RATE)
                    except:
                        is_speech = False
                    
                    if is_speech:
                        speech_count += 1
                        silence_count = 0
                        if speech_count >= min_speech_chunks:
                            recording_speech = True
                    else:
                        silence_count += 1
                        speech_count = 0
                    
                    # Arr√™t si silence prolong√© apr√®s parole
                    if recording_speech and silence_count >= silence_threshold_chunks:
                        break
                        
                except Exception as e:
                    log.warning(f"Erreur lecture audio: {e}")
                    break
            
            stream.stop_stream()
            stream.close()
            
            if not frames or not recording_speech:
                return None
            
            # Convertir en format faster-whisper
            audio_data = np.frombuffer(b''.join(frames), dtype=np.int16)
            return audio_data.astype(np.float32) / 32768.0
            
        except Exception as e:
            log.error(f"Erreur VAD: {e}")
            return None
    
    def get_current_config(self) -> Dict[str, Any]:
        """Retourne la configuration actuelle avec infos runtime"""
        return {
            "whisper_config": self.whisper_config,
            "use_faster_whisper": getattr(self, 'use_faster_whisper', False),
            "model_loaded": hasattr(self, 'model'),
            "device_index": self.device_index,
            "vad_enabled": self.whisper_config['vad']['enabled'],
            "components_preloaded": {
                "vad": self.vad is not None,
                "pyaudio": self.pyaudio_instance is not None
            }
        }
    
    def close_audio_resources(self):
        """Ferme proprement les ressources audio"""
        if self.audio_stream:
            self.audio_stream.close()
            self.audio_stream = None
        
        if self.pyaudio_instance:
            self.pyaudio_instance.terminate()
            self.pyaudio_instance = None
        
        log.info("üîá Ressources audio ferm√©es")
    
    def __del__(self):
        """Destructeur pour nettoyer automatiquement"""
        self.close_audio_resources()

# Point d'entr√©e pour tests
if __name__ == "__main__":
    # Test de la classe
    stt = SpeechToText()
    print("Configuration actuelle:")
    import json
    print(json.dumps(stt.get_current_config(), indent=2, ensure_ascii=False))

==================================================
FICHIER: .\lobes_temporaux\tts.py
==================================================

"""
tts.py - Orchestrateur TTS simplifi√© et unifi√©
REFACTORIS√â: Utilise AudioGenerator + AudioPipeline + VoiceCloner
"""

from pathlib import Path
import asyncio
import tempfile
import time
from typing import Optional, List, Dict, Any

# Import logger
import sys
sys.path.append(str(Path(__file__).parent.parent))

try:
    from hypothalamus.logger import log
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO)
    log = logging.getLogger(__name__)

# Modules de gestions des voix
from .audio_generator import AudioGenerator
from .audio_pipeline import AudioPipeline, split_text_for_streaming
from .voice_cloner import VoiceCloner


class TextToSpeech:
    """
    Orchestrateur TTS unifi√© et simplifi√©
    RESPONSABILIT√âS:
    - Initialiser et configurer les composants
    - Coordonner g√©n√©ration + pipeline + clonage
    - API publique pour l'application
    """
    
    def __init__(self, model_name=None, personality="Jarvis", edge_voice=None, sample_path=None):
        """
        Initialise le TTS unifi√©
        
        Args:
            model_name: Type de mod√®le (pour compatibilit√© legacy)
            personality: Nom de la voix/personnalit√©
            edge_voice: Voix Edge-TTS (pour compatibilit√© legacy)
            sample_path: Chemin √©chantillon (pour compatibilit√© legacy)
        """
        self.personality = personality
        
        # NOUVEAU: Modules unifi√©s
        self.voice_cloner = VoiceCloner()
        self.audio_generator = AudioGenerator()
        
        # R√©soudre la configuration voix
        self.voice_config = self._resolve_voice_config(model_name, personality, edge_voice, sample_path)
        
        if not self.voice_config:
            log.error(f"Configuration voix impossible pour '{personality}'")
            raise ValueError(f"Voix '{personality}' non trouv√©e")
        
        # Initialiser le pipeline avec la config
        self.pipeline = AudioPipeline(self.audio_generator, self.voice_config)
        
        # Statistiques et √©tat
        self.stats = {
            'conversations_handled': 0,
            'total_chunks_processed': 0,
            'errors': 0
        }
        
        log.success(f"TTS initialis√© - {personality} ({self.voice_config.get('model', 'unknown')})")
        log.info(f"Voice config: {self.voice_config}")
    
    def _resolve_voice_config(self, model_name, personality, edge_voice, sample_path):
        """
        R√©sout la configuration voix depuis les param√®tres
        Priorit√©: VoiceCloner > Param√®tres legacy > D√©faut
        """
        
        # PRIORIT√â 1: Chercher dans VoiceCloner (voix configur√©es)
        voice_config = self.voice_cloner.get_voice_config(personality)
        if voice_config:
            log.debug(f"Configuration trouv√©e dans VoiceCloner pour '{personality}'")
            return voice_config
        
        # PRIORIT√â 2: Construire depuis param√®tres legacy
        if model_name:
            log.debug(f"Construction config depuis param√®tres legacy: {model_name}")
            return self._build_legacy_voice_config(model_name, personality, edge_voice, sample_path)
        
        # PRIORIT√â 3: Essayer voix par d√©faut
        default_voice = self.voice_cloner.voices_config.get('default_voice', 'jarvis')
        default_config = self.voice_cloner.get_voice_config(default_voice)
        if default_config:
            log.warning(f"Utilisation voix par d√©faut '{default_voice}' au lieu de '{personality}'")
            return default_config
        
        # PRIORIT√â 4: Fallback Edge-TTS
        log.warning(f"Fallback Edge-TTS pour '{personality}'")
        return {
            'model': 'edge-tts',
            'edge_voice': 'fr-FR-DeniseNeural',
            'personality_config': {
                'voice_speed': 1.0,
                'voice_volume': 90
            }
        }
    
    def _build_legacy_voice_config(self, model_name, personality, edge_voice, sample_path):
        """Construit voice_config depuis param√®tres legacy"""
        
        if model_name == "edge-tts":
            return {
                'model': 'edge-tts',
                'edge_voice': edge_voice or "fr-FR-DeniseNeural",
                'personality_config': {
                    'voice_speed': 1.0,
                    'voice_volume': 90
                }
            }
        
        elif model_name == "xtts-v2":
            if not sample_path:
                log.error("sample_path requis pour XTTS-v2")
                return None
                
            return {
                'model': 'xtts-v2',
                'sample_path': sample_path,
                'personality_config': {
                    'voice_speed': 1.0
                }
            }
        
        else:
            # Coqui
            return {
                'model': model_name,
                'personality_config': {
                    'voice_speed': 1.0
                }
            }
    
    # ========================================================================
    # API PUBLIQUE - STREAMING INTELLIGENT
    # ========================================================================
    
    async def speak_streaming(self, text: str, chunk_size: int = 150):
        """
        API principale - Parle avec streaming intelligent
        
        Args:
            text: Texte √† synth√©tiser
            chunk_size: Taille max des chunks pour streaming
        """
        if not text.strip():
            return
        
        try:
            self.stats['conversations_handled'] += 1
            
            log.jarvis(f"üó£Ô∏è {self.personality}: {text}")
            
            # D√©couper en chunks optimaux
            chunks = split_text_for_streaming(text, chunk_size)
            self.stats['total_chunks_processed'] += len(chunks)
            
            log.debug(f"Streaming: {len(chunks)} chunks")
            
            # D√©marrer pipeline si n√©cessaire
            if not self.pipeline.pipeline_active:
                self.pipeline.start_streaming_workers()
            
            # Envoyer tous les chunks en parall√®le
            chunk_ids = await self.pipeline.queue_text_chunks(chunks)
            
            # Attendre que tous soient trait√©s (optionnel pour debug)
            await self._wait_for_chunks_completion(chunk_ids)
            
        except Exception as e:
            log.error(f"Erreur speak_streaming: {e}")
            self.stats['errors'] += 1
    
    async def speak_simple(self, text: str):
        """
        API simple - Une seule g√©n√©ration sans streaming
        Utile pour courts textes ou tests
        """
        try:
            log.debug(f"G√©n√©ration simple: {text[:50]}...")
            
            # G√©n√©rer directement
            audio_data = await self.audio_generator.generate_audio(text, self.voice_config)
            
            if audio_data:
                # Jouer imm√©diatement
                await self._play_audio_data(audio_data)
                log.success("Lecture simple termin√©e")
            else:
                log.error("G√©n√©ration simple √©chou√©e")
                
        except Exception as e:
            log.error(f"Erreur speak_simple: {e}")
            self.stats['errors'] += 1
    
    async def _wait_for_chunks_completion(self, chunk_ids: list, timeout: float = 30.0):
        """Attend que tous les chunks soient trait√©s (pour debug/sync)"""
        start_time = time.time()
        
        while time.time() - start_time < timeout:
            status = self.pipeline.get_status()
            
            # Si plus rien en queue, c'est probablement fini
            if (status['chunks_in_generation_queue'] == 0 and 
                status['chunks_in_playback_queue'] == 0):
                break
            
            await asyncio.sleep(0.5)
    
    async def _play_audio_data(self, audio_data: bytes):
        """Joue des donn√©es audio directement avec pygame"""
        try:
            import pygame
            
            # Pr√©-init pygame si n√©cessaire
            if not pygame.mixer.get_init():
                pygame.mixer.pre_init(frequency=22050, size=-16, channels=2, buffer=512)
                pygame.mixer.init()
            
            # Fichier temporaire
            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')
            temp_file.write(audio_data)
            temp_file.close()
            
            # Jouer
            pygame.mixer.music.load(temp_file.name)
            pygame.mixer.music.play()
            
            # Attendre fin
            while pygame.mixer.music.get_busy():
                await asyncio.sleep(0.1)
            
            pygame.mixer.music.stop()
            
            # Nettoyer
            import os
            try:
                os.unlink(temp_file.name)
            except:
                pass
                
        except Exception as e:
            log.error(f"Erreur lecture audio: {e}")
    
    # ========================================================================
    # GESTION DYNAMIQUE DES VOIX
    # ========================================================================
    
    async def switch_voice(self, new_personality: str):
        """Change de voix dynamiquement"""
        try:
            log.info(f"Changement voix: {self.personality} ‚Üí {new_personality}")
            
            # R√©soudre nouvelle config
            new_voice_config = self.voice_cloner.get_voice_config(new_personality)
            
            if not new_voice_config:
                log.error(f"Voix '{new_personality}' non trouv√©e")
                return False
            
            # Mettre √† jour
            self.personality = new_personality
            self.voice_config = new_voice_config
            
            # Mettre √† jour le pipeline
            self.pipeline.update_voice_config(new_voice_config)
            
            log.success(f"Voix chang√©e: {new_personality}")
            return True
            
        except Exception as e:
            log.error(f"Erreur changement voix: {e}")
            return False
    
    def list_available_voices(self) -> Dict[str, List]:
        """Liste toutes les voix disponibles"""
        voices_data = self.voice_cloner.get_all_voices()
        
        # S√©parer standard et clon√©es pour l'affichage
        standard_voices = []
        cloned_voices = []
        
        # Voix standard
        for voice_id, voice_data in voices_data['voices'].items():
            standard_voices.append({
                'id': voice_id,
                'name': voice_data.get('name', voice_id),
                'display_name': voice_data.get('display_name', voice_data.get('name', voice_id)),
                'model': voice_data.get('model', 'edge-tts')
            })
        
        # Voix clon√©es
        for voice_id, voice_data in voices_data['cloned_voices'].items():
            if voice_data.get('processing_status') == 'ready':
                cloned_voices.append({
                    'id': voice_id,
                    'name': voice_data.get('name', voice_id),
                    'display_name': voice_data.get('display_name', voice_data.get('name', voice_id)),
                    'model': 'xtts-v2',
                    'status': voice_data.get('processing_status', 'unknown')
                })
        
        return {
            'standard_voices': standard_voices,
            'cloned_voices': cloned_voices,
            'default_voice': voices_data.get('default_voice', 'jarvis')
        }
    
    # ========================================================================
    # M√âTHODES LEGACY (COMPATIBILIT√â)
    # ========================================================================
    
    async def speak(self, text: str):
        """M√©thode legacy - D√©l√®gue √† speak_streaming"""
        await self.speak_streaming(text)
    
    async def add_to_queue(self, text: str):
        """M√©thode legacy - Queue un chunk"""
        if self.pipeline:
            await self.pipeline.queue_text_chunk(text)
        else:
            await self.speak_simple(text)
    
    def start_tts_worker(self):
        """M√©thode legacy - D√©marre le pipeline"""
        if self.pipeline:
            self.pipeline.start_streaming_workers()
    
    def stop_tts_worker(self):
        """M√©thode legacy - Arr√™te le pipeline"""
        if self.pipeline:
            self.pipeline.stop_pipeline()
    
    # ========================================================================
    # STATUT ET DEBUG
    # ========================================================================
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut complet du syst√®me TTS"""
        base_status = {
            'personality': self.personality,
            'voice_config': self.voice_config,
            'stats': self.stats.copy()
        }
        
        # Ajouter statuts des composants
        if hasattr(self, 'audio_generator'):
            base_status['audio_generator'] = self.audio_generator.get_status()
        
        if hasattr(self, 'pipeline'):
            base_status['pipeline'] = self.pipeline.get_status()
        
        if hasattr(self, 'voice_cloner'):
            base_status['voice_cloner'] = self.voice_cloner.get_status()
        
        return base_status
    
    def get_personality(self) -> str:
        """Retourne la personnalit√© actuelle"""
        return self.personality
    
    def get_display_name(self) -> str:
        """Retourne le nom d'affichage format√©"""
        voice_data = self.voice_cloner.get_voice_config(self.personality)
        if voice_data and 'display_name' in voice_data:
            return voice_data['display_name']
        else:
            return f"Assistant virtuel - {self.personality}"
    
    # ========================================================================
    # OPTIMISATIONS ET R√âGLAGES
    # ========================================================================
    
    def update_voice_settings(self, speed: float = None, volume: int = None):
        """Met √† jour les r√©glages voix dynamiquement"""
        if 'personality_config' not in self.voice_config:
            self.voice_config['personality_config'] = {}
        
        if speed is not None:
            self.voice_config['personality_config']['voice_speed'] = max(0.5, min(2.0, speed))
            log.debug(f"Vitesse voix: {speed}")
        
        if volume is not None:
            self.voice_config['personality_config']['voice_volume'] = max(0, min(100, volume))
            log.debug(f"Volume voix: {volume}")
        
        # Propager au pipeline
        if self.pipeline:
            self.pipeline.update_voice_config(self.voice_config)
    
    def set_chunk_size(self, size: int):
        """Configure la taille des chunks pour streaming"""
        self.chunk_size = max(50, min(300, size))
        log.debug(f"Taille chunks: {self.chunk_size}")
    
    # ========================================================================
    # NETTOYAGE
    # ========================================================================
    
    def cleanup(self):
        """Nettoie toutes les ressources"""
        log.debug("Nettoyage TTS...")
        
        # Arr√™ter pipeline
        if hasattr(self, 'pipeline'):
            self.pipeline.stop_pipeline()
        
        # Nettoyer composants
        if hasattr(self, 'audio_generator'):
            self.audio_generator.cleanup()
        
        if hasattr(self, 'voice_cloner'):
            self.voice_cloner.cleanup()
        
        log.debug("TTS nettoy√©")
    
    def __del__(self):
        """Destructeur - Nettoyage automatique"""
        try:
            self.cleanup()
        except:
            pass


# ============================================================================
# UTILITAIRES ET HELPERS
# ============================================================================

def create_tts_from_voice_name(voice_name: str) -> TextToSpeech:
    """
    Factory function - Cr√©e un TTS depuis un nom de voix
    Remplace les anciens appels directs au constructeur
    """
    return TextToSpeech(personality=voice_name)


def create_edge_tts(voice_name: str, edge_voice: str) -> TextToSpeech:
    """Factory function - Cr√©e un TTS Edge-TTS"""
    return TextToSpeech(
        model_name="edge-tts",
        personality=voice_name,
        edge_voice=edge_voice
    )


def create_xtts(voice_name: str, sample_path: str) -> TextToSpeech:
    """Factory function - Cr√©e un TTS XTTS"""
    return TextToSpeech(
        model_name="xtts-v2",
        personality=voice_name,
        sample_path=sample_path
    )


# ============================================================================
# EXEMPLE D'UTILISATION
# ============================================================================

async def example_usage():
    """Exemple d'utilisation du nouveau TTS"""
    
    print("üé§ Nouveau TTS unifi√© - Exemple")
    
    # Cr√©er TTS depuis nom de voix (cherche dans VoiceCloner)
    tts = create_tts_from_voice_name("jarvis")
    
    # Lister voix disponibles
    voices = tts.list_available_voices()
    print(f"Voix standard: {len(voices['standard_voices'])}")
    print(f"Voix clon√©es: {len(voices['cloned_voices'])}")
    
    # Parler avec streaming
    await tts.speak_streaming("Bonjour ! Ceci est un test du nouveau syst√®me TTS unifi√©. Il utilise AudioGenerator pour la g√©n√©ration et AudioPipeline pour le streaming.")
    
    # Changer de voix si disponible
    if voices['cloned_voices']:
        cloned_voice = voices['cloned_voices'][0]
        success = await tts.switch_voice(cloned_voice['name'])
        if success:
            await tts.speak_streaming("Maintenant je parle avec une voix clon√©e !")
    
    # Statut final
    status = tts.get_status()
    print(f"Statistiques: {status['stats']}")
    
    # Nettoyage
    tts.cleanup()


if __name__ == "__main__":
    print("üó£Ô∏è TTS Unifi√© - Architecture compl√®te")
    
    # Test simple
    try:
        tts = create_edge_tts("Test", "fr-FR-DeniseNeural")
        print(f"TTS cr√©√©: {tts.personality}")
        print(f"Config: {tts.voice_config}")
        print(f"Statut: {tts.get_status()}")
        
        # Test voix disponibles
        voices = tts.list_available_voices()
        print(f"Voix disponibles: {len(voices['standard_voices']) + len(voices['cloned_voices'])}")
        
    except Exception as e:
        print(f"Erreur test: {e}")
    
    # Test async complet (d√©comment√© si voulu)
    # asyncio.run(example_usage())

==================================================
FICHIER: .\lobes_temporaux\voice_cloner.py
==================================================

"""
voice_cloner.py - Gestionnaire pur de clonage vocal
REFACTORIS√â: Ne fait plus que du clonage, pas de g√©n√©ration audio
"""

import os
import json
import time
import base64
import shutil
import hashlib
import tempfile
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, List, Union

# Audio processing
import numpy as np
import wave

# Import logger
import sys
sys.path.append(str(Path(__file__).parent.parent))

try:
    from hypothalamus.logger import log
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO)
    log = logging.getLogger(__name__)


class VoiceCloner:
    """
    Gestionnaire pur de clonage vocal avec int√©gration voices.json
    RESPONSABILIT√â UNIQUE: Cloner, g√©rer et configurer les voix
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialise le gestionnaire de clonage vocal"""
        
        # Chemins de configuration
        self.config_dir = Path(__file__).parent.parent / "config"
        self.voices_json_path = self.config_dir / "voices.json"
        self.cloned_voices_dir = self.config_dir / "cloned_voices"
        self.cloned_voices_dir.mkdir(exist_ok=True, parents=True)
        
        # Cr√©er les sous-dossiers
        self.samples_dir = self.cloned_voices_dir / "samples"
        self.models_dir = self.cloned_voices_dir / "models"
        self.samples_dir.mkdir(exist_ok=True)
        self.models_dir.mkdir(exist_ok=True)
        
        # Charger la configuration voices.json
        self.voices_config = self.load_voices_config()
        
        # √âtat XTTS pour calcul embeddings
        self.xtts_model = None
        self.xtts_loaded = False
        self.is_processing = False
        
        log.info(f"VoiceCloner initialis√© - {self.count_cloned_voices()} voix clon√©es")
    
    # ========================================================================
    # GESTION CONFIGURATION VOICES.JSON
    # ========================================================================
    
    def load_voices_config(self) -> Dict[str, Any]:
        """Charge le fichier voices.json"""
        try:
            with open(self.voices_json_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # Ajouter sections manquantes
            if 'cloned_voices' not in config:
                config['cloned_voices'] = {}
            if 'voices' not in config:
                config['voices'] = {}
            if 'default_voice' not in config:
                config['default_voice'] = 'jarvis'
            
            return config
        except Exception as e:
            log.error(f"Erreur chargement voices.json: {e}")
            return {
                "voices": {},
                "cloned_voices": {},
                "default_voice": "jarvis",
                "demo_text": "Test de voix clon√©e"
            }
    
    def save_voices_config(self):
        """Sauvegarde voices.json avec les voix clon√©es"""
        try:
            with open(self.voices_json_path, 'w', encoding='utf-8') as f:
                json.dump(self.voices_config, f, indent=2, ensure_ascii=False)
            log.debug("Configuration voices.json sauvegard√©e")
        except Exception as e:
            log.error(f"Erreur sauvegarde voices.json: {e}")
    
    def get_voice_config(self, voice_id: str) -> Optional[Dict[str, Any]]:
        """
        R√©cup√®re la configuration compl√®te d'une voix
        
        Args:
            voice_id: ID de la voix (personnalit√© ou ID technique)
            
        Returns:
            Configuration voix compatible AudioGenerator ou None
        """
        # Chercher dans les voix standards
        for vid, voice_data in self.voices_config.get('voices', {}).items():
            if vid == voice_id or voice_data.get('name') == voice_id:
                return {
                    'model': voice_data.get('model', 'edge-tts'),
                    'edge_voice': voice_data.get('edge_voice'),
                    'personality_config': voice_data.get('personality_config', {
                        'voice_speed': 1.0,
                        'voice_volume': 90
                    })
                }
        
        # Chercher dans les voix clon√©es
        for vid, voice_data in self.voices_config.get('cloned_voices', {}).items():
            if vid == voice_id or voice_data.get('name') == voice_id:
                config = {
                    'model': 'xtts-v2',
                    'sample_path': voice_data.get('sample_path'),
                    'personality_config': voice_data.get('personality_config', {
                        'voice_speed': 1.0
                    })
                }
                
                # ‚úÖ CORRECTION: Utiliser embedding_path de la config si pr√©sent
                if 'embedding_path' in voice_data:
                    config['embedding_path'] = voice_data['embedding_path']
                else:
                    # Fallback: calcul dynamique pour compatibilit√©
                    config['embedding_path'] = self._get_embedding_path(voice_data.get('sample_path'))
                
                return config
        
        log.warning(f"Voix non trouv√©e: {voice_id}")
        return None
    
    def _get_embedding_path(self, sample_path: str) -> Optional[str]:
        """Retourne le chemin vers l'embedding pr√©-calcul√© si il existe"""
        if not sample_path:
            return None
        
        sample_path = Path(sample_path)
        if not sample_path.is_absolute():
            sample_path = self.config_dir / sample_path
        
        embedding_path = sample_path.with_suffix('.pt')
        return str(embedding_path) if embedding_path.exists() else None
    
    def get_all_voices(self) -> Dict[str, Any]:
        """Retourne toutes les voix (standard + clon√©es) dans un format unifi√©"""
        return {
            'success': True, 
            'voices': self.voices_config.get('voices', {}),
            'cloned_voices': self.voices_config.get('cloned_voices', {}),
            'default_voice': self.voices_config.get('default_voice', 'jarvis')
        }
    
    def list_cloned_voices(self) -> List[Dict[str, Any]]:
        """Retourne la liste des voix clon√©es avec m√©tadonn√©es"""
        voices_list = []
        
        for voice_id, voice in self.voices_config.get('cloned_voices', {}).items():
            voices_list.append({
                'id': voice_id,
                'name': voice['name'],
                'display_name': voice['display_name'],
                'description': voice['description'],
                'duration': voice.get('duration', 0),
                'status': voice.get('processing_status', 'unknown'),
                'created_at': voice.get('created_at'),
                'model': voice.get('model', 'xtts-v2'),
                'sample_path': voice.get('sample_path'),
                'has_embedding': self._get_embedding_path(voice.get('sample_path')) is not None
            })
        
        # Trier par date de cr√©ation
        voices_list.sort(key=lambda x: x.get('created_at', 0), reverse=True)
        
        return voices_list
    
    def count_cloned_voices(self) -> int:
        """Compte le nombre de voix clon√©es"""
        return len(self.voices_config.get('cloned_voices', {}))
    
    def set_default_voice(self, voice_id: str) -> Dict[str, Any]:
        """D√©finit une voix comme voix par d√©faut"""
        # V√©rifier que la voix existe
        all_voices = {
            **self.voices_config.get('voices', {}),
            **self.voices_config.get('cloned_voices', {})
        }
        
        if voice_id not in all_voices:
            return {'success': False, 'error': 'Voix non trouv√©e'}
        
        self.voices_config['default_voice'] = voice_id
        self.save_voices_config()
        
        voice_name = all_voices[voice_id]['name']
        log.info(f"Voix par d√©faut: {voice_name}")
        
        return {
            'success': True,
            'voice_id': voice_id,
            'voice_name': voice_name
        }
    
    # ========================================================================
    # CLONAGE VOCAL
    # ========================================================================
    
    async def clone_voice(
        self,
        audio_data: bytes,
        voice_name: str,
        description: str = "",
        file_type: str = 'audio'
    ) -> Dict[str, Any]:
        """
        Clone une voix et l'ajoute √† voices.json
        
        Args:
            audio_data: Donn√©es audio WAV/MP4/etc.
            voice_name: Nom de la voix
            description: Description optionnelle
            file_type: 'audio' ou 'video'
            
        Returns:
            R√©sultat du clonage avec voice_id
        """
        if self.is_processing:
            return {'success': False, 'error': 'Traitement en cours, veuillez patienter'}
        
        self.is_processing = True
        
        try:
            # Valider l'audio
            validation = self.validate_audio_file(audio_data, file_type)
            
            if not validation['valid']:
                return {'success': False, 'error': validation['error']}
            
            # G√©n√©rer un ID unique
            voice_id = f"cloned_{hashlib.md5(f'{voice_name}_{time.time()}'.encode()).hexdigest()[:8]}"
            
            # Sauvegarder l'√©chantillon WAV
            sample_path = self.samples_dir / f"{voice_id}.wav"
            with open(sample_path, 'wb') as f:
                f.write(validation['wav_data'])
            
            log.info(f"√âchantillon sauvegard√©: {sample_path.name}")
            
            # Cr√©er l'entr√©e dans voices.json
            voice_entry = {
                "id": voice_id,
                "name": voice_name,
                "display_name": f"üé≠ {voice_name}",
                "gender": "unknown",
                "model": "xtts-v2",
                "sample_path": str(sample_path.relative_to(self.config_dir)),
                "description": description or f"Voix clon√©e le {time.strftime('%d/%m/%Y')}",
                "voice_type": "cloned",
                "duration": validation['duration'],
                "sample_rate": validation['sample_rate'],
                "created_at": time.time(),
                "personality_config": {
                    "voice_speed": 1.0,
                    "voice_volume": 90
                },
                "processing_status": "pending"
            }
            
            # Ajouter √† la configuration
            self.voices_config['cloned_voices'][voice_id] = voice_entry
            self.save_voices_config()
            log.debug(f"üîç [TRACE] Voix cr√©√©e - ID: {voice_id}, Name: {voice_name}")
            
            # Traiter l'embedding si XTTS est disponible
            if await self.initialize_xtts():
                success = await self._process_voice_embedding(voice_id, sample_path)
                if success:
                    voice_entry['processing_status'] = 'ready'
                    # ‚úÖ CORRECTION CLEF: Ajouter embedding_path √† la config
                    embedding_path = sample_path.with_suffix('.pt')
                    voice_entry['embedding_path'] = str(embedding_path.relative_to(self.config_dir))
                else:
                    voice_entry['processing_status'] = 'failed'
                self.save_voices_config()
            else:
                voice_entry['processing_status'] = 'no_model'
                log.warning("XTTS non disponible, voix sauvegard√©e sans embedding")
            
            log.success(f"Voix '{voice_name}' clon√©e avec succ√®s (ID: {voice_id})")
            
            return {
                'success': True,
                'voice_id': voice_id,
                'voice_name': voice_name,
                'duration': validation['duration'],
                'status': voice_entry['processing_status']
            }
            
        except Exception as e:
            log.error(f"Erreur clonage: {e}")
            return {'success': False, 'error': str(e)}
        finally:
            self.is_processing = False
    
    def validate_audio_file(self, audio_data: bytes, file_type: str = 'audio') -> Dict[str, Any]:
        """
        Valide un fichier audio pour le clonage
        
        Args:
            audio_data: Donn√©es binaires du fichier
            file_type: Type de fichier ('audio' ou 'video')
            
        Returns:
            Dictionnaire avec 'valid', 'error', 'wav_data', 'duration', 'sample_rate'
        """
        try:
            # √âcrire dans un fichier temporaire pour traitement
            with tempfile.NamedTemporaryFile(delete=False, suffix='.tmp') as temp_file:
                temp_file.write(audio_data)
                temp_input_path = temp_file.name
            
            # Fichier de sortie WAV
            temp_output_path = temp_input_path.replace('.tmp', '.wav')
            
            try:
                # Conversion avec ffmpeg
                cmd = [
                    'ffmpeg', '-y', '-i', temp_input_path,
                    '-ar', '22050',  # Fr√©quence d'√©chantillonnage pour XTTS
                    '-ac', '1',      # Mono
                    '-f', 'wav',
                    temp_output_path
                ]
                
                # Ex√©cuter ffmpeg silencieusement
                result = subprocess.run(
                    cmd, 
                    capture_output=True, 
                    text=True,
                    timeout=30
                )
                
                if result.returncode != 0:
                    return {
                        'valid': False,
                        'error': f"Conversion ffmpeg √©chou√©e: {result.stderr[:100]}"
                    }
                
                log.debug("Conversion r√©ussie: audio ‚Üí WAV")
                
                # Lire le fichier converti
                with open(temp_output_path, 'rb') as f:
                    wav_data = f.read()
                
                # Analyser la dur√©e avec wave
                with wave.open(temp_output_path, 'rb') as wav_file:
                    frames = wav_file.getnframes()
                    sample_rate = wav_file.getframerate()
                    duration = frames / float(sample_rate)
                
                # Validation dur√©e
                if duration < 5.0:
                    return {
                        'valid': False,
                        'error': f"√âchantillon trop court: {duration:.1f}s (min 5s requis)"
                    }
                
                if duration > 60.0:
                    return {
                        'valid': False,
                        'error': f"√âchantillon trop long: {duration:.1f}s (max 60s)"
                    }
                
                return {
                    'valid': True,
                    'wav_data': wav_data,
                    'duration': duration,
                    'sample_rate': sample_rate
                }
                
            finally:
                # Nettoyer les fichiers temporaires
                try:
                    os.remove(temp_input_path)
                except:
                    pass
                try:
                    os.remove(temp_output_path)
                except:
                    pass
                    
        except subprocess.TimeoutExpired:
            return {'valid': False, 'error': 'Timeout conversion audio'}
        except Exception as e:
            return {'valid': False, 'error': f"Erreur validation: {str(e)}"}
    
    # ========================================================================
    # GESTION EMBEDDINGS XTTS
    # ========================================================================
    
    async def initialize_xtts(self) -> bool:
        """Initialise XTTS pour calcul des embeddings"""
        if self.xtts_loaded:
            return True
        
        try:
            # R√©duire verbosit√©
            import logging
            logging.getLogger('TTS').setLevel(logging.ERROR)
            
            from TTS.api import TTS
            import torch
            
            # Charger le mod√®le XTTS
            self.xtts_model = TTS("tts_models/multilingual/multi-dataset/xtts_v2")
            
            log.info("XTTS initialis√© pour embeddings")
            self.xtts_loaded = True
            return True
            
        except ImportError:
            log.warning("TTS non install√© - Embeddings non disponibles")
            return False
        except Exception as e:
            log.error(f"Erreur initialisation XTTS: {e}")
            return False
    
    async def _process_voice_embedding(self, voice_id: str, sample_path: Path) -> bool:
        """
        Calcule et sauvegarde l'embedding vocal pour utilisation rapide
        OPTIMISATION: √âvite de recalculer lors de chaque g√©n√©ration
        """
        try:
            if not self.xtts_loaded:
                log.warning("XTTS non charg√©, embedding non calcul√©")
                return False
            
            log.info(f"Calcul embedding pour {voice_id}...")
            
            # Calculer les embeddings avec XTTS
            conditioning_latents = self.xtts_model.synthesizer.tts_model.get_conditioning_latents(
                audio_path=str(sample_path)
            )
            
            # Sauvegarder l'embedding dans un fichier .pt
            import torch
            embedding_path = sample_path.with_suffix('.pt')
            
            torch.save({
                'gpt_cond_latent': conditioning_latents[0],
                'speaker_embedding': conditioning_latents[1],
                'voice_id': voice_id,
                'created_at': time.time()
            }, str(embedding_path))
            
            log.success(f"Embedding sauvegard√© : {embedding_path.name}")
            log.info("‚ö° Les prochaines g√©n√©rations seront 2-3x plus rapides")
            return True
            
        except Exception as e:
            log.error(f"Erreur calcul embedding: {e}")
            import traceback
            log.debug(traceback.format_exc())
            return False
    
    def has_embedding(self, voice_id: str) -> bool:
        """V√©rifie si une voix a un embedding pr√©-calcul√©"""
        voice_data = self.voices_config.get('cloned_voices', {}).get(voice_id)
        if not voice_data:
            return False
        
        sample_path = voice_data.get('sample_path')
        if not sample_path:
            return False
        
        return self._get_embedding_path(sample_path) is not None
    
    async def recalculate_embedding(self, voice_id: str) -> Dict[str, Any]:
        """Recalcule l'embedding d'une voix existante"""
        voice_data = self.voices_config.get('cloned_voices', {}).get(voice_id)
        if not voice_data:
            return {'success': False, 'error': 'Voix non trouv√©e'}
        
        sample_path = self.config_dir / voice_data['sample_path']
        if not sample_path.exists():
            return {'success': False, 'error': 'Fichier √©chantillon manquant'}
        
        if not await self.initialize_xtts():
            return {'success': False, 'error': 'XTTS non disponible'}
        
        success = await self._process_voice_embedding(voice_id, sample_path)
        
        if success:
            voice_data['processing_status'] = 'ready'
            # ‚úÖ CORRECTION: Ajouter embedding_path apr√®s recalcul aussi
            embedding_path = sample_path.with_suffix('.pt')
            voice_data['embedding_path'] = str(embedding_path.relative_to(self.config_dir))
            self.save_voices_config()
            return {'success': True, 'message': 'Embedding recalcul√©'}
        else:
            return {'success': False, 'error': 'Calcul embedding √©chou√©'}
    
    # ========================================================================
    # GESTION VOIX
    # ========================================================================
    
    def rename_voice(self, voice_id: str, new_name: str, new_description: str = None) -> Dict[str, Any]:
        """Renomme une voix clon√©e"""
        if voice_id not in self.voices_config.get('cloned_voices', {}):
            return {'success': False, 'error': 'Voix non trouv√©e'}
        
        try:
            voice = self.voices_config['cloned_voices'][voice_id]
            voice['name'] = new_name
            voice['display_name'] = f"üé≠ {new_name}"
            
            if new_description:
                voice['description'] = new_description
            
            self.save_voices_config()
            
            log.info(f"Voix {voice_id} renomm√©e en '{new_name}'")
            return {'success': True, 'message': f"Voix renomm√©e en '{new_name}'"}
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def delete_voice(self, voice_id: str) -> Dict[str, Any]:
        """Supprime une voix clon√©e"""
        if voice_id not in self.voices_config.get('cloned_voices', {}):
            return {'success': False, 'error': 'Voix non trouv√©e'}
        
        try:
            voice = self.voices_config['cloned_voices'][voice_id]
            voice_name = voice['name']
            
            # Supprimer les fichiers
            sample_path = self.config_dir / voice['sample_path']
            if sample_path.exists():
                os.remove(sample_path)
                log.debug(f"√âchantillon supprim√©: {sample_path}")
            
            # Supprimer embedding si il existe
            embedding_path = sample_path.with_suffix('.pt')
            if embedding_path.exists():
                os.remove(embedding_path)
                log.debug(f"Embedding supprim√©: {embedding_path}")
            
            # Supprimer de la configuration
            del self.voices_config['cloned_voices'][voice_id]
            
            # Changer voix par d√©faut si n√©cessaire
            if self.voices_config.get('default_voice') == voice_id:
                self.voices_config['default_voice'] = 'jarvis'
            
            self.save_voices_config()
            
            log.info(f"Voix '{voice_name}' supprim√©e")
            return {'success': True, 'message': f"Voix '{voice_name}' supprim√©e"}
            
        except Exception as e:
            log.error(f"Erreur suppression voix: {e}")
            return {'success': False, 'error': str(e)}
    
    # ========================================================================
    # IMPORT/EXPORT
    # ========================================================================
    
    def export_voice(self, voice_id: str) -> Optional[Dict[str, Any]]:
        """Exporte une voix clon√©e pour sauvegarde/partage"""
        if voice_id not in self.voices_config.get('cloned_voices', {}):
            return None
        
        try:
            voice = self.voices_config['cloned_voices'][voice_id].copy()
            
            # Inclure l'audio en base64
            sample_path = self.config_dir / voice['sample_path']
            if sample_path.exists():
                with open(sample_path, 'rb') as f:
                    voice['audio_base64'] = base64.b64encode(f.read()).decode()
            
            # M√©tadonn√©es d'export
            voice['export_date'] = time.time()
            voice['export_version'] = '1.0'
            
            return voice
            
        except Exception as e:
            log.error(f"Erreur export: {e}")
            return None
    
    async def import_voice(self, voice_data: Dict[str, Any]) -> Dict[str, Any]:
        """Importe une voix export√©e"""
        try:
            if 'audio_base64' not in voice_data:
                return {'success': False, 'error': 'Donn√©es audio manquantes'}
            
            # D√©coder l'audio
            audio_data = base64.b64decode(voice_data['audio_base64'])
            
            # Recr√©er la voix
            result = await self.clone_voice(
                audio_data=audio_data,
                voice_name=voice_data.get('name', 'Voix import√©e'),
                description=voice_data.get('description', 'Import√©e') + f" (import {time.strftime('%d/%m/%Y')})"
            )
            
            return result
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    # ========================================================================
    # STATUT
    # ========================================================================
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut complet du module"""
        return {
            'xtts_loaded': self.xtts_loaded,
            'is_processing': self.is_processing,
            'total_voices': len(self.get_all_voices()['voices']) + len(self.get_all_voices()['cloned_voices']),
            'cloned_voices': self.count_cloned_voices(),
            'storage_used': self._calculate_storage(),
            'default_voice': self.voices_config.get('default_voice', 'jarvis'),
            'voices_with_embeddings': sum(1 for voice in self.list_cloned_voices() if voice['has_embedding'])
        }
    
    def _calculate_storage(self) -> str:
        """Calcule l'espace utilis√© par les voix clon√©es"""
        try:
            total_size = 0
            
            if self.cloned_voices_dir.exists():
                for file_path in self.cloned_voices_dir.rglob('*'):
                    if file_path.is_file():
                        total_size += file_path.stat().st_size
            
            # Convertir en unit√©s lisibles
            if total_size < 1024 * 1024:
                return f"{total_size / 1024:.1f} KB"
            else:
                return f"{total_size / (1024 * 1024):.1f} MB"
                
        except Exception:
            return "Inconnu"
    
    def cleanup(self):
        """Nettoie les ressources"""
        log.debug("Nettoyage VoiceCloner...")
        
        if self.xtts_model:
            del self.xtts_model
            self.xtts_model = None
            self.xtts_loaded = False
        
        log.debug("VoiceCloner nettoy√©")


# ============================================================================
# EXEMPLE D'UTILISATION
# ============================================================================

async def example_usage():
    """Exemple d'utilisation du VoiceCloner"""
    
    cloner = VoiceCloner()
    
    # Lister les voix existantes
    voices = cloner.list_cloned_voices()
    print(f"Voix clon√©es existantes: {len(voices)}")
    
    # Obtenir la config d'une voix pour AudioGenerator
    if voices:
        voice_id = voices[0]['id']
        config = cloner.get_voice_config(voice_id)
        print(f"Config pour {voice_id}: {config}")
    
    # Statut
    status = cloner.get_status()
    print(f"Statut: {status}")


if __name__ == "__main__":
    print("üé≠ VoiceCloner - Gestionnaire pur de clonage vocal")
    
    # Test de base
    cloner = VoiceCloner()
    print(f"Initialisation: {cloner.count_cloned_voices()} voix clon√©es")
    
    # Test config r√©cup√©ration
    config = cloner.get_voice_config('jarvis')
    print(f"Config voix 'jarvis': {config}")
    
    # asyncio.run(example_usage())

==================================================
FICHIER: .\lobes_temporaux\__init__.py
==================================================

"""
lobes_temporaux - Traitement audio Jarvis (STT/TTS)
"""

from .conversation_flow import ConversationFlow
from .stt import SpeechToText
from .tts import TextToSpeech

__all__ = ['SpeechToText', 'TextToSpeech', 'ConversationFlow']

==================================================
FICHIER: .\thalamus\app_config_endpoints.py
==================================================

"""
api_config_endpoints.py - Endpoints API pour les configurations JSON
√Ä int√©grer dans le serveur FastAPI principal pour servir les configurations
"""

from fastapi import APIRouter, HTTPException
from pathlib import Path
import sys

# Import du gestionnaire de configuration
sys.path.append(str(Path(__file__).parent.parent))
from thalamus.config_loader import ConfigLoader
from thalamus.interface_bridge import InterfaceBridge
from hypothalamus.logger import log

# Cr√©er le routeur API
config_router = APIRouter(prefix="/api", tags=["configuration"])

# Instances globales
config_loader = ConfigLoader()
interface_bridge = InterfaceBridge()

@config_router.get("/voices")
async def get_voices():
    """Retourne la liste des voix disponibles"""
    try:
        voices_data = interface_bridge.get_available_voices()
        return voices_data
    except Exception as e:
        log.error(f"Erreur API /voices: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.get("/models")
async def get_models():
    """Retourne la liste des mod√®les LLM disponibles"""
    try:
        models_data = interface_bridge.get_available_models()
        return models_data
    except Exception as e:
        log.error(f"Erreur API /models: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.get("/themes")
async def get_themes():
    """Retourne la liste des th√®mes disponibles"""
    try:
        themes_data = interface_bridge.get_available_themes()
        return themes_data
    except Exception as e:
        log.error(f"Erreur API /themes: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.get("/backgrounds")
async def get_backgrounds():
    """Retourne la liste des arri√®re-plans disponibles"""
    try:
        backgrounds_data = interface_bridge.get_available_backgrounds()
        return backgrounds_data
    except Exception as e:
        log.error(f"Erreur API /backgrounds: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.get("/demo-text")
async def get_demo_text():
    """Retourne le texte de d√©monstration pour les voix"""
    try:
        demo_text = config_loader.get_demo_text()
        return {
            "success": True,
            "demo_text": demo_text
        }
    except Exception as e:
        log.error(f"Erreur API /demo-text: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.get("/config/status")
async def get_config_status():
    """Retourne le statut des configurations"""
    try:
        status = interface_bridge.get_config_status()
        return status
    except Exception as e:
        log.error(f"Erreur API /config/status: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.post("/reload-config")
async def reload_configurations():
    """Recharge toutes les configurations depuis les fichiers JSON"""
    try:
        result = interface_bridge.reload_configurations()
        log.info("Configurations recharg√©es via API")
        return result
    except Exception as e:
        log.error(f"Erreur API /reload-config: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.get("/voice/{voice_id}")
async def get_voice_config(voice_id: str):
    """Retourne la configuration d√©taill√©e d'une voix"""
    try:
        voice_config = config_loader.get_voice_config(voice_id)
        if voice_config:
            return {
                "success": True,
                "voice": voice_config
            }
        else:
            return {
                "success": False,
                "error": f"Voix '{voice_id}' non trouv√©e"
            }
    except Exception as e:
        log.error(f"Erreur API /voice/{voice_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.get("/model/{model_id}")
async def get_model_config(model_id: str):
    """Retourne la configuration d√©taill√©e d'un mod√®le"""
    try:
        model_config = config_loader.get_model_config(model_id)
        if model_config:
            return {
                "success": True,
                "model": model_config
            }
        else:
            return {
                "success": False,
                "error": f"Mod√®le '{model_id}' non trouv√©"
            }
    except Exception as e:
        log.error(f"Erreur API /model/{model_id}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.get("/defaults")
async def get_defaults():
    """Retourne toutes les valeurs par d√©faut"""
    try:
        return {
            "success": True,
            "defaults": {
                "voice": config_loader.get_default_voice(),
                "model": config_loader.get_default_model(),
                "theme": config_loader.get_default_theme(),
                "background": config_loader.get_default_background()
            }
        }
    except Exception as e:
        log.error(f"Erreur API /defaults: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@config_router.get("/config/all")
async def get_all_configs():
    """Retourne toutes les configurations en une seule fois (pour optimisation)"""
    try:
        return {
            "success": True,
            "configurations": {
                "voices": interface_bridge.get_available_voices(),
                "models": interface_bridge.get_available_models(),
                "themes": interface_bridge.get_available_themes(),
                "backgrounds": interface_bridge.get_available_backgrounds(),
                "defaults": {
                    "voice": config_loader.get_default_voice(),
                    "model": config_loader.get_default_model(),
                    "theme": config_loader.get_default_theme(),
                    "background": config_loader.get_default_background()
                },
                "demo_text": config_loader.get_demo_text()
            }
        }
    except Exception as e:
        log.error(f"Erreur API /config/all: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Endpoint pour mettre √† jour une configuration sp√©cifique
@config_router.post("/config/{config_name}")
async def update_config(config_name: str, config_data: dict):
    """Met √† jour une configuration sp√©cifique"""
    try:
        if config_name not in ['voices', 'models', 'themes', 'backgrounds']:
            raise HTTPException(status_code=400, detail=f"Configuration '{config_name}' non support√©e")
        
        success = config_loader.save_config(config_name, config_data)
        
        if success:
            return {
                "success": True,
                "message": f"Configuration '{config_name}' mise √† jour"
            }
        else:
            return {
                "success": False,
                "error": f"Erreur sauvegarde '{config_name}'"
            }
            
    except Exception as e:
        log.error(f"Erreur API update /config/{config_name}: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# Fonction d'initialisation √† appeler depuis le serveur principal
def setup_config_routes(app):
    """
    Fonction pour configurer les routes de configuration dans l'app FastAPI principale
    
    Usage dans le serveur principal:
    from api_config_endpoints import setup_config_routes
    setup_config_routes(app)
    """
    app.include_router(config_router)
    log.info("Routes de configuration API configur√©es")

# Test standalone des endpoints
if __name__ == "__main__":
    import asyncio
    
    async def test_endpoints():
        print("üß™ Test des endpoints de configuration")
        
        try:
            # Test chargement voix
            voices = await get_voices()
            print(f"‚úÖ Voix: {voices['success']}, count: {len(voices.get('voices', []))}")
            
            # Test chargement mod√®les
            models = await get_models()
            print(f"‚úÖ Mod√®les: {models['success']}, count: {len(models.get('models', []))}")
            
            # Test chargement th√®mes
            themes = await get_themes()
            print(f"‚úÖ Th√®mes: {themes['success']}, count: {len(themes.get('themes', []))}")
            
            # Test chargement arri√®re-plans
            backgrounds = await get_backgrounds()
            print(f"‚úÖ Arri√®re-plans: {backgrounds['success']}, count: {len(backgrounds.get('backgrounds', []))}")
            
            # Test valeurs par d√©faut
            defaults = await get_defaults()
            print(f"‚úÖ D√©fauts: {defaults['success']}")
            if defaults['success']:
                print(f"   Voix: {defaults['defaults']['voice']}")
                print(f"   Mod√®le: {defaults['defaults']['model']}")
                print(f"   Th√®me: {defaults['defaults']['theme']}")
            
            # Test texte d√©mo
            demo = await get_demo_text()
            print(f"‚úÖ Demo text: {demo['success']}")
            if demo['success']:
                print(f"   Texte: '{demo['demo_text'][:50]}...'")
            
            # Test statut config
            status = await get_config_status()
            print(f"‚úÖ Statut: {status['success']}")
            
            print("\n‚úÖ Test endpoints termin√© avec succ√®s")
            
        except Exception as e:
            print(f"‚ùå Erreur test endpoints: {e}")
            import traceback
            traceback.print_exc()
    
    # Ex√©cuter le test
    asyncio.run(test_endpoints())

==================================================
FICHIER: .\thalamus\config_loader.py
==================================================

"""
config_loader.py - Gestionnaire centralis√© des configurations JSON
Charge et g√®re tous les fichiers de param√®tres (voices, models, themes, backgrounds)
"""

import json
from pathlib import Path
from typing import Dict, Any, Optional, List
import sys

# Import logger
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

class ConfigLoader:
    """Gestionnaire centralis√© des configurations JSON"""
    
    def __init__(self, config_dir: Path = None):
        if config_dir is None:
            config_dir = Path(__file__).parent.parent / "config"
        
        self.config_dir = config_dir
        self.config_dir.mkdir(exist_ok=True)
        
        # Cache des configurations
        self._cache = {}
        self._loaded_files = set()
        
        log.info("ConfigLoader initialis√©", "‚öôÔ∏è")
    
    def load_config(self, config_name: str, force_reload: bool = False) -> Dict[str, Any]:
        """
        Charge une configuration depuis son fichier JSON
        
        Args:
            config_name: Nom du fichier sans extension (voices, models, themes, backgrounds)
            force_reload: Force le rechargement m√™me si d√©j√† en cache
        
        Returns:
            Dict contenant la configuration
        """
        if not force_reload and config_name in self._cache:
            return self._cache[config_name]
        
        config_file = self.config_dir / f"{config_name}.json"
        
        try:
            if config_file.exists():
                with open(config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    self._cache[config_name] = config
                    self._loaded_files.add(config_name)
                    log.success(f"Configuration '{config_name}' charg√©e", "üìÅ")
                    return config
            else:
                log.warning(f"Fichier de config '{config_file}' introuvable")
                return self._get_default_config(config_name)
                
        except json.JSONDecodeError as e:
            log.error(f"Erreur JSON dans '{config_name}': {e}")
            return self._get_default_config(config_name)
        except Exception as e:
            log.error(f"Erreur chargement '{config_name}': {e}")
            return self._get_default_config(config_name)
    
    def save_config(self, config_name: str, config: Dict[str, Any]) -> bool:
        """
        Sauvegarde une configuration dans son fichier JSON
        
        Args:
            config_name: Nom du fichier sans extension
            config: Configuration √† sauvegarder
        
        Returns:
            True si succ√®s, False sinon
        """
        config_file = self.config_dir / f"{config_name}.json"
        
        try:
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=4, ensure_ascii=False)
            
            # Mettre √† jour le cache
            self._cache[config_name] = config
            self._loaded_files.add(config_name)
            
            log.success(f"Configuration '{config_name}' sauvegard√©e", "üíæ")
            return True
            
        except Exception as e:
            log.error(f"Erreur sauvegarde '{config_name}': {e}")
            return False
    
    def get_voices(self) -> Dict[str, Any]:
        """Retourne la configuration des voix"""
        return self.load_config('voices')
    
    def get_models(self) -> Dict[str, Any]:
        """Retourne la configuration des mod√®les LLM"""
        return self.load_config('models')
    
    def get_themes(self) -> Dict[str, Any]:
        """Retourne la configuration des th√®mes"""
        return self.load_config('themes')
    
    def get_backgrounds(self) -> Dict[str, Any]:
        """Retourne la configuration des arri√®re-plans"""
        return self.load_config('backgrounds')
    
    def get_voice_list(self) -> List[Dict[str, Any]]:
        """Retourne la liste des voix disponibles pour l'interface"""
        voices_config = self.get_voices()
        return [
            {
                'id': voice_id,
                'name': voice_data['name'],
                'display_name': voice_data['display_name'],
                'description': voice_data['description'],
                'gender': voice_data['gender'],
                'voice_type': voice_data.get('voice_type', 'standard'),
                'age_range': voice_data.get('age_range', 'adulte')
            }
            for voice_id, voice_data in voices_config.get('voices', {}).items()
        ]
    
    def get_model_list(self) -> List[Dict[str, Any]]:
        """Retourne la liste des mod√®les disponibles pour l'interface"""
        models_config = self.get_models()
        return [
            {
                'id': model_id,
                'name': model_data['name'],
                'display_name': model_data['display_name'],
                'description': model_data['description'],
                'size': model_data['size'],
                'speed': model_data['speed'],
                'quality': model_data['quality'],
                'available': model_data['available'],
                'ram_required': model_data['ram_required']
            }
            for model_id, model_data in models_config.get('llm_models', {}).items()
        ]
    
    def get_theme_list(self) -> List[Dict[str, Any]]:
        """Retourne la liste des th√®mes disponibles pour l'interface"""
        themes_config = self.get_themes()
        return [
            {
                'id': theme_id,
                'name': theme_data['current_name'],
                'description': theme_data['description'],
                'css_class': theme_data['css_class'],
                'primary_color': theme_data.get('primary_color', '#ffffff'),
                'text_color': theme_data.get('text_color', '#000000'),
                'accent_color': theme_data.get('accent_color', '#0066cc')
            }
            for theme_id, theme_data in themes_config.get('themes', {}).items()
        ]
    
    def get_background_list(self) -> List[Dict[str, Any]]:
        """Retourne la liste des arri√®re-plans disponibles pour l'interface"""
        backgrounds_config = self.get_backgrounds()
        return [
            {
                'id': bg_id,
                'name': bg_data['name'],
                'description': bg_data['description'],
                'type': bg_data['type'],
                'file': bg_data.get('file'),
                'opacity': bg_data.get('opacity', 0.2),
                'available': bg_data.get('available', True),
                'themes_compatible': bg_data.get('themes_compatible', [])
            }
            for bg_id, bg_data in backgrounds_config.get('backgrounds', {}).items()
            if bg_data.get('available', True)
        ]
    
    def get_default_voice(self) -> str:
        """Retourne l'ID de la voix par d√©faut"""
        voices_config = self.get_voices()
        return voices_config.get('default_voice', 'samantha')
    
    def get_default_model(self) -> str:
        """Retourne l'ID du mod√®le par d√©faut"""
        models_config = self.get_models()
        return models_config.get('config', {}).get('default_model', 'llama3.1:8b')
    
    def get_default_theme(self) -> str:
        """Retourne l'ID du th√®me par d√©faut"""
        themes_config = self.get_themes()
        return themes_config.get('config', {}).get('default_theme', 'light')
    
    def get_default_background(self) -> str:
        """Retourne l'ID de l'arri√®re-plan par d√©faut"""
        backgrounds_config = self.get_backgrounds()
        return backgrounds_config.get('default_background', 'default')
    
    def get_voice_config(self, voice_id: str) -> Optional[Dict[str, Any]]:
        """Retourne la configuration compl√®te d'une voix"""
        voices_config = self.get_voices()
        return voices_config.get('voices', {}).get(voice_id)
    
    def get_model_config(self, model_id: str) -> Optional[Dict[str, Any]]:
        """Retourne la configuration compl√®te d'un mod√®le"""
        models_config = self.get_models()
        return models_config.get('llm_models', {}).get(model_id)
    
    def get_demo_text(self) -> str:
        """Retourne le texte de d√©monstration pour les voix"""
        voices_config = self.get_voices()
        return voices_config.get('demo_text', "Bonjour, je suis votre assistant virtuel.")
    
    def _get_default_config(self, config_name: str) -> Dict[str, Any]:
        """Retourne une configuration par d√©faut en cas d'erreur"""
        defaults = {
            'voices': {
                'voices': {
                    'samantha': {
                        'id': 'samantha',
                        'name': 'Samantha',
                        'display_name': 'Samantha (Par d√©faut)',
                        'gender': 'female',
                        'model': 'edge-tts',
                        'edge_voice': 'fr-FR-DeniseNeural',
                        'description': 'Voix par d√©faut',
                        'audio_config': {'voice_speed': 1.0, 'voice_volume': 85}
                    }
                },
                'default_voice': 'samantha',
                'demo_text': 'Bonjour, je suis votre assistant virtuel.'
            },
            'models': {
                'llm_models': {
                    'llama3.1:8b': {
                        'id': 'llama3.1:8b',
                        'name': 'Llama 3.1 8B',
                        'display_name': 'Llama 3.1 8B (Par d√©faut)',
                        'available': True,
                        'default': True
                    }
                },
                'config': {'default_model': 'llama3.1:8b'}
            },
            'themes': {
                'themes': {
                    'light': {
                        'id': 'light',
                        'current_name': 'Mode Clair',
                        'css_class': 'theme-light'
                    }
                },
                'config': {'default_theme': 'light'}
            },
            'backgrounds': {
                'backgrounds': {
                    'default': {
                        'id': 'default',
                        'name': 'Par d√©faut',
                        'type': 'solid',
                        'available': True
                    }
                },
                'default_background': 'default'
            }
        }
        
        return defaults.get(config_name, {})
    
    def reload_all(self) -> bool:
        """Recharge toutes les configurations"""
        success = True
        for config_name in ['voices', 'models', 'themes', 'backgrounds']:
            try:
                self.load_config(config_name, force_reload=True)
            except Exception as e:
                log.error(f"Erreur rechargement {config_name}: {e}")
                success = False
        
        return success
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut du gestionnaire de configuration"""
        return {
            'loaded_files': list(self._loaded_files),
            'cache_size': len(self._cache),
            'config_dir': str(self.config_dir),
            'files_exist': {
                config: (self.config_dir / f"{config}.json").exists()
                for config in ['voices', 'models', 'themes', 'backgrounds']
            }
        }

# Instance globale
# config_loader = ConfigLoader()

# Test standalone
if __name__ == "__main__":
    print("üß™ Test ConfigLoader")
    
    try:
        # Test chargement
        voices = config_loader.get_voices()
        models = config_loader.get_models()
        themes = config_loader.get_themes()
        backgrounds = config_loader.get_backgrounds()
        
        print(f"‚úÖ Voix charg√©es: {len(voices.get('voices', {}))}")
        print(f"‚úÖ Mod√®les charg√©s: {len(models.get('llm_models', {}))}")
        print(f"‚úÖ Th√®mes charg√©s: {len(themes.get('themes', {}))}")
        print(f"‚úÖ Arri√®re-plans charg√©s: {len(backgrounds.get('backgrounds', {}))}")
        
        # Test listes pour interface
        voice_list = config_loader.get_voice_list()
        model_list = config_loader.get_model_list()
        
        print(f"‚úÖ Liste voix interface: {len(voice_list)} √©l√©ments")
        print(f"‚úÖ Liste mod√®les interface: {len(model_list)} √©l√©ments")
        
        # Test valeurs par d√©faut
        print(f"‚úÖ Voix par d√©faut: {config_loader.get_default_voice()}")
        print(f"‚úÖ Mod√®le par d√©faut: {config_loader.get_default_model()}")
        print(f"‚úÖ Th√®me par d√©faut: {config_loader.get_default_theme()}")
        
        print("\n‚úÖ Test ConfigLoader termin√© avec succ√®s")
        
    except Exception as e:
        print(f"‚ùå Erreur test: {e}")
        import traceback
        traceback.print_exc()

==================================================
FICHIER: .\thalamus\interface_bridge.py
==================================================

"""
interface_bridge.py - Pont entre interface web et modules Jarvis (Thalamus) - VERSION AVEC CONFIG
Responsabilit√É¬© : Interface simplifi√É¬©e pour l'acc√É¬®s aux modules + gestion configurations JSON
Utilise maintenant le ConfigLoader pour charger les param√®tres depuis les fichiers JSON
"""

from pathlib import Path
import sys

# Ajouter le chemin vers les modules Jarvis
sys.path.append(str(Path(__file__).parent.parent))

# Imports des modules existants (r√É¬©utilisation)
from cortex_prefrontal.llm_client import JarvisLLM
from lobes_temporaux.stt import SpeechToText
from lobes_temporaux.tts import TextToSpeech
from hypothalamus.device_manager import DeviceManager
from hypothalamus.voice_manager import VoiceManager
from hypothalamus.logger import log

# Import du nouveau gestionnaire de configuration
from thalamus.config_loader import ConfigLoader

class InterfaceBridge:
    """Pont simplifi√É¬© entre interface web et modules Jarvis (Thalamus) avec gestion config JSON"""
    
    def __init__(self):
        # Initialiser le gestionnaire de configuration
        self.config_loader = ConfigLoader()
        self.voices_json_path = Path(__file__).parent.parent / "config" / "voices.json" 
        log.info("Interface Bridge initialis√É¬© avec ConfigLoader (Thalamus)")
    
    @staticmethod
    def get_system_info():
        """Retourne les informations syst√É¬®me pour debug"""
        try:
            import psutil
            import platform
            
            return {
                'platform': platform.system(),
                'python_version': platform.python_version(),
                'cpu_count': psutil.cpu_count(),
                'memory_total': psutil.virtual_memory().total // (1024**3),  # GB
                'memory_available': psutil.virtual_memory().available // (1024**3)  # GB
            }
        except ImportError:
            return {
                'platform': 'Unknown',
                'python_version': 'Unknown',
                'note': 'psutil non install√É¬© - infos limit√É¬©es'
            }

    @staticmethod
    def validate_ollama_connection():
        """V√É¬©rifie la connexion √É  Ollama"""
        try:
            import ollama
            
            # Test simple de connexion
            models = ollama.list()
            return {
                'success': True,
                'models_count': len(models.get('models', [])),
                'models': [m['name'] for m in models.get('models', [])]
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'suggestion': 'V√É¬©rifiez qu\'Ollama est d√É¬©marr√É¬©'
            }

    @staticmethod
    def get_available_microphones():
        """Retourne la liste des microphones disponibles (via DeviceManager)"""
        try:
            device_manager = DeviceManager()
            
            import pyaudio
            p = pyaudio.PyAudio()
            devices = []
            
            for i in range(p.get_device_count()):
                try:
                    info = p.get_device_info_by_index(i)
                    if info['maxInputChannels'] > 0:
                        devices.append({
                            'index': i,
                            'name': info['name'],
                            'channels': info['maxInputChannels'],
                            'sample_rate': int(info['defaultSampleRate'])
                        })
                except:
                    continue
            
            p.terminate()
            return devices
            
        except Exception as e:
            log.error(f"Erreur √É¬©num√É¬©ration microphones: {e}")
            return []

    def get_available_voices(self):
        """Retourne la liste des voix disponibles (standard + clon√©es)"""
        try:
            import json
            from pathlib import Path
            
            voices_json_path = Path(__file__).parent.parent / "config" / "voices.json"
            
            if not voices_json_path.exists():
                log.warning(f"voices.json introuvable : {voices_json_path}")
                return {
                    'success': False,
                    'error': 'voices.json introuvable',
                    'voices': {}
                }
            
            # Charger voices.json
            with open(voices_json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # Fusionner voix standard et clon√©es
            all_voices = {}
            
            # Voix standard
            for voice_id, voice_data in data.get('voices', {}).items():
                all_voices[voice_id] = {
                    'id': voice_id,
                    'name': voice_data.get('name'),
                    'display_name': voice_data.get('display_name', voice_data.get('name')),
                    'model': voice_data.get('model'),
                    'edge_voice': voice_data.get('edge_voice'),
                    'description': voice_data.get('description', ''),
                    'type': 'standard'
                }
            
            # Voix clon√©es (SI status = ready)
            for voice_id, voice_data in data.get('cloned_voices', {}).items():
                if voice_data.get('processing_status') == 'ready':
                    all_voices[voice_id] = {
                        'id': voice_id,
                        'name': voice_data.get('name'),
                        'display_name': voice_data.get('display_name', voice_data.get('name')),
                        'model': 'xtts-v2',
                        'sample_path': voice_data.get('sample_path'),
                        'description': voice_data.get('description', ''),
                        'type': 'cloned'
                    }
            
            log.info(f"Voix charg√©es : {len(all_voices)} (standard + clon√©es)")
            
            return {
                'success': True,
                'voices': all_voices,
                'default_voice': data.get('default_voice', 'jarvis'),
                'demo_text': data.get('demo_text', 'Bonjour')
            }
            
        except Exception as e:
            log.error(f"Erreur chargement voix : {e}")
            import traceback
            log.error(traceback.format_exc())
            return {
                'success': False,
                'error': str(e),
                'voices': {}
            }

    def get_available_models(self):
        """Retourne la liste des mod√®les LLM disponibles depuis la configuration JSON"""
        try:
            return {
                'success': True,
                'models': self.config_loader.get_model_list(),
                'default_model': self.config_loader.get_default_model()
            }
        except Exception as e:
            log.error(f"Erreur chargement mod√®les: {e}")
            return {
                'success': False,
                'error': str(e),
                'models': []
            }

    def get_available_themes(self):
        """Retourne la liste des th√®mes disponibles depuis la configuration JSON"""
        try:
            return {
                'success': True,
                'themes': self.config_loader.get_theme_list(),
                'default_theme': self.config_loader.get_default_theme()
            }
        except Exception as e:
            log.error(f"Erreur chargement th√®mes: {e}")
            return {
                'success': False,
                'error': str(e),
                'themes': []
            }

    def get_available_backgrounds(self):
        """Version simplifi√©e - sans la fonction inutile"""
        try:
            images_path = Path("web_interface/images")
            
            if not images_path.exists():
                return {
                    'success': False,
                    'error': 'Dossier images inexistant',
                    'backgrounds': []
                }
            
            supported_formats = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']
            backgrounds = []
            
            # Option par d√©faut
            backgrounds.append({
                'name': 'Par d√©faut',
                'path': None,
                'filename': None
            })
            
            # Scanner les fichiers
            for file_path in images_path.iterdir():
                if file_path.is_file() and file_path.suffix.lower() in supported_formats:
                    display_name = file_path.stem.replace('_', ' ').replace('-', ' ').title()
                    
                    backgrounds.append({
                        'name': display_name,
                        'path': f"images/{file_path.name}",
                        'filename': file_path.name
                    })
            
            backgrounds[1:] = sorted(backgrounds[1:], key=lambda x: x['name'])
            
            return {
                'success': True,
                'backgrounds': backgrounds
            }
            
        except Exception as e:
            log.error(f"Erreur scan backgrounds: {e}")
            return {'success': False, 'error': str(e), 'backgrounds': []}

    def create_jarvis_instance(self, personality: str = None):
        """Cr√É¬©e une instance Jarvis compl√É¬®te en utilisant les configurations JSON"""
        try:
            # Utiliser la voix par d√©faut si aucune sp√©cifi√©e
            if personality is None:
                personality = self.config_loader.get_default_voice()
            
            # R√©cup√©rer la configuration de la voix
            voice_config = self.config_loader.get_voice_config(personality)
            if not voice_config:
                log.warning(f"Voix '{personality}' non trouv√©e, utilisation par d√©faut")
                personality = self.config_loader.get_default_voice()
                voice_config = self.config_loader.get_voice_config(personality)
            
            # R√©cup√©rer le mod√®le par d√©faut
            default_model = self.config_loader.get_default_model()
            
            # Cr√É¬©er les instances en r√É¬©utilisant les modules existants
            llm = JarvisLLM(personality=personality)
            
            # Pour le TTS, utiliser la configuration de la voix depuis JSON
            tts = TextToSpeech(
                model_name=voice_config['model'],
                personality=personality,
                edge_voice=voice_config.get('edge_voice')
            )
            
            display_name = voice_config['display_name']
            log.success(f"Instance Jarvis cr√É¬©√É¬©e: {display_name}")
            
            return {
                'llm': llm,
                'tts': tts,
                'config': voice_config,
                'personality': personality,
                'display_name': display_name,
                'model': default_model
            }
            
        except Exception as e:
            log.error(f"Erreur cr√É¬©ation instance Jarvis: {e}")
            raise

    @staticmethod
    def test_audio_pipeline(device_index: int = None):
        """Teste la pipeline audio compl√É¬®te (r√É¬©utilise modules existants)"""
        try:
            # Test STT avec module existant
            stt = SpeechToText(device_index=device_index)
            
            # Test TTS avec voix par d√É¬©faut
            tts = TextToSpeech(
                model_name="edge-tts",
                personality="Samantha", 
                edge_voice="fr-FR-DeniseNeural"
            )
            
            return {
                'stt_ready': True,
                'tts_ready': True,
                'device_index': device_index
            }
            
        except Exception as e:
            return {
                'stt_ready': False,
                'tts_ready': False,
                'error': str(e)
            }

    def format_display_name(self, personality: str) -> str:
        """Formate le nom d'affichage selon la configuration JSON"""
        voice_config = self.config_loader.get_voice_config(personality)
        if voice_config:
            return voice_config.get('display_name', f"Assistant virtuel - {personality}")
        else:
            return f"Assistant virtuel - {personality}"

    def get_personality_config(self, personality: str) -> dict:
        """Retourne la configuration d'une personnalit√É¬© depuis JSON"""
        voice_config = self.config_loader.get_voice_config(personality)
        if voice_config:
            return voice_config
        else:
            # Fallback sur config par d√©faut
            default_personality = self.config_loader.get_default_voice()
            return self.config_loader.get_voice_config(default_personality) or {}

    def get_config_status(self) -> dict:
        """Retourne le statut des configurations charg√©es"""
        return {
            'success': True,
            'config_loader_status': self.config_loader.get_status(),
            'available_configs': {
                'voices': len(self.config_loader.get_voice_list()),
                'models': len(self.config_loader.get_model_list()),
                'themes': len(self.config_loader.get_theme_list()),
                'backgrounds': len(self.config_loader.get_background_list())
            }
        }

    def reload_configurations(self) -> dict:
        """Recharge toutes les configurations depuis les fichiers JSON"""
        try:
            success = self.config_loader.reload_all()
            return {
                'success': success,
                'message': 'Configurations recharg√©es' if success else 'Erreurs lors du rechargement',
                'status': self.config_loader.get_status()
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def log_system_startup(self):
        """Log les informations de d√É¬©marrage syst√É¬®me avec config JSON"""
        system_info = self.get_system_info()
        ollama_status = self.validate_ollama_connection()
        config_status = self.get_config_status()
        
        log.info("=== D√É‚Ä∞MARRAGE JARVIS THALAMUS (CONFIG JSON) ===")
        log.info(f"Syst√É¬®me: {system_info.get('platform', 'Unknown')}")
        log.info(f"Python: {system_info.get('python_version', 'Unknown')}")
        log.info(f"RAM: {system_info.get('memory_available', '?')}GB disponible")
        
        if ollama_status['success']:
            log.success(f"Ollama connect√É¬© ({ollama_status['models_count']} mod√É¬®les)")
        else:
            log.warning(f"Ollama: {ollama_status['error']}")
        
        microphones = self.get_available_microphones()
        log.info(f"Microphones d√É¬©tect√É¬©s: {len(microphones)}")
        
        # Informations sur les configurations charg√©es
        available = config_status['available_configs']
        log.info(f"Configurations charg√©es:")
        log.info(f"  - Voix: {available['voices']}")
        log.info(f"  - Mod√®les: {available['models']}")
        log.info(f"  - Th√®mes: {available['themes']}")
        log.info(f"  - Arri√®re-plans: {available['backgrounds']}")
        
        log.info("=== THALAMUS INITIALIS√É‚Ä∞ (CONFIG JSON) ===")

# Test standalone
if __name__ == "__main__":
    print("üß™ Test Interface Bridge avec ConfigLoader (Thalamus)")
    
    bridge = InterfaceBridge()
    bridge.log_system_startup()
    
    try:
        # Test des configurations
        voices = bridge.get_available_voices()
        models = bridge.get_available_models()
        themes = bridge.get_available_themes()
        backgrounds = bridge.get_available_backgrounds()
        
        if voices['success']:
            print(f"‚úÖ Voix charg√©es: {len(voices['voices'])}")
            for voice in voices['voices'][:3]:  # Afficher les 3 premi√®res
                print(f"   - {voice['display_name']}: {voice['description']}")
        
        if models['success']:
            print(f"‚úÖ Mod√®les charg√©s: {len(models['models'])}")
            for model in models['models']:
                status = "‚úÖ" if model['available'] else "‚è≥"
                print(f"   {status} {model['display_name']}: {model['description']}")
        
        if themes['success']:
            print(f"‚úÖ Th√®mes charg√©s: {len(themes['themes'])}")
        
        if backgrounds['success']:
            print(f"‚úÖ Arri√®re-plans charg√©s: {len(backgrounds['backgrounds'])}")
        
        # Test cr√©ation instance
        instance = bridge.create_jarvis_instance("samantha")
        print(f"‚úÖ Instance cr√É¬©√É¬©e: {instance['display_name']}")
        
        # Test statut config
        config_status = bridge.get_config_status()
        print(f"‚úÖ Statut configurations: {config_status['config_loader_status']}")
        
        print("\n‚úÖ Test Interface Bridge termin√© avec succ√®s")
        
    except Exception as e:
        print(f"‚ùå Erreur test bridge: {e}")
        import traceback
        traceback.print_exc()

==================================================
FICHIER: .\thalamus\message_queue.py
==================================================

"""
message_queue.py - Syst√®me de file d'attente pour les messages
√âvite les traitements simultan√©s et les conflits
Nouveau module pour le dossier thalamus
"""

import asyncio
from typing import Dict, Any, Optional, Callable
from collections import deque
import time
import hashlib
from pathlib import Path
import sys

# Import logger
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log


class MessageQueue:
    """
    File d'attente intelligente pour √©viter les traitements simultan√©s
    et les doublons de messages
    """
    
    def __init__(self, max_size: int = 100, dedup_window: float = 2.0):
        """
        Initialise la queue
        
        Args:
            max_size: Taille maximale de la queue
            dedup_window: Fen√™tre de temps (secondes) pour la d√©duplication
        """
        self.queue = deque(maxlen=max_size)
        self.processing = False
        self.current_task: Optional[asyncio.Task] = None
        self.processing_lock = asyncio.Lock()
        
        # Syst√®me anti-duplication
        self.recent_hashes = {}  # hash -> timestamp
        self.dedup_window = dedup_window
        
        # Statistiques
        self.stats = {
            'processed': 0,
            'dropped': 0,
            'duplicates': 0,
            'errors': 0,
            'total_time': 0.0,
            'avg_processing_time': 0.0
        }
        
        log.info("MessageQueue initialis√©e (Thalamus)")
    
    def _compute_hash(self, message: Dict[str, Any]) -> str:
        """Calcule un hash unique pour un message"""
        # Cr√©er une repr√©sentation stable du message
        content = message.get('content', '')
        msg_type = message.get('type', '')
        
        # Ignorer le timestamp pour le hash
        hash_str = f"{msg_type}:{content}"
        return hashlib.md5(hash_str.encode()).hexdigest()
    
    def _is_duplicate(self, message: Dict[str, Any]) -> bool:
        """V√©rifie si un message est un doublon"""
        msg_hash = self._compute_hash(message)
        current_time = time.time()
        
        # Nettoyer les vieux hashes
        self.recent_hashes = {
            h: t for h, t in self.recent_hashes.items()
            if current_time - t < self.dedup_window * 2
        }
        
        # V√©rifier si c'est un doublon
        if msg_hash in self.recent_hashes:
            time_diff = current_time - self.recent_hashes[msg_hash]
            if time_diff < self.dedup_window:
                self.stats['duplicates'] += 1
                log.debug(f"Message dupliqu√© ignor√© (Œît={time_diff:.2f}s)")
                return True
        
        # Enregistrer le hash
        self.recent_hashes[msg_hash] = current_time
        return False
    
    async def add_message(self, message: Dict[str, Any]) -> bool:
        """
        Ajoute un message √† la queue
        
        Args:
            message: Message √† ajouter
            
        Returns:
            bool: True si ajout√©, False sinon
        """
        # V√©rifier les doublons
        if self._is_duplicate(message):
            return False
        
        # V√©rifier la capacit√©
        if len(self.queue) >= self.queue.maxlen:
            self.stats['dropped'] += 1
            log.warning(f"Queue pleine ({self.queue.maxlen}), message ignor√©: {message.get('type')}")
            return False
        
        # Ajouter timestamp si absent
        if 'timestamp' not in message:
            message['timestamp'] = time.time()
        
        # Ajouter le message
        self.queue.append(message)
        
        log.debug(f"Message ajout√©: {message.get('type')} (position {len(self.queue)})")
        return True
    
    async def process_next(self, handler_func: Callable) -> bool:
        """
        Traite le prochain message de la queue
        
        Args:
            handler_func: Fonction pour traiter le message
            
        Returns:
            bool: True si un message a √©t√© trait√©
        """
        if not self.queue:
            return False
        
        async with self.processing_lock:
            if not self.queue:  # Double v√©rification apr√®s le lock
                return False
            
            message = self.queue.popleft()
            
            # V√©rifier l'expiration (messages > 30s)
            age = time.time() - message.get('timestamp', 0)
            if age > 30:
                log.debug(f"Message expir√© ignor√© ({age:.1f}s): {message.get('type')}")
                return False
            
            # Traiter le message
            start_time = time.time()
            
            try:
                await handler_func(message)
                
                # Mettre √† jour les stats
                processing_time = time.time() - start_time
                self.stats['processed'] += 1
                self.stats['total_time'] += processing_time
                
                if self.stats['processed'] > 0:
                    self.stats['avg_processing_time'] = (
                        self.stats['total_time'] / self.stats['processed']
                    )
                
                log.debug(f"Message trait√© en {processing_time:.2f}s: {message.get('type')}")
                return True
                
            except Exception as e:
                self.stats['errors'] += 1
                log.error(f"Erreur traitement message: {e}")
                return False
    
    async def process_queue(
        self, 
        handler_func: Callable,
        max_batch: int = 10,
        delay_between: float = 0.1
    ):
        """
        Traite la queue de messages par batch
        
        Args:
            handler_func: Fonction pour traiter chaque message
            max_batch: Nombre maximum de messages √† traiter
            delay_between: D√©lai entre chaque message (secondes)
        """
        if self.processing:
            log.warning("Traitement d√©j√† en cours")
            return
        
        self.processing = True
        processed_count = 0
        
        try:
            while self.queue and processed_count < max_batch:
                success = await self.process_next(handler_func)
                
                if success:
                    processed_count += 1
                    
                    # Pause entre messages pour √©viter la surcharge
                    if delay_between > 0 and self.queue:
                        await asyncio.sleep(delay_between)
            
            if processed_count > 0:
                log.info(f"Batch trait√©: {processed_count} messages")
                
        finally:
            self.processing = False
    
    def clear(self, keep_stats: bool = False):
        """
        Vide la queue
        
        Args:
            keep_stats: Si True, conserve les statistiques
        """
        count = len(self.queue)
        self.queue.clear()
        self.recent_hashes.clear()
        
        if not keep_stats:
            self.stats = {
                'processed': 0,
                'dropped': 0,
                'duplicates': 0,
                'errors': 0,
                'total_time': 0.0,
                'avg_processing_time': 0.0
            }
        
        log.info(f"Queue vid√©e ({count} messages supprim√©s)")
    
    def get_status(self) -> Dict[str, Any]:
        """Retourne le statut d√©taill√© de la queue"""
        return {
            'size': len(self.queue),
            'capacity': self.queue.maxlen,
            'utilization': len(self.queue) / self.queue.maxlen * 100,
            'processing': self.processing,
            'stats': self.stats.copy(),
            'dedup_cache_size': len(self.recent_hashes)
        }
    
    def peek(self, n: int = 5) -> list:
        """
        Aper√ßu des prochains messages sans les retirer
        
        Args:
            n: Nombre de messages √† voir
            
        Returns:
            Liste des n prochains messages
        """
        preview = []
        for i, msg in enumerate(self.queue):
            if i >= n:
                break
            preview.append({
                'position': i + 1,
                'type': msg.get('type'),
                'age': time.time() - msg.get('timestamp', 0),
                'preview': str(msg.get('content', ''))[:50]
            })
        return preview
    
    async def wait_for_empty(self, timeout: float = 30.0) -> bool:
        """
        Attend que la queue soit vide
        
        Args:
            timeout: Temps maximum d'attente
            
        Returns:
            bool: True si la queue est vide, False si timeout
        """
        start_time = time.time()
        
        while self.queue or self.processing:
            if time.time() - start_time > timeout:
                return False
            await asyncio.sleep(0.1)
        
        return True


# Test standalone
if __name__ == "__main__":
    import asyncio
    
    async def test_handler(message: Dict[str, Any]):
        """Handler de test"""
        print(f"  Traitement: {message.get('type')} - {message.get('content')}")
        await asyncio.sleep(0.5)  # Simuler traitement
    
    async def test_queue():
        print("üß™ Test MessageQueue")
        
        queue = MessageQueue(max_size=10)
        
        # Test 1: Ajout de messages
        print("\n1Ô∏è‚É£ Ajout de messages...")
        messages = [
            {'type': 'text_message', 'content': 'Bonjour'},
            {'type': 'text_message', 'content': 'Bonjour'},  # Doublon
            {'type': 'voice_input', 'content': 'Test vocal'},
            {'type': 'text_message', 'content': 'Comment vas-tu?'},
        ]
        
        for msg in messages:
            added = await queue.add_message(msg)
            print(f"  {'‚úÖ' if added else '‚ùå'} {msg}")
            await asyncio.sleep(0.1)
        
        # Test 2: Aper√ßu
        print("\n2Ô∏è‚É£ Aper√ßu de la queue:")
        preview = queue.peek(3)
        for item in preview:
            print(f"  {item}")
        
        # Test 3: Traitement
        print("\n3Ô∏è‚É£ Traitement par batch...")
        await queue.process_queue(test_handler, max_batch=5)
        
        # Test 4: Statistiques
        print("\n4Ô∏è‚É£ Statistiques:")
        status = queue.get_status()
        print(f"  Taille: {status['size']}/{status['capacity']}")
        print(f"  Trait√©s: {status['stats']['processed']}")
        print(f"  Doublons: {status['stats']['duplicates']}")
        print(f"  Temps moyen: {status['stats']['avg_processing_time']:.2f}s")
        
        print("\n‚úÖ Test termin√©")
    
    asyncio.run(test_queue())


==================================================
FICHIER: .\thalamus\message_router.py
==================================================

"""
message_router.py - Routage intelligent des messages (Thalamus)
Responsabilit√© : Router les messages vers les bons modules selon leur type
"""

from pathlib import Path
import sys

# Import logger depuis hypothalamus
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

class MessageRouter:
    """Routeur intelligent de messages (Gare de triage thalamique)"""
    
    def __init__(self):
        # Mapping des types de messages vers les modules cibles
        self.route_map = {
            # Messages de conversation ‚Üí lobes_temporaux
            'text_message': 'conversation_flow',
            'voice_input': 'conversation_flow',
            'transcription': 'conversation_flow',
            
            # Messages de configuration ‚Üí hypothalamus
            'config_update': 'config_coordinator',
            'voice_change': 'config_coordinator',
            'device_change': 'config_coordinator',
            
            # Messages syst√®me ‚Üí hypothalamus
            'system_status': 'system_monitor',
            'health_check': 'system_monitor',
            
            # Messages de contr√¥le ‚Üí thalamus
            'ping': 'local',
            'connection_test': 'local'
        }
        
        log.info("MessageRouter initialis√© (Thalamus)")
    
    def get_target_module(self, message_type: str) -> str:
        """D√©termine le module cible pour un type de message"""
        target = self.route_map.get(message_type, 'unknown')
        
        if target == 'unknown':
            log.warning(f"Type de message non rout√©: {message_type}")
        else:
            log.debug(f"Route: {message_type} ‚Üí {target}")
        
        return target
    
    def is_local_message(self, message_type: str) -> bool:
        """V√©rifie si le message doit √™tre trait√© localement par le thalamus"""
        return self.route_map.get(message_type) == 'local'
    
    def get_module_priority(self, message_type: str) -> int:
        """Retourne la priorit√© de traitement (1=urgent, 3=normal, 5=diff√©r√©)"""
        priority_map = {
            # Messages urgents
            'voice_input': 1,
            'transcription': 1,
            'connection_test': 1,
            
            # Messages normaux
            'text_message': 3,
            'config_update': 3,
            'ping': 3,
            
            # Messages diff√©r√©s
            'health_check': 5,
            'system_status': 5
        }
        
        return priority_map.get(message_type, 3)  # Normal par d√©faut
    
    def validate_message(self, message: dict) -> tuple[bool, str]:
        """Valide la structure d'un message"""
        if not isinstance(message, dict):
            return False, "Message doit √™tre un dictionnaire"
        
        if 'type' not in message:
            return False, "Champ 'type' manquant"
        
        message_type = message['type']
        
        # Validation sp√©cifique par type
        if message_type in ['text_message', 'transcription']:
            if 'content' not in message or not message['content'].strip():
                return False, "Champ 'content' manquant ou vide"
        
        elif message_type == 'config_update':
            if 'config' not in message:
                return False, "Champ 'config' manquant"
        
        return True, "Message valide"
    
    def get_routing_stats(self) -> dict:
        """Retourne les statistiques de routage"""
        module_counts = {}
        for target in self.route_map.values():
            module_counts[target] = module_counts.get(target, 0) + 1
        
        return {
            'total_routes': len(self.route_map),
            'modules_count': len(set(self.route_map.values())),
            'distribution': module_counts
        }

# Test standalone
if __name__ == "__main__":
    router = MessageRouter()
    
    # Test routing
    test_messages = [
        {'type': 'text_message', 'content': 'Hello'},
        {'type': 'config_update', 'config': {'voice': 'Jarvis'}},
        {'type': 'ping'},
        {'type': 'unknown_type'}
    ]
    
    for msg in test_messages:
        valid, reason = router.validate_message(msg)
        if valid:
            target = router.get_target_module(msg['type'])
            priority = router.get_module_priority(msg['type'])
            print(f"‚úÖ {msg['type']} ‚Üí {target} (priorit√©: {priority})")
        else:
            print(f"‚ùå {msg}: {reason}")
    
    print(f"\nüìä Stats: {router.get_routing_stats()}")

==================================================
FICHIER: .\thalamus\websocket_relay.py
==================================================

"""
websocket_relay.py - Relais WebSocket central (Thalamus)
Responsabilit√© : Communication temps r√©el client/serveur
Migr√© depuis web_modules/websocket_handler.py
"""

import json
import asyncio
from typing import List
from fastapi import WebSocket
from pathlib import Path
import sys

# Import logger depuis hypothalamus
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

class WebSocketRelay:
    """Relais centralis√© des connexions WebSocket (Thalamus)"""
    
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.is_initialized = False
        self.connection_lock = asyncio.Lock()
    
    async def handle_connection(self, websocket: WebSocket, conversation_flow, config_coordinator):
        """G√®re une nouvelle connexion WebSocket"""
        await websocket.accept()
        self.active_connections.append(websocket)
        
        try:
            # Initialisation automatique
            if not self.is_initialized:
                await self.send_to_client(websocket, {
                    'type': 'status',
                    'content': 'Initialisation automatique...'
                })
                
                success = await conversation_flow.auto_initialize()
                
                if success:
                    personality = conversation_flow.get_personality()
                    await self.send_to_client(websocket, {
                        'type': 'status',
                        'content': 'Pr√™t !',
                        'personality': f'Assistant virtuel - {personality}'
                    })
                    self.is_initialized = True
                else:
                    await self.send_to_client(websocket, {
                        'type': 'error',
                        'content': '√âchec de l\'initialisation automatique'
                    })
                    return
            else:
                # D√©j√† initialis√©, envoyer le statut
                personality = conversation_flow.get_personality()
                await self.send_to_client(websocket, {
                    'type': 'status',
                    'content': 'Connexion √©tablie',
                    'personality': f'Assistant virtuel - {personality}'
                })
            
            # Connecter les √©v√©nements de conversation
            conversation_flow.set_websocket_callback(self.broadcast_to_all)
            
            # Boucle de r√©ception des messages
            await self._message_loop(websocket, conversation_flow, config_coordinator)
            
        except Exception as e:
            log.error(f"Erreur WebSocket: {e}")
        finally:
            if websocket in self.active_connections:
                self.active_connections.remove(websocket)
    
    async def _message_loop(self, websocket: WebSocket, conversation_flow, config_coordinator):
        """Boucle de traitement des messages"""
        try:
            while websocket in self.active_connections:
                try:
                    # V√©rifier l'√©tat de la connexion
                    if websocket.client_state.value != 1:  # CONNECTED
                        break
                        
                    data = await websocket.receive_text()
                    message = json.loads(data)
                    
                    # Traitement des messages...
                    await self._route_message(message, conversation_flow, config_coordinator)
                    
                except Exception as e:
                    log.error(f"Erreur message loop: {e}")
                    break
                    
        except Exception as e:
            log.error(f"Erreur critique message loop: {e}")
        finally:
            # Nettoyage garanti
            if websocket in self.active_connections:
                self.active_connections.remove(websocket)
            log.debug("Connexion WebSocket nettoy√©e")
    
    async def _route_message(self, message: dict, conversation_flow, config_coordinator):
        """Route les messages vers les bons modules (Thalamus routing)"""
        
        message_type = message.get('type')
        
        if message_type == 'voice_input':
            # Entr√©e vocale ‚Üí ConversationFlow (lobes_temporaux)
            await conversation_flow.process_voice_input()    
        elif message_type == 'text_message':  # ‚úÖ AJOUTER CE BLOC
            # Message texte ‚Üí ConversationFlow
            text = message.get('text', '')
            print(f"üîç [DEBUG] Message re√ßu: '{text}' (longueur: {len(text)})")
            if text:
                await conversation_flow.process_text_message(text)
            else:
                log.warning("Message texte vide re√ßu")
                    
        elif message_type == 'config_update':
            # Mise √† jour config ‚Üí ConfigCoordinator (hypothalamus)
            try:
                print(f"üîç [DEBUG] AVANT config_coordinator.update_config")
                result = await config_coordinator.update_config(message['config'])
                print(f"üîç [DEBUG] APR√àS config_coordinator.update_config") 
            except Exception as e:
                print(f"‚ùå [DEBUG] EXCEPTION dans update_config: {e}")
                import traceback
                traceback.print_exc()
                result = {'success': False, 'message': f'Erreur: {e}'}

            await self.broadcast_to_all({
                'type': 'config_updated',
                'success': result['success'],
                'message': result.get('message', '')
            })
            
            print(f"üîç R√âPONSE envoy√©e au client") 
            
        elif message_type == 'ping':
            # Keep-alive
            await self.broadcast_to_all({'type': 'pong'})
        
        else:
            log.warning(f"Type de message inconnu: {message_type}")
    
    async def send_to_client(self, websocket: WebSocket, message: dict):
        """Envoie un message √† un client sp√©cifique"""
        try:
            # V√©rifier l'√©tat de la connexion AVANT d'envoyer
            if websocket.client_state.value != 1:  # 1 = CONNECTED
                log.debug("WebSocket ferm√©e, suppression de la liste")
                if websocket in self.active_connections:
                    self.active_connections.remove(websocket)
                return False
                
            await websocket.send_text(json.dumps(message))
            return True
            
        except Exception as e:
            log.error(f"Erreur envoi message: {e}")
            # Nettoyer imm√©diatement les connexions ferm√©es
            if websocket in self.active_connections:
                self.active_connections.remove(websocket)
            return False
    
    async def broadcast_to_all(self, message: dict):
        """Diffuse un message √† tous les clients connect√©s"""
        if not self.active_connections:
            return
        
        # Copier la liste pour √©viter les modifications pendant l'it√©ration
        connections_copy = self.active_connections.copy()
        
        for websocket in connections_copy:
            success = await self.send_to_client(websocket, message)
            if not success:
                # La connexion a √©t√© nettoy√©e dans send_to_client
                pass
    
    def get_connection_count(self) -> int:
        """Retourne le nombre de connexions actives"""
        return len(self.active_connections)
    
    async def shutdown(self):
        """Ferme toutes les connexions proprement"""
        for websocket in self.active_connections[:]:
            try:
                await websocket.close()
            except:
                pass
        self.active_connections.clear()
        log.info("WebSocket Relay ferm√© proprement")

==================================================
FICHIER: .\thalamus\whisper_config_api.py
==================================================

"""
API pour gestion dynamique de la configuration Whisper
Permet l'ajustement en live depuis l'interface web
"""

from flask import Blueprint, request, jsonify
from pathlib import Path
import json
from typing import Dict, Any, Optional

# Import logger
import sys
sys.path.append(str(Path(__file__).parent.parent))
from hypothalamus.logger import log

# Blueprint pour les endpoints Whisper
whisper_bp = Blueprint('whisper_config', __name__)

class WhisperConfigManager:
    """Gestionnaire de configuration Whisper pour l'API"""
    
    def __init__(self, config_path: Optional[str] = None):
        if config_path is None:
            config_path = Path(__file__).parent.parent / "whisper_config.json"
        
        self.config_path = Path(config_path)
        self.stt_instance = None  # R√©f√©rence vers l'instance STT active
    
    def set_stt_instance(self, stt_instance):
        """D√©finit l'instance STT √† notifier lors des changements"""
        self.stt_instance = stt_instance
    
    def get_config(self) -> Dict[str, Any]:
        """R√©cup√®re la configuration actuelle"""
        try:
            if self.config_path.exists():
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                return {"error": "Fichier de configuration non trouv√©"}
        except Exception as e:
            log.error(f"Erreur lecture config Whisper: {e}")
            return {"error": str(e)}
    
    def update_config(self, updates: Dict[str, Any]) -> Dict[str, Any]:
        """Met √† jour la configuration"""
        try:
            # Charger config actuelle
            current_config = self.get_config()
            if "error" in current_config:
                return current_config
            
            # Appliquer les mises √† jour (deep merge)
            updated_config = self._deep_merge(current_config, updates)
            
            # Valider la nouvelle configuration
            validation_result = self._validate_config(updated_config)
            if not validation_result["valid"]:
                return {
                    "success": False,
                    "error": f"Configuration invalide: {validation_result['errors']}"
                }
            
            # Sauvegarder
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(updated_config, f, indent=2, ensure_ascii=False)
            
            # Notifier l'instance STT si disponible
            if self.stt_instance:
                reload_success = self.stt_instance.reload_config()
                if not reload_success:
                    return {
                        "success": False,
                        "error": "Configuration sauvegard√©e mais √©chec du rechargement"
                    }
            
            log.success("Configuration Whisper mise √† jour", "‚öôÔ∏è")
            
            return {
                "success": True,
                "message": "Configuration mise √† jour avec succ√®s",
                "config": updated_config
            }
            
        except Exception as e:
            log.error(f"Erreur mise √† jour config: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _deep_merge(self, base: Dict, updates: Dict) -> Dict:
        """Fusion profonde de deux dictionnaires"""
        result = base.copy()
        
        for key, value in updates.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
        
        return result
    
    def _validate_config(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """Valide la configuration Whisper"""
        errors = []
        
        # V√©rifier les sections obligatoires
        required_sections = ['model', 'transcription', 'vad', 'audio', 'performance', 'debug']
        for section in required_sections:
            if section not in config:
                errors.append(f"Section manquante: {section}")
        
        # Validation sp√©cifique par section
        if 'model' in config:
            model_config = config['model']
            if 'name' not in model_config:
                errors.append("model.name est requis")
            elif model_config['name'] not in ['tiny', 'base', 'small', 'medium', 'large', 'large-v2', 'large-v3']:
                errors.append(f"model.name invalide: {model_config['name']}")
        
        if 'transcription' in config:
            trans_config = config['transcription']
            if 'no_speech_threshold' in trans_config:
                threshold = trans_config['no_speech_threshold']
                if not (0.0 <= threshold <= 1.0):
                    errors.append("no_speech_threshold doit √™tre entre 0.0 et 1.0")
        
        if 'vad' in config:
            vad_config = config['vad']
            if 'aggressiveness' in vad_config:
                agg = vad_config['aggressiveness']
                if not (0 <= agg <= 3):
                    errors.append("vad.aggressiveness doit √™tre entre 0 et 3")
        
        return {
            "valid": len(errors) == 0,
            "errors": errors
        }
    
    def reset_to_defaults(self) -> Dict[str, Any]:
        """Remet la configuration aux valeurs par d√©faut"""
        try:
            # Cr√©er une instance temporaire pour r√©cup√©rer les defaults
            from lobes_temporaux.stt import SpeechToText
            temp_stt = SpeechToText.__new__(SpeechToText)
            default_config = temp_stt._get_default_whisper_config()
            
            # Sauvegarder
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            
            # Recharger si instance disponible
            if self.stt_instance:
                self.stt_instance.reload_config()
            
            log.info("Configuration Whisper remise aux valeurs par d√©faut", "‚öôÔ∏è")
            
            return {
                "success": True,
                "message": "Configuration remise aux valeurs par d√©faut",
                "config": default_config
            }
            
        except Exception as e:
            log.error(f"Erreur reset config: {e}")
            return {
                "success": False,
                "error": str(e)
            }

# Instance globale du gestionnaire
config_manager = WhisperConfigManager()

@whisper_bp.route('/api/whisper/config', methods=['GET'])
def get_whisper_config():
    """R√©cup√®re la configuration Whisper actuelle"""
    config = config_manager.get_config()
    return jsonify({
        "success": "error" not in config,
        "config": config
    })

@whisper_bp.route('/api/whisper/config', methods=['POST'])
def update_whisper_config():
    """Met √† jour la configuration Whisper"""
    try:
        updates = request.get_json()
        if not updates:
            return jsonify({
                "success": False,
                "error": "Aucune donn√©e JSON fournie"
            }), 400
        
        result = config_manager.update_config(updates)
        status_code = 200 if result.get("success", False) else 400
        
        return jsonify(result), status_code
        
    except Exception as e:
        return jsonify({
            "success": False,
            "error": f"Erreur traitement requ√™te: {str(e)}"
        }), 500

@whisper_bp.route('/api/whisper/config/reset', methods=['POST'])
def reset_whisper_config():
    """Remet la configuration aux valeurs par d√©faut"""
    result = config_manager.reset_to_defaults()
    status_code = 200 if result.get("success", False) else 500
    return jsonify(result), status_code

@whisper_bp.route('/api/whisper/config/reload', methods=['POST'])
def reload_whisper_config():
    """Force le rechargement de la configuration"""
    try:
        if config_manager.stt_instance:
            success = config_manager.stt_instance.reload_config()
            return jsonify({
                "success": success,
                "message": "Configuration recharg√©e" if success else "√âchec du rechargement"
            })
        else:
            return jsonify({
                "success": False,
                "error": "Aucune instance STT active"
            }), 400
            
    except Exception as e:
        return jsonify({
            "success": False,
            "error": str(e)
        }), 500

@whisper_bp.route('/api/whisper/presets', methods=['GET'])
def get_whisper_presets():
    """Retourne des presets de configuration Whisper"""
    presets = {
        "qualite_maximale": {
            "transcription": {
                "beam_size": 5,
                "best_of": 5,
                "temperature": [0.0],
                "no_speech_threshold": 0.8,
                "condition_on_previous_text": True
            },
            "model": {
                "name": "large"
            }
        },
        "vitesse_maximale": {
            "transcription": {
                "beam_size": 1,
                "best_of": 1,
                "temperature": [0.0],
                "no_speech_threshold": 0.4,
                "condition_on_previous_text": False
            },
            "model": {
                "name": "tiny"
            }
        },
        "equilibre": {
            "transcription": {
                "beam_size": 3,
                "best_of": 3,
                "temperature": [0.0, 0.2],
                "no_speech_threshold": 0.6,
                "condition_on_previous_text": True
            },
            "model": {
                "name": "small"
            }
        },
        "sensible_au_silence": {
            "transcription": {
                "no_speech_threshold": 0.3,
                "condition_on_previous_text": False
            },
            "vad": {
                "aggressiveness": 1
            }
        },
        "resistant_au_bruit": {
            "transcription": {
                "no_speech_threshold": 0.8,
                "condition_on_previous_text": True
            },
            "vad": {
                "aggressiveness": 3
            }
        }
    }
    
    return jsonify({
        "success": True,
        "presets": presets
    })

@whisper_bp.route('/api/whisper/config/preset/<preset_name>', methods=['POST'])
def apply_whisper_preset(preset_name: str):
    """Applique un preset de configuration"""
    # R√©cup√©rer les presets
    presets_response = get_whisper_presets()
    presets = presets_response.get_json()["presets"]
    
    if preset_name not in presets:
        return jsonify({
            "success": False,
            "error": f"Preset '{preset_name}' non trouv√©"
        }), 404
    
    # Appliquer le preset
    preset_config = presets[preset_name]
    result = config_manager.update_config(preset_config)
    
    if result.get("success", False):
        result["message"] = f"Preset '{preset_name}' appliqu√© avec succ√®s"
    
    status_code = 200 if result.get("success", False) else 400
    return jsonify(result), status_code

# Fonction d'initialisation pour lier l'instance STT
def init_whisper_config_api(app, stt_instance=None):
    """Initialise l'API de configuration Whisper"""
    app.register_blueprint(whisper_bp)
    
    if stt_instance:
        config_manager.set_stt_instance(stt_instance)
        log.success("API configuration Whisper initialis√©e", "üîß")
    
    return config_manager

if __name__ == "__main__":
    # Test des endpoints
    from flask import Flask
    
    app = Flask(__name__)
    init_whisper_config_api(app)
    
    print("üß™ Serveur de test API Whisper")
    print("Endpoints disponibles:")
    print("  GET    /api/whisper/config")
    print("  POST   /api/whisper/config")
    print("  POST   /api/whisper/config/reset")
    print("  POST   /api/whisper/config/reload")
    print("  GET    /api/whisper/presets")
    print("  POST   /api/whisper/config/preset/<name>")
    
    app.run(debug=True, port=5001)

==================================================
FICHIER: .\thalamus\__init__.py
==================================================

"""
thalamus - Hub de communication central (Gare de triage neurologique)
R√¥le : Router les informations entre l'interface web et les modules Jarvis
"""

from .websocket_relay import WebSocketRelay
from .message_router import MessageRouter
from .interface_bridge import InterfaceBridge

__all__ = ['WebSocketRelay', 'MessageRouter', 'InterfaceBridge']

==================================================
FICHIER: .\web_interface\index.html
==================================================

<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Jarvis - Assistant Vocal</title>
    <link rel="stylesheet" href="static/styles/index.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 100 100'><text y='.9em' font-size='90'>ü§ñ</text></svg>">
</head>
<body class="theme-light">
    <!-- Header avec navigation -->
    <header class="header">
        <div class="header-left">
            <div class="logo">
                <span class="logo-icon">ü§ñ</span>
                <span class="logo-text" id="assistant-name">Jarvis - initialisation</span>
                <span class="status-indicator" id="status-indicator">‚óè</span>
            </div>
        </div>
        
        <nav class="header-nav">
            <button class="nav-btn" onclick="toggleVoiceLab()">
                <span class="icon">üé≠</span> Clonage Voix
            </button>
            <button class="nav-btn" onclick="toggleCameraPanel()">
                <span class="icon">üì∑</span> Vision IA
            </button>

            <button class="nav-btn" onclick="openSettings()">
                <span class="icon">‚öôÔ∏è</span> Param√®tres
            </button>
            <button class="nav-btn" onclick="toggleDebug()">
                <span class="icon">üîç</span> Debug
            </button>
            <button class="nav-btn" onclick="toggleTheme()">
                <span class="icon" id="theme-icon">üåô</span>
                <span id="theme-text">Mode Gray - blue</span>
            </button>
            <button class="nav-btn" onclick="showHelp()">
                <span class="icon">‚ùì</span> Aide
            </button>
        </nav>
    </header>

    <!-- Contenu principal -->
    <main class="main-content voice-hidden">
        <!-- Zone dialogue (gauche haut) -->
        <aside class="dialogue-section">
            <section class="dialogue-section">
                <div class="dialogue-header">
                    <h2>üí¨ Conversation</h2>
                    <div class="dialogue-controls">
                        <button class="control-btn" onclick="clearConversation()" title="Effacer la conversation">
                            üóëÔ∏è
                        </button>
                        <button class="control-btn" onclick="exportConversation()" title="Exporter">
                            üíæ
                        </button>
                    </div>
                </div>
                
                <div class="dialogue-container" id="dialogue-container">
                    <div class="message-bubble system">
                        <div class="message-content">
                            <p>üöÄ Connexion √† votre assistant virtuel en cours...</p>
                        </div>
                    </div>
                </div>
            </section>

            <!-- Zone saisie (gauche bas) -->
            <section class="input-section">
                <div class="input-container">
                    <div class="input-row">
                        <textarea 
                            id="message-input" 
                            placeholder="Tapez votre message ici ou utilisez le microphone..."
                            rows="3"
                            onkeydown="handleInputKeydown(event)"
                        ></textarea>
                        
                        <div class="input-buttons">
                            <button 
                                class="mic-button" 
                                id="mic-button"
                                onclick="toggleVoiceInput()"
                                title="Activer/D√©sactiver le microphone"
                            >
                                <span class="mic-icon">üé§</span>
                                <span class="mic-status">Parler</span>
                            </button>
                            
                            <button 
                                class="send-button"
                                onclick="sendMessage()"
                                title="Envoyer le message"
                            >
                                ‚û§
                            </button>
                        </div>
                    </div>
                </div>
            </section>
        </aside>
        <!-- Voice Lab Panel -->
        <aside class="voice-section" id="voice-section">
            <div class="voice-header">
                <h3>üé≠ Laboratoire Vocal</h3>
                <button onclick="toggleVoiceLab()" class="close-voice">‚úñ</button>
            </div>
            
            <div class="setting-group">
                <!-- Formulaire de clonage -->
                <div class="voice-clone-form">
                    <h4>Cr√©er une nouvelle voix</h4>
                    
                    <input type="text" 
                    id="voice-name-input" 
                    placeholder="Nom de la voix"
                    maxlength="50"
                    required>
                    
                    <textarea id="voice-description-input" 
                            placeholder="Description (optionnel)"
                            rows="2"></textarea>
                    
                    <div class="debug-tabs"><span></span>
                        <button id="btn-primary" onclick="startVoiceRecording()" class="btn-primary">
                            üéôÔ∏è Enregistrer (10s max)
                        </button>
                        <button id="btn-secondary" onclick="stopVoiceRecording()" style="display:none" class="btn-danger">
                            ‚èπÔ∏è Arr√™ter
                        </button>
                        <div id="recording-indicator" class="recording-indicator"></div>
                        <div id="recording-timer" style="display:none"></div>
                    </div>
                    
                    <!-- Upload fichier -->
                    <div class="upload-option">
                        <label class="file-upload-label">
                            <input type="file" 
                                id="voice-upload" 
                                accept="audio/*,video/*"
                                style="display:none">
                            üìÅ Charger un fichier audio/vid√©o
                        </label>
                        <small>Formats: MP3, WAV, OGG, MP4, AVI, MKV... (Max 50MB)</small>
                    </div>
                </div>
                
                
            <!-- Aper√ßu audio -->
            <div id="audio-preview" style="display:none"></div>
            
            <!-- Bouton de sauvegarde -->
            <div class="form-actions">
                <button id="save-voice-btn" 
                        onclick="saveClonedVoice()" 
                        class="btn-success"
                        disabled>
                    üíæ Sauvegarder la voix
                </button>
                <button onclick="resetVoiceForm()" class="btn-secondary">
                    üîÑ R√©initialiser
                </button>
            </div>
        
            <!-- Section test -->
            <div class="test-section">
                <h4>üîä Tester une voix</h4>
                <div class="form-group">
                    <textarea id="test-text-input" 
                            placeholder="Texte √† tester (optionnel)"
                            rows="2">Bonjour, ceci est un test de ma voix clon√©e.</textarea>
                </div>
            </div>
            
            
            <!-- Liste des voix clon√©es -->
            <div class="cloned-voices-section">
                <h4>Mes voix clon√©es</h4>
                <div id="cloned-voices-list"></div>
            </div>
            
            <!-- Info XTTS -->
            <div class="xtts-info">
                <details>
                    <summary>‚ÑπÔ∏è √Ä propos du clonage vocal</summary>
                    <div class="info-content">
                        <p><strong>Technologies utilis√©es:</strong></p>
                        <ul>
                            <li>üéØ XTTS-v2 pour le clonage haute fid√©lit√©</li>
                            <li>üé§ Whisper pour la transcription</li>
                            <li>üîä Edge-TTS pour les voix pr√©d√©finies</li>
                        </ul>
                        <p><strong>Conseils pour un bon clonage:</strong></p>
                        <ul>
                            <li>Enregistrez dans un environnement calme</li>
                            <li>Parlez clairement et naturellement</li>
                            <li>Dur√©e optimale: 10-20 secondes</li>
                            <li>√âvitez les bruits de fond</li>
                        </ul>
                    </div>
                </details>
            </div>
            </div>
        </aside>

        <!-- Camera Panel -->
        <aside class="camera-section" id="camera-section">
            <div class="camera-header">
                <h3>üì∑ Vision IA</h3>
                <button class="close-camera" onclick="toggleCameraPanel()" >‚úñ</button>
            </div>
            
            <div class="panel-content">
                <div class="camera-container">
                    <video id="camera-feed" autoplay playsinline></video>
                    <div id="camera-info"></div>
                </div>
                
                <div class="camera-tabs">
                    <button onclick="captureImage()" class="btn-primary">
                        üì∏ Capturer
                    </button>
                    <button onclick="switchCamera()" class="btn-secondary">
                        üîÑ Changer cam√©ra
                    </button>
                    <button onclick="toggleCameraFilters()" class="btn-secondary">
                        üé® Filtres
                    </button>
                </div>
                
                <div id="captured-images" class="captured-images-grid"></div>
                
                <div id="analysis-options"></div>
                
                <div id="camera-dev-status"></div>
            </div>
        </aside>

        <!-- Zone debug/logs (droite) -->
        <aside class="debug-section" id="debug-section">
            <div class="debug-header">
                <h3>üîç Debug & Logs</h3>
                <button class="close-debug" onclick="toggleDebug()">‚úï</button>
            </div>
            
            <div class="debug-tabs">
                <button class="tab-btn active" onclick="switchDebugTab('logs')">Logs</button>
                <button class="tab-btn" onclick="switchDebugTab('stats')">Stats</button>
                <button class="tab-btn" onclick="switchDebugTab('config')">Config</button>
            </div>
            
            <div class="debug-content">
                <!-- Onglet Logs -->
                <div id="debug-logs" class="debug-tab active">
                    <div class="log-container" id="log-container">
                        <div class="log-entry info">
                            <span class="log-time">--:--:--</span>
                            <span class="log-message">Interface web initialis√©e</span>
                        </div>
                    </div>
                </div>
                
                <!-- Onglet Stats -->
                <div id="debug-stats" class="debug-tab">
                    <div class="stats-grid">
                        <div class="stat-card">
                            <div class="stat-value" id="stat-messages">0</div>
                            <div class="stat-label">Messages</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-value" id="stat-tokens">0</div>
                            <div class="stat-label">Tokens</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-value" id="stat-avgtime">0.0s</div>
                            <div class="stat-label">Temps Moyen</div>
                        </div>
                        <div class="stat-card">
                            <div class="stat-value" id="stat-tts-efficiency">100%</div>
                            <div class="stat-label">Efficacit√© TTS</div>
                        </div>
                    </div>
                </div>
                
                <!-- Onglet Config -->
                <div id="debug-config" class="debug-tab">
                    <div class="config-item">
                        <label>Personnalit√©:</label>
                        <span id="config-personality">-</span>
                    </div>
                    <div class="config-item">
                        <label>Mod√®le LLM:</label>
                        <span id="config-llm">-</span>
                    </div>
                    <div class="config-item">
                        <label>Mod√®le TTS:</label>
                        <span id="config-tts">-</span>
                    </div>
                    <div class="config-item">
                        <label>P√©riph√©rique Audio:</label>
                        <span id="config-audio">-</span>
                    </div>
                </div>
            </div>
        </aside>
    </main>

    <!-- Modal Param√®tres -->
    <div class="modal" id="settings-modal">
        <div class="modal-content">
            <div class="modal-header">
                <h2>‚öôÔ∏è Param√®tres</h2>
                <button class="modal-close" onclick="closeSettings()">‚úï</button>
            </div>
            
            <div class="modal-body">
                <div class="settings-tabs">
                    <button class="tab-btn active" onclick="switchSettingsTab('voice')">üîä Voix</button>
                    <button class="tab-btn" onclick="switchSettingsTab('audio')">üé§ Audio</button>
                    <button class="tab-btn" onclick="switchSettingsTab('llm')">üß† LLM</button>
                    <button class="tab-btn" onclick="switchSettingsTab('interface')">üé® Interface</button>
                </div>
                
                <div class="settings-content">
                    <!-- Param√®tres Voix -->
                    <div id="settings-voice" class="settings-tab active">
                        <div class="setting-group">
                            <label for="voice-personality">Personnalit√©:</label>
                            <select id="voice-personality">
                                <option value="Jarvis">Chargement...</option>
                            </select>
                        </div>
                        
                        <div class="setting-group">
                            <label for="voice-speed">Vitesse de parole:</label>
                            <input type="range" id="voice-speed" min="0.5" max="2.0" step="0.1" value="1.0">
                            <span id="voice-speed-value">1.0x</span>
                        </div>
                        
                        <div class="setting-group">
                            <label for="voice-volume">Volume:</label>
                            <input type="range" id="voice-volume" min="0" max="100" value="90">
                            <span id="voice-volume-value">90%</span>
                        </div>
                        
                        <button class="test-btn" onclick="testVoice()">üîä Tester la voix</button>
                    </div>
                    
                    <!-- Param√®tres Audio -->
                    <div id="settings-audio" class="settings-tab">
                        <div class="setting-group">
                            <label for="audio-device">Microphone:</label>
                            <select id="audio-device">
                                <option value="default">P√©riph√©rique par d√©faut</option>
                            </select>
                        </div>
                        
                        <div class="setting-group">
                            <label for="audio-sensitivity">Sensibilit√©:</label>
                            <input type="range" id="audio-sensitivity" min="1" max="10" value="5">
                            <span id="audio-sensitivity-value">5</span>
                        </div>
                        
                        <button class="test-btn" onclick="testMicrophone()">üé§ Tester le micro</button>
                    </div>
                    
                    <!-- Param√®tres LLM -->
                    <div id="settings-llm" class="settings-tab">
                        <div class="setting-group">
                            <label for="llm-model">Mod√®le:</label>
                            <select id="llm-model">
                            </select>
                        </div>
                        <div class="setting-group">
                            <label for="role-select">
                                <i class="icon">üë®‚Äçüè´</i> R√¥le
                            </label>
                            <select id="role-select" class="select-input">
                                <!-- Sera peupl√© dynamiquement -->
                            </select>
                        </div>
                        
                        <div class="setting-group">
                            <label for="llm-temperature">Cr√©ativit√© (Temperature):</label>
                            <input type="range" id="llm-temperature" min="0.1" max="1.0" step="0.1" value="0.7">
                            <span id="llm-temperature-value">0.7</span>
                        </div>
                    </div>
                    
                    <!-- Param√®tres Interface -->
                    <div id="settings-interface" class="settings-tab">
                        <div class="setting-group">
                            <label for="interface-theme">Th√®me:</label>
                            <select id="interface-theme">
                                <option value="light">Light</option>
                                <option value="dark">Gray - blue</option>
                                <option value="jarvis">Dark - green</option>
                            </select>
                        </div>
                        
                        <div class="setting-group">
                            <label for="interface-background">Arri√®re-plan:</label>
                            <select id="interface-background">
                                <option value="default">Par d√©faut</option>
                            </select>
                        </div>

                        <div class="setting-group">
                            <label for="background-opacity">Transparence arri√®re-plan:</label>
                            <input type="range" id="background-opacity" min="5" max="100" step="5" value="30">
                            <span id="background-opacity-value">30%</span>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="modal-footer">
                <button class="btn-secondary" onclick="resetSettings()">R√©initialiser</button>
                <button class="btn-primary" onclick="saveSettingsFromModal()">Sauvegarder</button>
            </div>
        </div>
    </div>

    <!-- Modal Aide -->
    <div class="modal" id="help-modal">
        <div class="modal-content">
            <div class="modal-header">
                <h2>‚ùì Aide & Guide d'utilisation</h2>
                <button class="modal-close" onclick="closeHelp()">‚úï</button>
            </div>
            
            <div class="modal-body">
                <div class="help-section">
                    <h3>üé§ Utilisation du microphone</h3>
                    <p>Cliquez sur le bouton microphone pour commencer l'√©coute vocale. Parlez clairement et attendez la transcription.</p>
                    <p><kbd>Ctrl</kbd> + <kbd>M</kbd> : Activer le microphone</p>
                </div>
                
                <div class="help-section">
                    <h3>üí¨ Zone de conversation</h3>
                    <p>Tapez vos messages dans la zone de saisie ou utilisez Ctrl+Entr√©e pour envoyer rapidement.</p>
                    <p><kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>Space</kbd> : Focus sur la zone de saisie </p>
                    <p><kbd>Ctrl</kbd> + <kbd>Enter</kbd> : Valider le message</p>
                    <p><kbd>Ctrl</kbd> + <kbd>f</kbd> : Changer de th√®me</p>
                </div>
                
                <div class="help-section">
                    <h3>üîç Debug et logs</h3>
                    <p>Utilisez la zone debug (droite) pour surveiller les performances et diagnostiquer les probl√®mes.</p>
                    <p><kbd>Ctrl</kbd> + <kbd>d</kbd> : Ouvrir/Fermer l'onglet Debug</p>
                    <p><kbd>Ctrl</kbd> + <kbd>Shift</kbd>  + <kbd>c</kbd> : Exporter les logs</p>
                </div>
                
                <div class="help-section">
                    <h3>‚öôÔ∏è Param√®tres</h3>
                    <p>Personnalisez la voix, l'audio et l'interface selon vos pr√©f√©rences.</p>
                    <p><kbd>Ctrl</kbd> + <kbd>r</kbd> : Red√©marrer Jarvis</p>
                    <p><kbd>Ctrl</kbd> + <kbd>f</kbd> : Changer de th√®me</p>
                </div>
            </div>
        </div>
    </div>

    <!-- Scripts -->
    <!-- üß† Scripts Modulaires - Architecture Cerveau -->
    <!-- Ordre d'import CRITIQUE pour les d√©pendances -->
    
    <!-- 1. Variables globales et √©tat (Cortex Pr√©frontal) -->
    <script src="static/js/variables-globals.js"></script>
    
    <!-- 2. Fonctions utilitaires (Bo√Æte √† outils) -->
    <script src="static/js/utils.js"></script>
    
    <!-- 3. Chargeur de configuration (Hippocampe) -->
    <script src="static/js/config-loader.js"></script>
    <script src="static/js/Settings-frontend.js"></script>
    
    <!-- 4. Gestionnaire de th√®mes (Lobes Occipitaux) -->
    <script src="static/js/theme-manager.js"></script>
    
    <!-- 5. Debug et logs (Syst√®me de surveillance) -->
    <script src="static/js/debug-logger.js"></script>
    
    <!-- 6. Gestionnaire WebSocket (Thalamus) -->
    <script src="static/js/websocket-manager.js"></script>
    
    <!-- 7. Gestionnaire de messages (Lobes Temporaux) -->
    <script src="static/js/message-handler.js"></script>
    
    <!-- 8. Gestion des voix (Hypothalamus) -->
    <script src="static/js/voice-manager.js"></script>
    <script src="static/js/settings-modal.js"></script>
    <script src="static/js/voice-lab.js"></script>

    <!-- 9. Script specifique sur les outils annexe -->
    <script src="static/js/camera-panel.js"></script>

    <!-- 10. Coordination principale (Tronc C√©r√©bral) -->
    <script src="static/js/app-main.js"></script>
         
</body>
</html>

==================================================
FICHIER: .\web_interface\js\app-main.js
==================================================

/**
 * app-main.js - Initialisation principale et coordination
 * üß† Tronc C√©r√©bral - Fonctions vitales et coordination g√©n√©rale
 */

async function initializeJarvis() {
    addLogEntry('üöÄ D√©marrage de Jarvis unifi√©...', 'info');
    
    try {
        // 1. V√©rifier les pr√©requis navigateur
        if (!checkBrowserCompatibility()) {
            return false;
        }
        
        // 2. üöÄ NOUVEAU: Initialiser la configuration unifi√©e
        const configLoaded = await initializeConfig();
        
        if (!configLoaded) {
            addLogEntry('‚ö†Ô∏è Utilisation de la configuration par d√©faut', 'warning');
        } else {
            addLogEntry('‚úÖ Configuration unifi√©e charg√©e', 'success');
        }
        
        // 3. Initialiser les modules dans l'ordre logique
        await initializeModules();
        
        // 4. √âtablir la connexion WebSocket
        addLogEntry('üîå √âtablissement de la connexion...', 'info');
        initializeWebSocket();
        
        // 5. üöÄ SUPPRIM√â: Plus de loadSettings() - d√©j√† fait par initializeConfig()
        
        // 6. Initialiser l'interface utilisateur
        await updateUI();
        
        // 7. D√©marrer les services de fond
        startBackgroundServices();
        
        addLogEntry('‚úÖ Jarvis unifi√© initialis√© avec succ√®s !', 'success');
        showToast('ü§ñ Jarvis est pr√™t !', 'success');
        
        return true;
        
    } catch (error) {
        handleError(error, 'Initialisation Jarvis');
        addLogEntry('‚ùå √âchec de l\'initialisation', 'error');
        showToast('‚ùå Erreur d\'initialisation', 'error');
        return false;
    }
}

/**
 * üöÄ NOUVEAU: Initialisation de la configuration unifi√©e
 */
async function initializeConfig() {
    try {
        addLogEntry('üéØ Initialisation configuration unifi√©e...', 'info');
        
        // Charger la configuration depuis l'API REST unifi√©e
        const response = await fetch('/api/config');
        const data = await response.json();

        if (data && typeof data === 'object' && !data.error && Object.keys(data).length > 0) {
            // Stocker la config dans une variable globale pour acc√É¬®s rapide
            window.jarvisConfig = data;
            
            // Appliquer imm√É¬©diatement les param√É¬®tres d'interface
            await applyInterfaceConfigFromServer(data.interface || {});
            
            addLogEntry('üìÑ Configuration unifi√©e charg√©e depuis le serveur', 'success');
            return true;
        } else {
            addLogEntry('‚ùå Erreur chargement configuration serveur', 'error');
            return false;
        }
        
    } catch (error) {
        addLogEntry(`‚ùå Erreur init config unifi√©e: ${error.message}`, 'error');
        return false;
    }
}

/**
 * üöÄ NOUVEAU: Application imm√©diate de la config interface
 */
async function applyInterfaceConfigFromServer(interfaceConfig) {
    try {
        // Appliquer le th√®me
        if (interfaceConfig.theme) {
            currentTheme = interfaceConfig.theme;
            document.body.className = `theme-${interfaceConfig.theme}`;
            updateThemeButton();
            addLogEntry(`üé® Th√®me appliqu√©: ${interfaceConfig.theme}`, 'info');
        }
        
        // Appliquer le background
        if (interfaceConfig.background && interfaceConfig.background !== 'default') {
            await applyBackgroundFromConfig(interfaceConfig.background);
        }
        
        // Appliquer l'opacit√© du background
        if (interfaceConfig.background_opacity !== undefined) {
            await applyBackgroundOpacityFromConfig(interfaceConfig.background_opacity);
        }
        
        // Appliquer la visibilit√© des panneaux
        if (interfaceConfig.panels) {
            voiceVisible = interfaceConfig.panels.voice_lab_visible || false;
            cameraVisible = interfaceConfig.panels.camera_visible || false;
            debugVisible = interfaceConfig.panels.debug_visible || false;
            
            updateVoiceVisibility();
            updateCameraVisibility();
            updateDebugVisibility();
        }
        
    } catch (error) {
        addLogEntry(`‚ö†Ô∏è Erreur application config interface: ${error.message}`, 'warning');
    }
}

async function applyBackgroundFromConfig(backgroundPath) {
    const dialogueContainer = document.getElementById('dialogue-container');
    if (!dialogueContainer) return;
    
    const dialogueSection = dialogueContainer.closest('.dialogue-section');
    if (!dialogueSection) return;
    
    let imagePath = backgroundPath.startsWith('images/') ? 
        `static/${backgroundPath}` : `static/images/${backgroundPath}`;
    
    dialogueSection.style.setProperty('--bg-image-url', `url('${imagePath}')`);
    dialogueSection.classList.add('bg-image');
    
    addLogEntry(`üñºÔ∏è Background appliqu√©: ${backgroundPath}`, 'info');
}

async function applyBackgroundOpacityFromConfig(opacity) {
    const style = document.createElement('style');
    style.id = 'background-opacity-config';
    
    const existing = document.getElementById('background-opacity-config');
    if (existing) existing.remove();
    
    style.textContent = `
        .dialogue-section.bg-image::before {
            content: '';
            position: absolute;
            top: 0; left: 0; right: 0; bottom: 0;
            background-image: var(--bg-image-url);
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            opacity: ${opacity / 100};
            z-index: 1;
            pointer-events: none;
            border-radius: inherit;
        }
    `;
    
    document.head.appendChild(style);
    addLogEntry(`üé® Opacit√© background: ${opacity}%`, 'info');
}

/**
 * üöÄ MODIFI√â: Initialisation des modules avec config unifi√©e
 */
async function initializeModules() {
    try {
        addLogEntry('üîß Initialisation des modules...', 'info');
        
        // ‚úÖ Initialisation du gestionnaire de voix
        if (typeof initVoices === 'function') {
            await initVoices();
            addLogEntry('üé§ Module voix initialis√©', 'info');
        }
        
        // ‚úÖ Charger les listes
        await loadVoicesFromAPI();
        await loadRolesFromAPI();
        await loadBackgroundsFromAPI();
        await loadModelsFromAPI();
        
        // ‚úÖ Visibilit√© des panneaux
        updateVoiceVisibility();
        updateCameraVisibility(); 
        updateDebugVisibility();
        
        addLogEntry('‚úÖ Modules unifi√©s initialis√©s', 'success');
        
    } catch (error) {
        addLogEntry(`‚ùå Erreur init modules: ${error.message}`, 'error');
        throw error;
    }
}

/**
 * üöÄ NOUVEAU: Chargement des voix depuis l'API unifi√©e
 */
async function loadVoicesFromAPI() {
    try {
        const response = await fetch('/api/voice/all/list');
        const data = await response.json();
        
        if (data.success) {
            // Peupler le select des voix
            await populateVoiceSelectFromAPI(data.voices, data.cloned_voices);
            addLogEntry(`üé§ ${Object.keys(data.voices || {}).length} voix standard + ${Object.keys(data.cloned_voices || {}).length} clon√©es`, 'info');
        }
    } catch (error) {
        addLogEntry(`‚ùå Erreur chargement voix API: ${error.message}`, 'error');
    }
}

async function populateVoiceSelectFromAPI(standardVoices, clonedVoices) {
    const voiceSelect = document.getElementById('voice-personality');
    if (!voiceSelect) return;
    
    voiceSelect.innerHTML = '';
    
    // Voix standard
    if (standardVoices && Object.keys(standardVoices).length > 0) {
        const standardGroup = document.createElement('optgroup');
        standardGroup.label = 'üé§ Voix standard';
        
        Object.entries(standardVoices).forEach(([id, voice]) => {
            const option = document.createElement('option');
            option.value = id;
            option.textContent = voice.display_name || voice.name;
            standardGroup.appendChild(option);
        });
        
        voiceSelect.appendChild(standardGroup);
    }
    
    // Voix clon√©es
    if (clonedVoices && Object.keys(clonedVoices).length > 0) {
        const clonedGroup = document.createElement('optgroup');
        clonedGroup.label = 'üé≠ Voix clon√©es';
        
        Object.entries(clonedVoices).forEach(([id, voice]) => {
            if (voice.processing_status === 'ready') {
                const option = document.createElement('option');
                option.value = id;
                option.textContent = voice.display_name || voice.name;
                clonedGroup.appendChild(option);
            }
        });
        
        voiceSelect.appendChild(clonedGroup);
    }
    
    // S√©lectionner la voix actuelle depuis la config
    if (window.jarvisConfig?.voice?.personality) {
        voiceSelect.value = window.jarvisConfig.voice.personality;
    }
}

/**
 * üöÄ NOUVEAU: Chargement des r√¥les depuis l'API
 */
async function loadRolesFromAPI() {
    try {
        const response = await fetch('config/roles.json');
        const data = await response.json();
        
        const roleSelect = document.getElementById('role-select');
        if (roleSelect && data.roles) {
            roleSelect.innerHTML = '';
            Object.values(data.roles).forEach(role => {
                const option = document.createElement('option');
                option.value = role.id;
                option.textContent = role.name;
                roleSelect.appendChild(option);
            });
            
            // S√©lectionner le r√¥le actuel
            if (window.jarvisConfig?.llm?.role) {
                roleSelect.value = window.jarvisConfig.llm.role;
            }
            
            addLogEntry(`üë®‚Äçüè´ ${Object.keys(data.roles).length} r√¥les charg√©s`, 'info');
        }
    } catch (error) {
        addLogEntry(`‚ùå Erreur chargement r√¥les: ${error.message}`, 'error');
    }
}

/**
 * üöÄ NOUVEAU: Chargement des mod√®les depuis l'API
 */
async function loadModelsFromAPI() {
    try {
        const response = await fetch('/api/models');
        const data = await response.json();
        
        const modelSelect = document.getElementById('llm-model');
        if (modelSelect && data.success && data.models) {
            modelSelect.innerHTML = '';
            data.models.forEach(model => {
                const option = document.createElement('option');
                option.value = model;
                option.textContent = model;
                modelSelect.appendChild(option);
            });
            
            // S√©lectionner le mod√®le actuel
            if (window.jarvisConfig?.llm?.model) {
                modelSelect.value = window.jarvisConfig.llm.model;
            }
            
            addLogEntry(`üß† ${data.models.length} mod√®les LLM charg√©s`, 'info');
        }
    } catch (error) {
        addLogEntry(`‚ùå Erreur chargement mod√®les: ${error.message}`, 'error');
    }
}

/**
 * üöÄ NOUVEAU: Chargement des backgrounds depuis l'API
 */
async function loadBackgroundsFromAPI() {
    try {
        const response = await fetch('/api/backgrounds');
        const data = await response.json();
        
        const backgroundSelect = document.getElementById('interface-background');
        if (backgroundSelect && data.success && data.backgrounds) {
            backgroundSelect.innerHTML = '';
            
            data.backgrounds.forEach(bg => {
                const option = document.createElement('option');
                option.value = bg.path;
                option.textContent = bg.name;
                backgroundSelect.appendChild(option);
            });
            
            // S√©lectionner le background actuel
            if (window.jarvisConfig?.interface?.background) {
                backgroundSelect.value = window.jarvisConfig.interface.background;
            }
            
            addLogEntry(`üñºÔ∏è ${data.backgrounds.length} arri√®re-plans charg√©s`, 'info');
        }
    } catch (error) {
        addLogEntry(`‚ùå Erreur chargement backgrounds: ${error.message}`, 'error');
    }
}

/**
 * üöÄ MODIFI√â: Mise √† jour de l'interface avec config unifi√©e
 */
async function updateUI() {
    try {
        addLogEntry('üé® Mise √† jour interface unifi√©e...', 'info');
        
        // Mettre √† jour les informations de configuration affich√©es
        if (window.jarvisConfig) {
            updateConfigDisplay(window.jarvisConfig);
        }
        
        // Initialiser les sliders avec les valeurs de la config
        initializeSlidersFromConfig();
        
        // Mettre √† jour les s√©lecteurs avec les valeurs actuelles
        updateSelectorsFromConfig();
        
        addLogEntry('‚úÖ Interface unifi√©e mise √† jour', 'success');
        
    } catch (error) {
        addLogEntry(`‚ùå Erreur mise √† jour UI: ${error.message}`, 'error');
    }
}

function updateConfigDisplay(config) {
    // Mettre √† jour l'affichage de la config dans le debug panel si pr√©sent
    const configElements = {
        'config-personality': config.voice?.personality || 'Non d√©fini',
        'config-llm': config.llm?.model || 'Non d√©fini',
        'config-tts': config.voice?.tts_model || 'Non d√©fini'
    };
    
    Object.entries(configElements).forEach(([elementId, value]) => {
        const element = document.getElementById(elementId);
        if (element) {
            element.textContent = value;
        }
    });
}

function initializeSlidersFromConfig() {
    if (!window.jarvisConfig) return;
    
    const sliderMappings = [
        { id: 'voice-speed', value: window.jarvisConfig.audio?.output?.speed || 1.0, suffix: 'x' },
        { id: 'voice-volume', value: window.jarvisConfig.audio?.output?.volume || 90, suffix: '%' },
        { id: 'audio-sensitivity', value: window.jarvisConfig.audio?.input?.sensitivity || 5, suffix: '' },
        { id: 'llm-temperature', value: window.jarvisConfig.llm?.temperature || 0.7, suffix: '' },
        { id: 'background-opacity', value: window.jarvisConfig.interface?.background_opacity || 30, suffix: '%' }
    ];
    
    sliderMappings.forEach(mapping => {
        const slider = document.getElementById(mapping.id);
        const valueElement = document.getElementById(`${mapping.id}-value`);
        
        if (slider) {
            slider.value = mapping.value;
        }
        if (valueElement) {
            valueElement.textContent = mapping.value + mapping.suffix;
        }
    });
}

function updateSelectorsFromConfig() {
    if (!window.jarvisConfig) return;
    
    const selectorMappings = [
        { id: 'interface-theme', value: window.jarvisConfig.interface?.theme },
        { id: 'interface-background', value: window.jarvisConfig.interface?.background },
        { id: 'voice-personality', value: window.jarvisConfig.voice?.personality },
        { id: 'llm-model', value: window.jarvisConfig.llm?.model },
        { id: 'role-select', value: window.jarvisConfig.llm?.role }
    ];
    
    selectorMappings.forEach(mapping => {
        const element = document.getElementById(mapping.id);
        if (element && mapping.value) {
            element.value = mapping.value;
        }
    });
}

/**
 * ‚úÖ GARD√â: Fonctions existantes compatibles
 */
function checkBrowserCompatibility() {
    // V√©rifications basiques du navigateur
    const required = ['fetch', 'WebSocket', 'Promise', 'localStorage'];
    const missing = required.filter(feature => !(feature in window));
    
    if (missing.length > 0) {
        addLogEntry(`‚ùå Fonctionnalit√©s manquantes: ${missing.join(', ')}`, 'error');
        return false;
    }
    
    return true;
}

function startBackgroundServices() {
    addLogEntry('‚öôÔ∏è D√©marrage des services...', 'info');
    
    // Keep-alive WebSocket (d√©j√† d√©marr√© dans websocket-manager.js)   
    // Nettoyage p√©riodique des logs
    setInterval(() => {
        cleanupLogs();
    }, 300000); // Toutes les 5 minutes
    
    // V√©rification de l'√©tat de connexion
    setInterval(() => {
        if (!isConnected && ws && ws.readyState === WebSocket.CLOSED) {
            addLogEntry('üîÑ Reconnexion automatique...', 'info');
            initializeWebSocket();
        }
    }, 30000); // Toutes les 30 secondes
    
    addLogEntry('‚úÖ Services de fond d√©marr√©s', 'success');
}

/**
 * üöÄ NOUVEAU: Fonction de recharge de la configuration
 */
async function reloadConfig() {
    try {
        addLogEntry('üîÑ Rechargement configuration...', 'info');
        
        const configLoaded = await initializeConfig();
        if (configLoaded) {
            await updateUI();
            addLogEntry('‚úÖ Configuration recharg√©e', 'success');
            showToast('‚úÖ Configuration mise √† jour', 'success');
        } else {
            addLogEntry('‚ùå √âchec rechargement configuration', 'error');
            showToast('‚ùå Erreur rechargement', 'error');
        }
        
    } catch (error) {
        addLogEntry(`‚ùå Erreur rechargement: ${error.message}`, 'error');
        showToast('‚ùå Erreur rechargement', 'error');
    }
}

/**
 * ‚úÖ GARD√â: Gestionnaire d'erreurs
 */
function handleError(error, context = '') {
    console.error(`Erreur ${context}:`, error);
    addLogEntry(`‚ùå Erreur ${context}: ${error.message}`, 'error');
}

/**
 * ‚úÖ GARD√â: Gestion de la visibilit√© de page
 */
function handleVisibilityChange() {
    if (document.hidden) {
        addLogEntry('üëÅÔ∏è Page masqu√©e, r√©duction activit√©', 'info');
        // R√©duire les activit√©s de fond
    } else {
        addLogEntry('üëÅÔ∏è Page visible, reprise activit√©', 'info');
        // Reprendre les activit√©s normales
        if (!isConnected) {
            initializeWebSocket();
        }
    }
}

/**
 * üöÄ MODIFI√â: Point d'entr√©e principal
 */
async function main() {
    try {
        // Gestionnaire de visibilit√©
        document.addEventListener('visibilitychange', handleVisibilityChange);
        
        // Initialisation principale
        const success = await initializeJarvis();
        
        if (!success) {
            // Mode d√©grad√©
            addLogEntry('üîß Tentative de mode d√©grad√©...', 'warning');
            setTimeout(() => {
                if (confirm('L\'initialisation a √©chou√©. Essayer le mode d√©grad√© ?')) {
                    // Mode d√©grad√©: juste l'interface sans WebSocket
                    initializeConfig();
                    updateUI();
                    showToast('‚ö†Ô∏è Mode d√©grad√© activ√©', 'warning');
                }
            }, 2000);
        }
    } catch (error) {
        handleError(error, 'Initialisation principale');
        showToast('‚ùå Erreur critique', 'error');
    }
}

// Point d'entr√©e unique de l'application
document.addEventListener('DOMContentLoaded', main);

// Fallback si DOMContentLoaded a d√©j√† √©t√© d√©clench√©
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', main);
} else {
    main();
}

// Export des fonctions pour utilisation globale
if (typeof window !== 'undefined') {
    window.reloadConfig = reloadConfig;
    window.updateUI = updateUI;
    window.initializeConfig = initializeConfig;
}

console.log('üöÄ App-main unifi√© charg√©');

==================================================
FICHIER: .\web_interface\js\camera-panel.js
==================================================

/**
 * camera-panel.js - Panneau de vision par cam√©ra
 * üì∑ Interface pour la future int√©gration vision IA
 * Pr√©paration de l'infrastructure pour analyse d'image
 */

// √âtat du panneau cam√©ra
let cameraStream = null;
let currentCamera = 'user'; // 'user' (face) ou 'environment' (arri√®re)
let capturedImages = [];
let maxCapturedImages = 5;

/**
 * Toggle le panneau cam√©ra
 */
function toggleCameraPanel() {
    cameraVisible = !cameraVisible;
    updateCameraVisibility();
    
    addLogEntry(`üîç Camera: ${cameraVisible ? 'activ√©' : 'd√©sactiv√©'}`, 'info');
    saveSettings();
}

/**
 * Met √† jour la visibilit√© du panneau de camera
 */
function updateCameraVisibility() {
    const cameraSection = document.getElementById('camera-section');
    const mainContent = document.querySelector('.main-content');
    
    if (!cameraSection || !mainContent) return;
    
    if (cameraVisible) {
        cameraSection.classList.remove('hidden');
        mainContent.classList.remove('camera-hidden');
    } else {
        cameraSection.classList.add('hidden');
        mainContent.classList.add('camera-hidden');
    }
}

/**
 * D√©marre la cam√©ra
 */
async function startCamera() {
    try {
        const video = document.getElementById('camera-feed');
        
        if (!video) {
            addLogEntry('‚ö†Ô∏è √âl√©ment vid√©o non trouv√©', 'warning');
            return;
        }
        
        // Configuration de la cam√©ra
        const constraints = {
            video: {
                width: { ideal: 1280 },
                height: { ideal: 720 },
                facingMode: currentCamera
            },
            audio: false
        };
        
        // Demander l'acc√®s √† la cam√©ra
        cameraStream = await navigator.mediaDevices.getUserMedia(constraints);
        
        // Attacher le stream √† la vid√©o
        video.srcObject = cameraStream;
        
        // Attendre que la vid√©o soit pr√™te
        video.onloadedmetadata = () => {
            video.play();
            updateCameraInfo();
            enableCameraControls(true);
            addLogEntry(`üì∑ Cam√©ra activ√©e (${video.videoWidth}x${video.videoHeight})`, 'success');
        };
        
    } catch (error) {
        console.error('Erreur cam√©ra:', error);
        
        let errorMessage = 'Impossible d\'acc√©der √† la cam√©ra';
        
        if (error.name === 'NotAllowedError') {
            errorMessage = 'Acc√®s cam√©ra refus√©. V√©rifiez les permissions.';
        } else if (error.name === 'NotFoundError') {
            errorMessage = 'Aucune cam√©ra d√©tect√©e';
        } else if (error.name === 'NotReadableError') {
            errorMessage = 'Cam√©ra d√©j√† utilis√©e par une autre application';
        }
        
        addLogEntry(`‚ùå ${errorMessage}`, 'error');
        showToast(`‚ùå ${errorMessage}`, 'error');
        showCameraError(errorMessage);
    }
}

/**
 * Arr√™te la cam√©ra
 */
function stopCamera() {
    if (cameraStream) {
        // Arr√™ter toutes les pistes
        cameraStream.getTracks().forEach(track => {
            track.stop();
        });
        
        cameraStream = null;
        
        // Nettoyer la vid√©o
        const video = document.getElementById('camera-feed');
        if (video) {
            video.srcObject = null;
        }
        
        enableCameraControls(false);
        addLogEntry('üì∑ Cam√©ra d√©sactiv√©e', 'info');
    }
}

/**
 * Bascule entre cam√©ra avant/arri√®re (mobile)
 */
async function switchCamera() {
    currentCamera = currentCamera === 'user' ? 'environment' : 'user';
    
    stopCamera();
    await startCamera();
    
    addLogEntry(`üì∑ Bascul√© vers cam√©ra ${currentCamera === 'user' ? 'avant' : 'arri√®re'}`, 'info');
}

/**
 * Capture une image de la cam√©ra
 */
function captureImage() {
    const video = document.getElementById('camera-feed');
    const canvas = document.createElement('canvas');
    
    if (!video || !video.srcObject) {
        showToast('‚ö†Ô∏è Cam√©ra non active', 'warning');
        return;
    }
    
    // D√©finir les dimensions du canvas
    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    
    // Dessiner l'image
    const ctx = canvas.getContext('2d');
    ctx.drawImage(video, 0, 0);
    
    // Effet de flash
    flashEffect();
    
    // Convertir en blob
    canvas.toBlob((blob) => {
        if (blob) {
            // Cr√©er un objet image
            const imageData = {
                id: `img_${Date.now()}`,
                blob: blob,
                url: URL.createObjectURL(blob),
                timestamp: new Date().toISOString(),
                width: canvas.width,
                height: canvas.height
            };
            
            // Ajouter √† la liste des captures
            capturedImages.unshift(imageData);
            
            // Limiter le nombre d'images
            if (capturedImages.length > maxCapturedImages) {
                const removed = capturedImages.pop();
                URL.revokeObjectURL(removed.url);
            }
            
            // Afficher la capture
            displayCapturedImage(imageData);
            
            showToast('üì∏ Image captur√©e', 'success');
            addLogEntry(`üì∏ Capture ${imageData.id} (${canvas.width}x${canvas.height})`, 'info');
            
            // Afficher les options d'analyse
            showAnalysisOptions(imageData);
        }
    }, 'image/jpeg', 0.9);
}

/**
 * Effet de flash lors de la capture
 */
function flashEffect() {
    const flash = document.createElement('div');
    flash.className = 'camera-flash';
    document.getElementById('camera-panel')?.appendChild(flash);
    
    setTimeout(() => flash.remove(), 300);
}

/**
 * Affiche une image captur√©e
 */
function displayCapturedImage(imageData) {
    const container = document.getElementById('captured-images');
    
    if (!container) return;
    
    // Cr√©er l'√©l√©ment image
    const imageCard = document.createElement('div');
    imageCard.className = 'captured-image-card';
    imageCard.innerHTML = `
        <img src="${imageData.url}" alt="Capture ${imageData.id}">
        <div class="image-overlay">
            <button onclick="analyzeImage('${imageData.id}')" class="mini-btn" title="Analyser">
                üîç
            </button>
            <button onclick="saveImage('${imageData.id}')" class="mini-btn" title="Sauvegarder">
                üíæ
            </button>
            <button onclick="deleteImage('${imageData.id}')" class="mini-btn" title="Supprimer">
                ‚ùå
            </button>
        </div>
    `;
    
    // Ajouter au d√©but
    if (container.firstChild) {
        container.insertBefore(imageCard, container.firstChild);
    } else {
        container.appendChild(imageCard);
    }
}

/**
 * Affiche les options d'analyse (futures fonctionnalit√©s)
 */
function showAnalysisOptions(imageData) {
    const optionsDiv = document.getElementById('analysis-options');
    
    if (!optionsDiv) return;
    
    optionsDiv.innerHTML = `
        <div class="analysis-card">
            <h4>üîç Derni√®re capture</h4>
            <p>Dimensions: ${imageData.width}x${imageData.height}</p>
            <div class="future-features">
                <h5>üöß Fonctionnalit√©s en d√©veloppement:</h5>
                <button class="future-btn" disabled onclick="performOCR('${imageData.id}')">
                    üìù Extraire le texte (OCR)
                </button>
                <button class="future-btn" disabled onclick="detectObjects('${imageData.id}')">
                    üéØ D√©tecter les objets
                </button>
                <button class="future-btn" disabled onclick="describeScene('${imageData.id}')">
                    üñºÔ∏è D√©crire la sc√®ne
                </button>
                <button class="future-btn" disabled onclick="analyzeFaces('${imageData.id}')">
                    üë§ Analyse faciale
                </button>
                <button class="future-btn" disabled onclick="readDocument('${imageData.id}')">
                    üìÑ Lire le document
                </button>
                <button class="future-btn" disabled onclick="translateText('${imageData.id}')">
                    üåê Traduire le texte
                </button>
            </div>
        </div>
    `;
}

/**
 * Analyse une image (placeholder pour future impl√©mentation)
 */
function analyzeImage(imageId) {
    const image = capturedImages.find(img => img.id === imageId);
    
    if (!image) {
        showToast('‚ö†Ô∏è Image non trouv√©e', 'warning');
        return;
    }
    
    showToast('üîç Analyse d\'image (fonctionnalit√© en d√©veloppement)', 'info');
    addLogEntry(`üîç Demande d'analyse pour ${imageId} (non impl√©ment√©)`, 'info');
    
    // Simuler une analyse
    setTimeout(() => {
        showToast('‚ÑπÔ∏è L\'analyse d\'image sera bient√¥t disponible', 'info');
    }, 1000);
}

/**
 * Sauvegarde une image captur√©e
 */
function saveImage(imageId) {
    const image = capturedImages.find(img => img.id === imageId);
    
    if (!image) {
        showToast('‚ö†Ô∏è Image non trouv√©e', 'warning');
        return;
    }
    
    // Cr√©er un lien de t√©l√©chargement
    const a = document.createElement('a');
    a.href = image.url;
    a.download = `capture_${imageId}.jpg`;
    a.click();
    
    showToast('üíæ Image sauvegard√©e', 'success');
    addLogEntry(`üíæ Image ${imageId} t√©l√©charg√©e`, 'info');
}

/**
 * Supprime une image captur√©e
 */
function deleteImage(imageId) {
    const index = capturedImages.findIndex(img => img.id === imageId);
    
    if (index === -1) return;
    
    // Lib√©rer l'URL
    URL.revokeObjectURL(capturedImages[index].url);
    
    // Supprimer de la liste
    capturedImages.splice(index, 1);
    
    // Supprimer de l'affichage
    const container = document.getElementById('captured-images');
    if (container) {
        const cards = container.querySelectorAll('.captured-image-card');
        if (cards[capturedImages.length]) {
            cards[capturedImages.length].remove();
        }
    }
    
    showToast('üóëÔ∏è Image supprim√©e', 'info');
}

/**
 * Active/d√©sactive les contr√¥les cam√©ra
 */
function enableCameraControls(enabled) {
    const controls = document.querySelectorAll('.camera-controls button:not(.always-enabled)');
    controls.forEach(btn => {
        btn.disabled = !enabled;
    });
}

/**
 * Met √† jour les informations de la cam√©ra
 */
function updateCameraInfo() {
    const video = document.getElementById('camera-feed');
    const infoDiv = document.getElementById('camera-info');
    
    if (!video || !infoDiv) return;
    
    const track = cameraStream?.getVideoTracks()[0];
    const settings = track?.getSettings();
    
    if (settings) {
        infoDiv.innerHTML = `
            <div class="camera-stats">
                <span>üìπ ${settings.width}x${settings.height}</span>
                <span>üéØ ${settings.frameRate?.toFixed(0) || 30} FPS</span>
                <span>üì± ${settings.facingMode || currentCamera}</span>
            </div>
        `;
    }
}

/**
 * Affiche l'√©tat de d√©veloppement des fonctionnalit√©s
 */
function showCameraDevStatus() {
    const statusDiv = document.getElementById('camera-dev-status');
    
    if (!statusDiv) return;
    
    statusDiv.innerHTML = `
        <div class="dev-status">
            <h3>üöß Vision IA - En D√©veloppement</h3>
            <p>La reconnaissance visuelle arrive bient√¥t !</p>
            
            <div class="feature-roadmap">
                <h4>üìÖ Roadmap des fonctionnalit√©s:</h4>
                
                <div class="feature-section">
                    <h5>‚úÖ Disponible</h5>
                    <ul>
                        <li>Acc√®s cam√©ra (avant/arri√®re)</li>
                        <li>Capture d'image haute r√©solution</li>
                        <li>Sauvegarde locale des captures</li>
                        <li>Pr√©visualisation en temps r√©el</li>
                    </ul>
                </div>
                
                <div class="feature-section">
                    <h5>üîÑ En cours</h5>
                    <ul>
                        <li>OCR - Extraction de texte</li>
                        <li>D√©tection d'objets basique</li>
                        <li>Analyse de documents</li>
                    </ul>
                </div>
                
                <div class="feature-section">
                    <h5>üìã Planifi√©</h5>
                    <ul>
                        <li>Description de sc√®ne (GPT-4 Vision)</li>
                        <li>Traduction visuelle en temps r√©el</li>
                        <li>Reconnaissance de codes QR/barres</li>
                        <li>Analyse d'√©motions (opt-in)</li>
                        <li>D√©tection de mouvements</li>
                        <li>R√©alit√© augment√©e simple</li>
                    </ul>
                </div>
            </div>
            
            <div class="dev-note">
                <p>üí° <strong>Note:</strong> Les fonctionnalit√©s d'IA n√©cessiteront:</p>
                <ul>
                    <li>Installation de mod√®les sp√©cifiques (YOLO, Tesseract)</li>
                    <li>Configuration GPU recommand√©e pour performances</li>
                    <li>Connexion API pour certaines analyses avanc√©es</li>
                </ul>
            </div>
        </div>
    `;
}

/**
 * Affiche une erreur cam√©ra
 */
function showCameraError(message) {
    const video = document.getElementById('camera-feed');
    const container = video?.parentElement;
    
    if (container) {
        const errorDiv = document.createElement('div');
        errorDiv.className = 'camera-error';
        errorDiv.innerHTML = `
            <div class="error-content">
                <h3>‚ùå Erreur Cam√©ra</h3>
                <p>${message}</p>
                <button onclick="retryCamera()" class="retry-btn">
                    üîÑ R√©essayer
                </button>
            </div>
        `;
        
        container.appendChild(errorDiv);
    }
}

/**
 * R√©essaye de d√©marrer la cam√©ra
 */
async function retryCamera() {
    const errorDiv = document.querySelector('.camera-error');
    if (errorDiv) {
        errorDiv.remove();
    }
    
    await startCamera();
}

/**
 * Applique des filtres √† la vid√©o (fun feature)
 */
let currentFilter = 0;
const filters = [
    'none',
    'grayscale(100%)',
    'sepia(100%)',
    'contrast(150%)',
    'brightness(150%)',
    'hue-rotate(90deg)',
    'hue-rotate(180deg)',
    'invert(100%)',
    'blur(3px)',
    'saturate(200%)'
];

function toggleCameraFilters() {
    const video = document.getElementById('camera-feed');
    
    if (!video) return;
    
    currentFilter = (currentFilter + 1) % filters.length;
    video.style.filter = filters[currentFilter];
    
    const filterName = filters[currentFilter] === 'none' ? 'Aucun' : filters[currentFilter];
    showToast(`üé® Filtre: ${filterName}`, 'info');
}

/**
 * Nettoie les ressources au d√©chargement
 */
function cleanupCameraResources() {
    stopCamera();
    
    // Lib√©rer les URLs des images
    capturedImages.forEach(img => {
        URL.revokeObjectURL(img.url);
    });
    
    capturedImages = [];
}

/**
 * Initialisation au chargement de la page
 */
document.addEventListener('DOMContentLoaded', () => {
    // V√©rifier le support de getUserMedia
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        const panel = document.getElementById('camera-panel');
        if (panel) {
            panel.innerHTML = `
                <div class="no-camera-support">
                    <h3>‚ùå Cam√©ra non support√©e</h3>
                    <p>Votre navigateur ne supporte pas l'acc√®s √† la cam√©ra.</p>
                    <p>Utilisez un navigateur moderne (Chrome, Firefox, Edge).</p>
                </div>
            `;
        }
    }
    
    // Nettoyer √† la fermeture
    window.addEventListener('beforeunload', cleanupCameraResources);
});

// Export pour utilisation externe
if (typeof window !== 'undefined') {
    window.CameraPanel = {
        toggle: toggleCameraPanel,
        capture: captureImage,
        switchCamera: switchCamera,
        applyFilter: toggleCameraFilters,
        getCapturedImages: () => capturedImages
    };
}

==================================================
FICHIER: .\web_interface\js\config-loader.js
==================================================

/**
 * config-loader.js - Chargement des configurations JSON
 * üóÑÔ∏è Hippocampe - Gestion de la m√©moire de configuration
 */

/**
 * Charge toutes les configurations JSON au d√©marrage
 * @returns {Promise<boolean>} Succ√®s du chargement
 */

/**
 * Peuple les param√®tres de l'interface depuis la configuration charg√©e
 */
function populateSettingsFromConfig() {
    addLogEntry('üîÑ Mise √† jour interface depuis configuration', 'info');
    
    // Peupler les voix disponibles
    populateVoiceSelect();
    
    // Peupler les mod√®les LLM
    populateModelSelect();
    
    // Peupler les th√®mes
    populateThemeSelect();
    
    // Peupler les arri√®re-plans
    populateBackgroundSelect();
}

/**
 * Peuple la liste des voix
 */
async function populateVoiceSelect() {
    const voiceSelect = document.getElementById('voice-personality');
    if (!voiceSelect) return;
    
    if (!window.voiceManager?.isLoaded) {
        setTimeout(populateVoiceSelect, 500);
        return;
    }
    
    // 1. Remplir la liste
    window.voiceManager.populateSelect('voice-personality');
    
    // 2. R√©cup√©rer la vraie valeur depuis le serveur (comme au d√©marrage)
    try {
        const response = await fetch('/api/voice/current');
        const data = await response.json();
        
        if (data.voice_id && voiceSelect.querySelector(`option[value="${data.voice_id}"]`)) {
            voiceSelect.value = data.voice_id;
            addLogEntry(`‚úÖ Voix serveur restaur√©e: ${data.voice_id}`, 'success');
            return;
        }
    } catch (error) {
        console.warn('Erreur API voice/current:', error);
    }
    
    // 3. Fallback localStorage
    const savedSettings = loadSavedSettings();
    if (savedSettings?.personality) {
        voiceSelect.value = savedSettings.personality;
        addLogEntry(`‚úÖ Voix localStorage: ${savedSettings.personality}`, 'info');
    }
}

/**
 * Peuple la liste des mod√®les LLM
 */
function populateModelSelect() {
    const llmSelect = document.getElementById('llm-model');
    if (!llmSelect || !config.models?.llm_models) return;
    
    llmSelect.innerHTML = '';
    Object.values(config.models.llm_models).forEach(model => {
        const option = document.createElement('option');
        option.value = model.id;
        option.textContent = model.display_name;
        if (model.default) option.selected = true;
        llmSelect.appendChild(option);
    });
}

/**
 * Peuple la liste des r√¥les pour LLM
 */
async function loadRoles() {
    try {
        const response = await fetch('config/roles.json');
        const data = await response.json();
        config.roles = data;
        
        const roleSelect = document.getElementById('role-select');
        if (roleSelect && data.roles) {
            roleSelect.innerHTML = '';
            Object.values(data.roles).forEach(role => {
                const option = document.createElement('option');
                option.value = role.id;
                option.textContent = role.name;
                roleSelect.appendChild(option);
            });
            
            // üöÄ AJOUTER : S√©lectionner la valeur par d√©faut ou sauvegard√©e
            const savedSettings = loadSavedSettings();
            const currentRole = savedSettings?.role || data.default_role;
            if (currentRole) {
                roleSelect.value = currentRole;
            }
        }
        
        addLogEntry('‚úÖ R√¥les charg√©s', 'success');
    } catch (error) {
        addLogEntry(`‚ùå Erreur chargement r√¥les: ${error.message}`, 'error');
    }
}

/**
 * Peuple la liste des th√®mes
 */
function populateThemeSelect() {
    const themeSelect = document.getElementById('interface-theme');
    if (!themeSelect || !config.themes?.themes) return;
    
    themeSelect.innerHTML = '';
    Object.values(config.themes.themes).forEach(theme => {
        const option = document.createElement('option');
        option.value = theme.id;
        option.textContent = theme.current_name;
        themeSelect.appendChild(option);
    });
    
    themeSelect.value = currentTheme;
}

/**
 * Peuple la liste des arri√®re-plans depuis l'API
 */
async function populateBackgroundSelect() {
    try {
        const response = await fetch('/api/backgrounds');
        const data = await response.json();
        
        const backgroundSelect = document.getElementById('interface-background');
        if (!backgroundSelect || !data.success) return;
        
        backgroundSelect.innerHTML = '';
        
        data.backgrounds.forEach(bg => {
            const option = document.createElement('option');
            option.value = bg.path || 'default';
            option.textContent = bg.name;
            backgroundSelect.appendChild(option);
        });
        
        // S√©lectionner la valeur sauvegard√©e
        const savedBackground = localStorage.getItem('jarvis-background') || 'default';
        backgroundSelect.value = savedBackground;
        
        addLogEntry('‚úÖ Arri√®re-plans charg√©s', 'success');
    } catch (error) {
        addLogEntry(`‚ùå Erreur chargement arri√®re-plans: ${error.message}`, 'error');
    }
}

/**
 * Met √† jour l'affichage de l'arri√®re-plan actuel
 */
function updateBackgroundDisplay(backgroundPath, backgroundName) {
    // Mettre √† jour le texte dans l'onglet Config
    const configBg = document.getElementById('config-background');
    if (configBg) {
        configBg.textContent = backgroundName || backgroundPath || 'Par d√©faut';
    }
    
    // Mettre √† jour la preview
    const previewContainer = document.getElementById('background-preview-container');
    const previewImg = document.getElementById('background-preview');
    
    if (previewContainer && previewImg) {
        if (backgroundPath && backgroundPath !== 'default') {
            const imagePath = backgroundPath.startsWith('images/') ? 
                `static/${backgroundPath}` : 
                `static/images/${backgroundPath}`;
            
            previewImg.src = imagePath;
            previewImg.onload = () => {
                previewContainer.style.display = 'block';
                console.log('‚úÖ Preview image charg√©e');
            };
            previewImg.onerror = () => {
                console.error('‚ùå Preview image √©chou√©e');
                previewContainer.style.display = 'none';
            };
        } else {
            previewContainer.style.display = 'none';
        }
    }
}
async function populateBackgroundSelect() {
    try {
        const response = await fetch('/api/backgrounds');
        const data = await response.json();
        
        const backgroundSelect = document.getElementById('interface-background');
        if (!backgroundSelect || !data.success) return;
        
        backgroundSelect.innerHTML = '';
        
        // ‚úÖ Utiliser les images scann√©es dynamiquement
        data.backgrounds.forEach(bg => {
            const option = document.createElement('option');
            option.value = bg.path || 'default';  // Utiliser le path comme valeur
            option.textContent = bg.name;
            option.dataset.filename = bg.filename || '';
            backgroundSelect.appendChild(option);
        });
        
        // ‚úÖ S√©lectionner l'arri√®re-plan actuel
        const saved = localStorage.getItem('jarvis-background');
        if (saved) {
            backgroundSelect.value = saved;
        }
        
    } catch (error) {
        console.error('Erreur chargement backgrounds:', error);
    }
}

async function populateAllSelects() {
    addLogEntry('üìã Chargement des listes de s√©lection...', 'info');
    
    // Peupler les voix disponibles
    populateVoiceSelect();
    
    // Peupler les mod√®les LLM
    populateModelSelect();
    
    // Peupler les R√¥les LLM
    loadRoles();
    
    // Peupler les th√®mes
    populateThemeSelect();
    
    // ‚úÖ AJOUTER : Peupler les arri√®re-plans
    await populateBackgroundSelect();
}

/**
 * Applique un arri√®re-plan par path
 */
function setBackground(backgroundPath) {
    const body = document.body;
    
    // ‚úÖ Nettoyer
    body.style.backgroundImage = '';
    body.style.backgroundColor = '';
    body.classList.remove('bg-image');
    
    if (backgroundPath && backgroundPath !== 'default') {
        // ‚úÖ Appliquer l'image
        body.style.backgroundImage = `url('${backgroundPath}')`;
        body.style.backgroundSize = 'cover';
        body.style.backgroundPosition = 'center';
        body.style.backgroundRepeat = 'no-repeat';
        body.classList.add('bg-image');
        
        addLogEntry(`üñºÔ∏è Arri√®re-plan: ${backgroundPath}`, 'info');
    } else {
        // ‚úÖ Par d√©faut
        body.style.backgroundColor = 'var(--bg-primary)';
        addLogEntry('üé® Arri√®re-plan par d√©faut', 'info');
    }
    
    // ‚úÖ Sauvegarder
    localStorage.setItem('jarvis-background', backgroundPath || 'default');
}

/**
 * Charge la configuration actuelle depuis l'API
 */
async function loadCurrentConfig() {
    try {
        const response = await fetch('/api/config');
        const data = await response.json();
        console.log('R√©ponse compl√®te du serveur (/api/config) :', data);
        if (data.voice) {
            const serverConfig = data.voice;

            const displayName = `Assistant virtuel - ${serverConfig.display_name}`;
            updatePersonality(displayName);
            addLogEntry(`‚úÖ ${displayName} charg√©`, 'success');
            
            // Mettre √† jour le th√®me si diff√©rent
            if (data.interface.theme !== currentTheme) {
                setTheme(data.interface.theme);
            }
            
            // Mettre √† jour l'affichage de configuration
            updateConfigDisplay(data);
            
            return serverConfig;
        } else {
            addLogEntry('‚ö†Ô∏è Config serveur non disponible', 'warning');
            return null;
        }
    } catch (error) {
        addLogEntry(`‚ùå Erreur chargement config serveur: ${error.message}`, 'error');
        return null;
    }
}

/**
 * Met √† jour l'affichage de la configuration dans l'interface
 * @param {Object} configData - Donn√©es de configuration
 */
function updateConfigDisplay(configData) {
    const elements = {
        'config-llm': configData.llm.model || 'llama3.1:8b',
        'config-tts': configData.voice.tts_model || 'edge-tts', 
        'config-personality': configData.voice.display_name || configData.voice.personality || 'Samantha',
        'role-select': configData.llm.role || 'assistant_general',
        'config-audio': configData.audio.output.device_index !== null ? `Device ${configData.audio.output.device_index}` : 'Auto',
        'config-theme': configData.interface.theme || 'light'
    };
    
    Object.entries(elements).forEach(([id, value]) => {
        const element = document.getElementById(id);
        if (element) {
            element.textContent = value;
        }
    });
    
    // Mettre √† jour la personnalit√© dans le header si disponible
    if (configData.display_name) {
        updatePersonality(configData.display_name);
    }
}

==================================================
FICHIER: .\web_interface\js\debug-logger.js
==================================================

/**
 * debug-logger.js - Gestion du debug et des logs
 * üîç Syst√®me de surveillance - Monitoring et diagnostic
 */

/**
 * Bascule l'affichage du panneau de debug
 */
function toggleDebug() {
    debugVisible = !debugVisible;
    updateDebugVisibility();
    
    addLogEntry(`üîç Debug: ${debugVisible ? 'activ√©' : 'd√©sactiv√©'}`, 'info');
    saveSettings();
}

/**
 * Met √† jour la visibilit√© du panneau de debug
 */
function updateDebugVisibility() {
    const debugSection = document.getElementById('debug-section');
    const mainContent = document.querySelector('.main-content');
    
    if (!debugSection || !mainContent) return;
    
    if (debugVisible) {
        debugSection.classList.remove('hidden');
        mainContent.classList.remove('debug-hidden');
    } else {
        debugSection.classList.add('hidden');
        mainContent.classList.add('debug-hidden');
    }
}

/**
 * Change d'onglet dans le panneau de debug
 * @param {string} tabName - Nom de l'onglet (logs, stats, config)
 * @param {Event} event - √âv√©nement du clic
 */
function switchDebugTab(tabName, event) {
    // Masquer tous les onglets debug
    document.querySelectorAll('.debug-tab').forEach(tab => {
        tab.classList.remove('active');
    });
    
    // D√©sactiver tous les boutons
    document.querySelectorAll('.debug-tabs .tab-btn').forEach(btn => {
        btn.classList.remove('active');
    });
    
    // Activer l'onglet et le bouton s√©lectionn√©s
    const targetTab = document.getElementById(`debug-${tabName}`);
    if (targetTab) {
        targetTab.classList.add('active');
        interfaceState.currentDebugTab = tabName;
    }
    
    if (event && event.target) {
        event.target.classList.add('active');
    }
    
    // Mettre √† jour le contenu selon l'onglet
    switch (tabName) {
        case 'stats':
            updateStatsDisplay();
            break;
        case 'config':
            updateDebugConfigDisplay();
            break;
    }
}

/**
 * Ajoute une entr√©e au log
 * @param {string} message - Message du log
 * @param {string} type - Type de log (info, success, warning, error)
 */
function addLogEntry(message, type = 'info') {
    const container = document.getElementById('log-container');
    if (!container) return;
    
    const logDiv = document.createElement('div');
    logDiv.className = `log-entry ${type}`;
    
    const timeSpan = document.createElement('span');
    timeSpan.className = 'log-time';
    timeSpan.textContent = new Date().toLocaleTimeString();
    
    const messageSpan = document.createElement('span');
    messageSpan.className = 'log-message';
    messageSpan.textContent = message;
    
    logDiv.appendChild(timeSpan);
    logDiv.appendChild(messageSpan);
    container.appendChild(logDiv);
    
    // Limiter le nombre de logs pour √©viter les probl√®mes de performance
    const maxLogs = 100;
    while (container.children.length > maxLogs) {
        container.removeChild(container.firstChild);
    }
    
    // Scroll automatique vers le bas
    container.scrollTop = container.scrollHeight;
    
    // √âmettre un √©v√©nement pour les autres modules
    document.dispatchEvent(new CustomEvent('logAdded', {
        detail: { message, type, timestamp: new Date() }
    }));
}

/**
 * Met √† jour l'affichage de configuration dans le debug
 */
function updateDebugConfigDisplay() {
    // Cette fonction sera appel√©e quand on ouvre l'onglet config
    const elements = {
        'config-personality': 'Chargement...',
        'config-llm': 'Chargement...',
        'config-tts': 'Chargement...',
        'config-audio': 'Chargement...'
    };
    
    // Mettre √† jour imm√©diatement avec les valeurs par d√©faut
    Object.entries(elements).forEach(([id, value]) => {
        const element = document.getElementById(id);
        if (element) {
            element.textContent = value;
        }
    });
    
    // Puis charger les vraies valeurs
    loadCurrentConfig().then(config => {
        if (config) {
            updateConfigDisplay(config);
        }
    });
}

/**
 * Exporte tous les logs en fichier
 */
function exportLogs() {
    const logEntries = document.querySelectorAll('.log-entry');
    if (logEntries.length === 0) {
        addLogEntry('‚ö†Ô∏è Aucun log √† exporter', 'warning');
        return;
    }
    
    let logText = `Logs Jarvis - ${new Date().toLocaleString()}\n`;
    logText += `================================\n\n`;
    
    logEntries.forEach(entry => {
        const time = entry.querySelector('.log-time')?.textContent || '';
        const message = entry.querySelector('.log-message')?.textContent || '';
        const type = entry.className.split(' ').find(cls => cls !== 'log-entry') || 'info';
        
        logText += `[${time}] ${type.toUpperCase()}: ${message}\n`;
    });
    
    logText += `\n\nInformations syst√®me:\n`;
    logText += `- Th√®me actuel: ${currentTheme}\n`;
    logText += `- Voice visible: ${voiceVisible}\n`;
    logText += `- Camera visible: ${cameraVisible}\n`;
    logText += `- Debug visible: ${debugVisible}\n`;
    logText += `- Connexion WebSocket: ${isConnected ? 'Connect√©e' : 'D√©connect√©e'}\n`;
    logText += `- Messages: ${stats.messages}\n`;
    logText += `- Tokens: ${stats.tokens}\n`;
    
    // Cr√©er et t√©l√©charger le fichier
    const blob = new Blob([logText], { type: 'text/plain;charset=utf-8' });
    const url = URL.createObjectURL(blob);
    
    const a = document.createElement('a');
    a.href = url;
    a.download = `jarvis-logs-${new Date().toISOString().split('T')[0]}.txt`;
    a.style.display = 'none';
    
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    
    URL.revokeObjectURL(url);
    
    addLogEntry('üíæ Logs export√©s', 'success');
}

/**
 * Efface tous les logs
 */
function clearLogs() {
    if (!confirm('Effacer tous les logs ?')) return;
    
    const container = document.getElementById('log-container');
    if (container) {
        container.innerHTML = `
            <div class="log-entry info">
                <span class="log-time">${new Date().toLocaleTimeString()}</span>
                <span class="log-message">Logs effac√©s</span>
            </div>
        `;
    }
    
    addLogEntry('üóëÔ∏è Logs pr√©c√©dents effac√©s', 'info');
}

/**
 * Filtre les logs par type
 * @param {string} filterType - Type de filtre (all, info, success, warning, error)
 */
function filterLogs(filterType) {
    const logEntries = document.querySelectorAll('.log-entry');
    
    logEntries.forEach(entry => {
        if (filterType === 'all' || entry.classList.contains(filterType)) {
            entry.style.display = 'flex';
        } else {
            entry.style.display = 'none';
        }
    });
    
    addLogEntry(`üîç Filtrage logs: ${filterType}`, 'info');
}

/**
 * G√©n√®re un rapport de diagnostic
 */
function generateDiagnosticReport() {
    const report = {
        timestamp: new Date().toISOString(),
        jarvis: {
            version: "0.5.0", // √Ä adapter selon votre versioning
            theme: currentTheme,
            voiceVisible: voiceVisible,
            cameraVisible: cameraVisible,
            debugVisible: debugVisible,
            connection: isConnected
        },
        stats: { ...stats },
        configuration: {
            hasThemes: !!(config.themes && Object.keys(config.themes).length > 0),
            hasVoices: !!(config.voices && Object.keys(config.voices).length > 0),
            hasModels: !!(config.models && Object.keys(config.models).length > 0),
            hasBackgrounds: !!(config.backgrounds && Object.keys(config.backgrounds).length > 0)
        },
        interface: {
            settingsModalOpen: interfaceState.settingsModalOpen,
            helpModalOpen: interfaceState.helpModalOpen,
            currentSettingsTab: interfaceState.currentSettingsTab,
            currentDebugTab: interfaceState.currentDebugTab
        },
        browser: {
            userAgent: navigator.userAgent,
            language: navigator.language,
            platform: navigator.platform,
            cookieEnabled: navigator.cookieEnabled,
            onLine: navigator.onLine
        }
    };
    
    return report;
}

/**
 * Exporte un rapport de diagnostic complet
 */
function exportDiagnosticReport() {
    const report = generateDiagnosticReport();
    
    const reportText = JSON.stringify(report, null, 2);
    
    const blob = new Blob([reportText], { type: 'application/json' });
    const url = URL.createObjectURL(blob);
    
    const a = document.createElement('a');
    a.href = url;
    a.download = `jarvis-diagnostic-${new Date().toISOString().split('T')[0]}.json`;
    a.style.display = 'none';
    
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    
    URL.revokeObjectURL(url);
    
    addLogEntry('üîß Rapport de diagnostic export√©', 'success');
}

/**
 * Active/d√©sactive le mode debug avanc√©
 */
function toggleAdvancedDebug() {
    const isAdvanced = localStorage.getItem('jarvis-advanced-debug') === 'true';
    const newState = !isAdvanced;
    
    localStorage.setItem('jarvis-advanced-debug', newState.toString());
    
    if (newState) {
        // Activer le debug avanc√©
        window.JarvisDebug = {
            config,
            stats,
            interfaceState,
            generateReport: generateDiagnosticReport,
            clearConfig: () => { config = { themes: {}, voices: {}, backgrounds: {}, models: {} }; },
            forceReconnect: initializeWebSocket
        };
        
        addLogEntry('üîß Mode debug avanc√© activ√© (window.JarvisDebug disponible)', 'success');
    } else {
        // D√©sactiver le debug avanc√©
        if (window.JarvisDebug) {
            delete window.JarvisDebug;
        }
        
        addLogEntry('üîß Mode debug avanc√© d√©sactiv√©', 'info');
    }
}

/**
 * Cr√©e des boutons de contr√¥le pour les logs
 */
function createLogControls() {
    const debugHeader = document.querySelector('.debug-header');
    if (!debugHeader) return;
    
    // V√©rifier si les contr√¥les existent d√©j√†
    if (debugHeader.querySelector('.log-controls')) return;
    
    const controlsDiv = document.createElement('div');
    controlsDiv.className = 'log-controls';
    controlsDiv.style.display = 'flex';
    controlsDiv.style.gap = '0.5rem';
    controlsDiv.style.alignItems = 'center';
    
    // Bouton export logs
    const exportBtn = document.createElement('button');
    exportBtn.textContent = 'üíæ';
    exportBtn.title = 'Exporter les logs';
    exportBtn.className = 'control-btn';
    exportBtn.addEventListener('click', exportLogs);
    
    // Bouton clear logs
    const clearBtn = document.createElement('button');
    clearBtn.textContent = 'üóëÔ∏è';
    clearBtn.title = 'Effacer les logs';
    clearBtn.className = 'control-btn';
    clearBtn.addEventListener('click', clearLogs);
    
    // Bouton diagnostic
    const diagnosticBtn = document.createElement('button');
    diagnosticBtn.textContent = 'üîß';
    diagnosticBtn.title = 'Export diagnostic';
    diagnosticBtn.className = 'control-btn';
    diagnosticBtn.addEventListener('click', exportDiagnosticReport);
    
    controlsDiv.appendChild(exportBtn);
    controlsDiv.appendChild(clearBtn);
    controlsDiv.appendChild(diagnosticBtn);
    
    // Ins√©rer avant le bouton de fermeture
    const closeBtn = debugHeader.querySelector('.close-debug');
    if (closeBtn) {
        debugHeader.insertBefore(controlsDiv, closeBtn);
    } else {
        debugHeader.appendChild(controlsDiv);
    }
}

/**
 * Initialise les √©v√©nements du debug
 */
function initializeDebugEvents() {
    // Bouton toggle debug principal
    const debugButton = document.querySelector('[onclick="toggleDebug()"]');
    if (debugButton) {
        debugButton.removeAttribute('onclick');
        debugButton.addEventListener('click', toggleDebug);
    }
    
    // Bouton fermeture debug
    const closeDebugButton = document.querySelector('.close-debug');
    if (closeDebugButton) {
        closeDebugButton.addEventListener('click', toggleDebug);
    }
    
    // Onglets debug
    document.querySelectorAll('.debug-tabs .tab-btn').forEach((button, index) => {
        const tabNames = ['logs', 'stats', 'config'];
        const tabName = tabNames[index];
        if (tabName) {
            button.addEventListener('click', (event) => switchDebugTab(tabName, event));
        }
    });
    
    // Raccourci clavier pour toggle debug (F12 ou Ctrl+D)
    document.addEventListener('keydown', function(event) {
        if  (event.ctrlKey && event.key === 'd') {
            event.preventDefault();
            toggleDebug();
        }
    });
    
    // Cr√©er les contr√¥les de logs
    createLogControls();
    
    // V√©rifier si le debug avanc√© √©tait activ√©
    if (localStorage.getItem('jarvis-advanced-debug') === 'true') {
        toggleAdvancedDebug();
    }
}

/**
 * Initialise l'√©tat du debug au d√©marrage
 */
function initializeState() {
    // Charger l'√©tat du debug depuis les param√®tres sauvegard√©s
    const savedSettings = loadSavedSettings();
    if (savedSettings && typeof savedSettings.voiceVisible === 'boolean') {
        voiceVisible = savedSettings.voiceVisible;
    }
    
    if (savedSettings && typeof savedSettings.cameraVisible === 'boolean') {
        cameraVisible = savedSettings.cameraVisible;
    }
    
    if (savedSettings && typeof savedSettings.debugVisible === 'boolean') {
        debugVisible = savedSettings.debugVisible;
    }
    
    if (typeof initVoices === 'function') {
        initVoices();
    }
    updateCameraVisibility();
    updateDebugVisibility();
}

// Initialiser les √©v√©nements et l'√©tat d√®s que le DOM est pr√™t
document.addEventListener('DOMContentLoaded', function() {
    initializeDebugEvents();
    initializeState();
});

// Log de d√©marrage
addLogEntry('üîç Syst√®me de debug initialis√©', 'info');

==================================================
FICHIER: .\web_interface\js\message-handler.js
==================================================

/**
 * message-handler.js - Gestion des messages et de la conversation
 * üí¨ Lobes Temporaux - Traitement du langage et communication
 * üöÄ OPTIMIS√â: DocumentFragment + Batching pour streaming performance
 */

// üöÄ Variables d'optimisation streaming
let tokenBuffer = '';
let bufferFlushTimer = null;
const BATCH_SIZE = 5;          // Flush tous les 5 caract√®res
const BATCH_TIMEOUT = 100;     // Ou tous les 100ms minimum
const SCROLL_THRESHOLD = 50;   // Seuil pour auto-scroll

/**
 * Envoie un message utilisateur
 */
function sendMessage() {
    const input = document.getElementById('message-input');
    const message = input.value.trim();

    if (!message) {
        addLogEntry('‚ö†Ô∏è Message vide ignor√©', 'warning');
        return;
    }
    
    console.log('üìù Envoi message:', message); 
    
    if (!isConnected) {
        addLogEntry('‚ùå Connexion requise pour envoyer un message', 'error');
        return;
    }
    
    // Ajouter le message √† l'interface
    addUserMessage(message);
    
    // Envoyer via WebSocket
    if (sendTextMessage(message)) {
        // Vider l'input seulement si l'envoi r√©ussit
        input.value = '';
        
        // Mettre √† jour les statistiques
        stats.messages++;
        updateStatsDisplay();
        
        addLogEntry(`üì§ Message envoy√©: ${message.substring(0, 50)}${message.length > 50 ? '...' : ''}`, 'info');
    }
}

/**
 * G√®re l'entr√©e vocale
 */
function toggleVoiceInput() {
    if (!isConnected) {
        addLogEntry('‚ùå Connexion requise pour l\'entr√©e vocale', 'error');
        return;
    }
    
    if (isListening) {
        addLogEntry('üëÇ √âcoute en cours, veuillez patienter...', 'info');
        return;
    }
    
    // D√©marrer l'√©coute
    if (requestVoiceInput()) {
        addLogEntry('üé§ D√©marrage de l\'√©coute vocale', 'info');
    }
}

/**
 * Met √† jour l'√©tat d'√©coute dans l'interface
 * @param {boolean} listening - √âtat d'√©coute
 */
function setListeningState(listening) {
    isListening = listening;
    const micButton = document.getElementById('mic-button');
    const micStatus = micButton?.querySelector('.mic-status');
    
    if (!micButton || !micStatus) return;
    
    if (listening) {
        micButton.classList.add('active');
        micStatus.textContent = '√âcoute...';
    } else {
        micButton.classList.remove('active');
        micStatus.textContent = 'Parler';
    }
}

/**
 * Ajoute un message utilisateur √† la conversation
 * @param {string} content - Contenu du message
 */
function addUserMessage(content) {
    const container = document.getElementById('dialogue-container');
    if (!container) return;
    
    const messageDiv = createMessageBubble('user', content);
    container.appendChild(messageDiv);
    scrollToBottom();
}

/**
 * Ajoute un message syst√®me √† la conversation
 * @param {string} content - Contenu du message
 * @param {string} type - Type de message (info, success, warning, error)
 */
function addSystemMessage(content, type = 'info') {
    const container = document.getElementById('dialogue-container');
    if (!container) return;
    
    const messageDiv = createMessageBubble('system', content);
    
    // Ajouter une classe pour le type si n√©cessaire
    if (type !== 'info') {
        messageDiv.classList.add(`system-${type}`);
    }
    
    container.appendChild(messageDiv);
    scrollToBottom();
}

/**
 * D√©marre une nouvelle r√©ponse de l'assistant
 */
function startNewAssistantResponse() {
    // Reset du buffer si n√©cessaire
    resetTokenBuffer();
    
    const container = document.getElementById('dialogue-container');
    if (!container) return;
    
    const messageDiv = createMessageBubble('assistant', '');
    messageDiv.id = 'current-response';
    container.appendChild(messageDiv);
    scrollToBottom();
}

/**
 * üöÄ OPTIMIS√â - Ajoute du contenu √† la r√©ponse actuelle (streaming avec batching)
 * @param {string} token - Token √† ajouter
 */
function appendToCurrentResponse(token) {
    // Accumuler dans le buffer
    tokenBuffer += token;
    
    // Flush imm√©diat si:
    // - Buffer d√©passe la taille limite
    // - Token contient un saut de ligne (fin de phrase/paragraphe)
    // - Token contient de la ponctuation forte (. ! ?)
    if (tokenBuffer.length >= BATCH_SIZE || 
        token.includes('\n') || 
        /[.!?]/.test(token)) {
        flushTokenBuffer();
        return;
    }
    
    // Flush diff√©r√© pour optimiser les petits tokens
    if (bufferFlushTimer) clearTimeout(bufferFlushTimer);
    bufferFlushTimer = setTimeout(flushTokenBuffer, BATCH_TIMEOUT);
}

/**
 * üöÄ OPTIMIS√â - Applique le buffer accumul√© au DOM (batch update)
 */
function flushTokenBuffer() {
    if (!tokenBuffer) return;
    
    const currentResponse = document.getElementById('current-response');
    if (!currentResponse) {
        resetTokenBuffer();
        return;
    }
    
    const content = currentResponse.querySelector('.message-content');
    if (content) {
        // Mise √† jour DOM en une seule fois (√©vite multiple reflows)
        content.textContent += tokenBuffer;
        
        // Scroll intelligent - seulement si l'utilisateur suit la conversation
        smartScrollToBottom();
    }
    
    // Reset buffer
    resetTokenBuffer();
}

/**
 * üöÄ OPTIMIS√â - Scroll intelligent qui √©vite les interruptions utilisateur
 */
function smartScrollToBottom() {
    const container = document.getElementById('dialogue-container');
    if (!container) return;
    
    // V√©rifier si l'utilisateur est proche du bas (suit la conversation)
    const isNearBottom = container.scrollTop >= 
        container.scrollHeight - container.clientHeight - SCROLL_THRESHOLD;
    
    // Scroller seulement si l'utilisateur suit activement
    if (isNearBottom) {
        container.scrollTop = container.scrollHeight;
    }
}

/**
 * Reset du buffer de tokens
 */
function resetTokenBuffer() {
    tokenBuffer = '';
    if (bufferFlushTimer) {
        clearTimeout(bufferFlushTimer);
        bufferFlushTimer = null;
    }
}

/**
 * Finalise la r√©ponse actuelle
 */
function finishCurrentResponse() {
    // Flush final du buffer pour √©viter les tokens perdus
    flushTokenBuffer();
    
    const currentResponse = document.getElementById('current-response');
    if (!currentResponse) return;
    
    currentResponse.removeAttribute('id');
    addTimeStamp(currentResponse);
    scrollToBottom(); // Scroll final pour s'assurer de la visibilit√© compl√®te
}

/**
 * Cr√©e une bulle de message
 * @param {string} type - Type de message (user, assistant, system)
 * @param {string} content - Contenu du message
 * @returns {HTMLElement} √âl√©ment de message
 */
function createMessageBubble(type, content) {
    const messageDiv = document.createElement('div');
    messageDiv.className = `message-bubble ${type}`;
    
    const contentDiv = document.createElement('div');
    contentDiv.className = 'message-content';
    contentDiv.textContent = content;
    
    messageDiv.appendChild(contentDiv);
    
    return messageDiv;
}

/**
 * Ajoute un timestamp √† un message
 * @param {HTMLElement} messageDiv - √âl√©ment de message
 */
function addTimeStamp(messageDiv) {
    const timeDiv = document.createElement('div');
    timeDiv.className = 'message-time';
    timeDiv.textContent = new Date().toLocaleTimeString();
    messageDiv.appendChild(timeDiv);
}

/**
 * Fait d√©filer la conversation vers le bas (fallback classique)
 */
function scrollToBottom() {
    const container = document.getElementById('dialogue-container');
    if (container) {
        container.scrollTop = container.scrollHeight;
    }
}

/**
 * Efface la conversation
 */
function clearConversation() {
    // Reset du buffer avant effacement
    resetTokenBuffer();
    
    if (!confirm('Effacer toute la conversation ?')) return;
    
    const container = document.getElementById('dialogue-container');
    if (!container) return;
    
    container.innerHTML = `
        <div class="welcome-message">
            <div class="message-bubble system">
                <div class="message-content">
                    <p>üóëÔ∏è Conversation effac√©e</p>
                </div>
            </div>
        </div>
    `;
    
    // R√©initialiser les statistiques
    resetStats();
    updateStatsDisplay();
    
    addLogEntry('üóëÔ∏è Conversation effac√©e', 'info');
}

/**
 * Exporte la conversation en fichier texte
 */
function exportConversation() {
    // Flush final avant export pour s'assurer que tout est visible
    flushTokenBuffer();
    
    const messages = document.querySelectorAll('.message-bubble:not(.system)');
    if (messages.length === 0) {
        addLogEntry('‚ö†Ô∏è Aucun message √† exporter', 'warning');
        return;
    }
    
    let exportText = `Conversation Jarvis - ${new Date().toLocaleString()}\n`;
    exportText += `================================\n\n`;
    
    messages.forEach(msg => {
        const type = msg.classList.contains('user') ? 'Vous' : 'Assistant';
        const content = msg.querySelector('.message-content')?.textContent || '';
        const time = msg.querySelector('.message-time')?.textContent || '';
        
        exportText += `[${time}] ${type}:\n${content}\n\n`;
    });
    
    exportText += `\nStatistiques de session:\n`;
    exportText += `- Messages: ${stats.messages}\n`;
    exportText += `- Tokens: ${stats.tokens}\n`;
    exportText += `- Temps moyen: ${stats.totalTime.toFixed(1)}s\n`;
    
    // Cr√©er et t√©l√©charger le fichier
    const blob = new Blob([exportText], { type: 'text/plain;charset=utf-8' });
    const url = URL.createObjectURL(blob);
    
    const a = document.createElement('a');
    a.href = url;
    a.download = `conversation-jarvis-${new Date().toISOString().split('T')[0]}.txt`;
    a.style.display = 'none';
    
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    
    URL.revokeObjectURL(url);
    
    addLogEntry('üíæ Conversation export√©e', 'success');
}

/**
 * Met √† jour la personnalit√© affich√©e
 * @param {string} personalityDisplay - Nom d'affichage de la personnalit√©
 */
function updatePersonality(personalityDisplay) {
    // Extraire le nom depuis "Assistant virtuel - Nom"
    const name = personalityDisplay.replace('Assistant virtuel - ', '');
    
    // Mettre √† jour le titre de la page
    const assistantName = document.getElementById('assistant-name');
    if (assistantName) {
        assistantName.textContent = personalityDisplay;
    }
    
    // Mettre √† jour l'affichage de config
    const configElement = document.getElementById('config-personality');
    if (configElement) {
        configElement.textContent = personalityDisplay;
    }
    
    // Mettre √† jour le titre de la page
    document.title = `${name} - Assistant Vocal`;
    
    addLogEntry(`üë§ Assistant mis √† jour: ${name}`, 'info');
}

/**
 * D√©finit la valeur de l'input de message
 * @param {string} value - Valeur √† d√©finir
 */
function setInputValue(value) {
    const input = document.getElementById('message-input');
    if (input) {
        input.value = value;
    }
}

/**
 * G√®re les raccourcis clavier pour l'input
 * @param {KeyboardEvent} event - √âv√©nement clavier
 */
function handleInputKeydown(event) {
    if (event.ctrlKey && event.key === 'Enter') {
        event.preventDefault();
        sendMessage();
    } 
}

/**
 * Auto-resize du textarea
 * @param {HTMLTextAreaElement} textarea - √âl√©ment textarea
 */
function autoResizeTextarea(textarea) {
    textarea.style.height = 'auto';
    textarea.style.height = textarea.scrollHeight + 'px';
}

/**
 * Initialise les √©v√©nements des messages
 */
function initializeMessageEvents() {
    // Bouton d'envoi
    //const sendButton = document.getElementById('send-button');
    //if (sendButton) {
    //    sendButton.addEventListener('click', sendMessage);
    //}
    
    // Bouton microphone
    const micButton = document.getElementById('mic-button');
    if (micButton) {
        micButton.addEventListener('click', toggleVoiceInput);
    }
    
    // Input de message
    const messageInput = document.getElementById('message-input');
    if (messageInput) {
        messageInput.addEventListener('keydown', handleInputKeydown);
        
        // Auto-resize du textarea
        messageInput.addEventListener('input', function() {
            autoResizeTextarea(this);
        });
    }
    
    // Boutons de contr√¥le de conversation
    const clearButton = document.querySelector('[onclick="clearConversation()"]');
    if (clearButton) {
        clearButton.removeAttribute('onclick');
        clearButton.addEventListener('click', clearConversation);
    }
    
    const exportButton = document.querySelector('[onclick="exportConversation()"]');
    if (exportButton) {
        exportButton.removeAttribute('onclick');
        exportButton.addEventListener('click', exportConversation);
    }
}

// Initialiser les √©v√©nements d√®s que le DOM est pr√™t
document.addEventListener('DOMContentLoaded', initializeMessageEvents);

// Raccourcis clavier globaux
document.addEventListener('keydown', function(event) {
    // Microphone avec Ctrl+M
    if (event.ctrlKey && event.key === 'm') {
        event.preventDefault();
        toggleVoiceInput();
    }
});

// üöÄ Nettoyage automatique en cas de changement de page
window.addEventListener('beforeunload', function() {
    resetTokenBuffer();
});

==================================================
FICHIER: .\web_interface\js\settings-frontend.js
==================================================

/**
 * settings-frontend.js - Frontend unifi√© pour configuration
 * üéØ Toutes les op√©rations via REST
 */
class ConfigAPI {
    constructor() {
        this.baseUrl = '/api/config';
    }

    // === LECTURE ===
    async getFullConfig() {
        const response = await fetch(`${this.baseUrl}`);
        const data = await response.json();
        return data.success ? data.config : null;
    }

    async getVoiceConfig() {
        const response = await fetch(`${this.baseUrl}/voice`);
        const data = await response.json();
        return data.success ? data.voice_config : null;
    }

    async getInterfaceConfig() {
        const response = await fetch(`${this.baseUrl}/interface`);
        const data = await response.json();
        return data.success ? data.config : null;
    }

    // === √âCRITURE ===
    async updateConfig(updates) {
        const response = await fetch(`${this.baseUrl}`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(updates)
        });
        const data = await response.json();
        return data.success;
    }

    async updateVoice(personality, ttsModel, options = {}) {
        const response = await fetch(`${this.baseUrl}/voice`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({
                personality,
                tts_model: ttsModel,
                ...options
            })
        });
        const data = await response.json();
        return data.success;
    }

    async updateInterface(interfaceConfig) {
        const response = await fetch(`${this.baseUrl}/interface`, {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(interfaceConfig)
        });
        const data = await response.json();
        return data.success;
    }
}

// Instance globale
const configAPI = new ConfigAPI();

/**
 * üöÄ THEME MANAGER UNIFI√â - Plus de localStorage
 *//*
function setTheme(theme) {
    // 1. Appliquer imm√©diatement l'UI
    document.body.className = `theme-${theme}`;
    currentTheme = theme;
    updateThemeButton();
    
    // 2. Sauvegarder via API
    configAPI.updateInterface({ theme })
        .then(success => {
            if (success) {
                addLogEntry(`‚úÖ Th√®me sauvegard√©: ${theme}`, 'success');
            }
        })
        .catch(error => {
            addLogEntry(`‚ùå Erreur sauvegarde th√®me: ${error.message}`, 'error');
        });
}

function setBackground(backgroundPath) {
    const dialogueContainer = document.getElementById('dialogue-container');
    if (!dialogueContainer) return;
    
    const dialogueSection = dialogueContainer.closest('.dialogue-section');
    if (!dialogueSection) return;
    
    // 1. Appliquer imm√©diatement l'UI
    dialogueSection.style.removeProperty('--bg-image-url');
    dialogueSection.classList.remove('bg-image');
    
    if (backgroundPath && backgroundPath !== 'default') {
        let imagePath = backgroundPath.startsWith('images/') ? 
            `static/${backgroundPath}` : `static/images/${backgroundPath}`;
        
        dialogueSection.style.setProperty('--bg-image-url', `url('${imagePath}')`);
        dialogueSection.classList.add('bg-image');
    }
    
    // 2. Sauvegarder via API
    configAPI.updateInterface({ background: backgroundPath })
        .then(success => {
            if (success) {
                addLogEntry(`‚úÖ Background sauvegard√©: ${backgroundPath}`, 'success');
            }
        })
        .catch(error => {
            addLogEntry(`‚ùå Erreur sauvegarde background: ${error.message}`, 'error');
        });
}

function setBackgroundOpacity(opacity) {
    // 1. Appliquer imm√©diatement l'UI
    const style = document.createElement('style');
    style.id = 'background-opacity-override';
    
    const existing = document.getElementById('background-opacity-override');
    if (existing) existing.remove();
    
    style.textContent = `
        .dialogue-section.bg-image::before {
            content: '';
            position: absolute;
            top: 0; left: 0; right: 0; bottom: 0;
            background-image: var(--bg-image-url);
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            opacity: ${opacity / 100};
            z-index: 1;
            pointer-events: none;
            border-radius: inherit;
        }
    `;
    
    document.head.appendChild(style);
    
    // 2. Sauvegarder via API
    configAPI.updateInterface({ background_opacity: opacity })
        .then(success => {
            if (success) {
                addLogEntry(`‚úÖ Opacit√© sauvegard√©e: ${opacity}%`, 'success');
            }
        })
        .catch(error => {
            addLogEntry(`‚ùå Erreur sauvegarde opacit√©: ${error.message}`, 'error');
        });
}
*/
/**
 * üöÄ SETTINGS MODAL UNIFI√â
 */
async function saveSettings() {
    try {
        addLogEntry('üíæ Sauvegarde unifi√©e...', 'info');
        
        // Collecter TOUTES les valeurs depuis l'interface
        const updates = {};
        
        // Voix
        const personality = document.getElementById('voice-personality')?.value;
        if (personality) {
            updates.voice = { personality };
        }
        
        // Audio
        const voiceSpeed = document.getElementById('voice-speed')?.value;
        const voiceVolume = document.getElementById('voice-volume')?.value;
        if (voiceSpeed || voiceVolume) {
            updates.audio = { output: {} };
            if (voiceSpeed) updates.audio.output.speed = parseFloat(voiceSpeed);
            if (voiceVolume) updates.audio.output.volume = parseInt(voiceVolume);
        }
        
        // LLM
        const llmModel = document.getElementById('llm-model')?.value;
        const llmTemperature = document.getElementById('llm-temperature')?.value;
        const role = document.getElementById('role-select')?.value;
        if (llmModel || llmTemperature || role) {
            updates.llm = {};
            if (llmModel) updates.llm.model = llmModel;
            if (llmTemperature) updates.llm.temperature = parseFloat(llmTemperature);
            if (role) updates.llm.role = role;
        }
        
        // Interface
        const theme = document.getElementById('interface-theme')?.value;
        const background = document.getElementById('interface-background')?.value;
        const backgroundOpacity = document.getElementById('background-opacity')?.value;
        if (theme || background || backgroundOpacity) {
            updates.interface = {};
            if (theme) updates.interface.theme = theme;
            if (background) updates.interface.background = background;
            if (backgroundOpacity) updates.interface.background_opacity = parseInt(backgroundOpacity);
        }
        
        console.log('üîç [DEBUG] Updates unifi√©s:', updates);
        
        // üöÄ UNE SEULE REQU√äTE pour tout sauvegarder
        const success = await configAPI.updateConfig(updates);
        
        if (success) {
            addLogEntry('‚úÖ Configuration sauvegard√©e', 'success');
            closeSettings();
        } else {
            addLogEntry('‚ùå Erreur sauvegarde', 'error');
        }
        
    } catch (error) {
        console.error('‚ùå Erreur sauvegarde unifi√©e:', error);
        addLogEntry('‚ùå Erreur: ' + error.message, 'error');
    }
}

/**
 * üöÄ CHARGEMENT UNIFI√â des param√®tres
 */
async function loadSettings() {
    try {
        addLogEntry('üìÑ Chargement configuration...', 'info');
        
        // üöÄ UNE SEULE REQU√äTE pour tout charger
        const config = await configAPI.getFullConfig();
        
        if (!config) {
            addLogEntry('‚ùå Erreur chargement config', 'error');
            return;
        }
        
        console.log('üîç [DEBUG] Config charg√©e:', config);
        
        // Appliquer aux contr√¥les
        applyConfigToUI(config);
        
        addLogEntry('‚úÖ Configuration charg√©e', 'success');
        
    } catch (error) {
        console.error('‚ùå Erreur chargement:', error);
        addLogEntry('‚ùå Erreur: ' + error.message, 'error');
    }
}

function applyConfigToUI(config) {
    // Voix
    if (config.voice?.personality) {
        const voiceSelect = document.getElementById('voice-personality');
        if (voiceSelect) voiceSelect.value = config.voice.personality;
    }
    
    // Audio
    if (config.audio?.output) {
        const speedSlider = document.getElementById('voice-speed');
        const volumeSlider = document.getElementById('voice-volume');
        
        if (speedSlider && config.audio.output.speed !== undefined) {
            speedSlider.value = config.audio.output.speed;
            const speedValue = document.getElementById('voice-speed-value');
            if (speedValue) speedValue.textContent = config.audio.output.speed + 'x';
        }
        
        if (volumeSlider && config.audio.output.volume !== undefined) {
            volumeSlider.value = config.audio.output.volume;
            const volumeValue = document.getElementById('voice-volume-value');
            if (volumeValue) volumeValue.textContent = config.audio.output.volume + '%';
        }
    }
    
    // LLM
    if (config.llm) {
        const modelSelect = document.getElementById('llm-model');
        const tempSlider = document.getElementById('llm-temperature');
        const roleSelect = document.getElementById('role-select');
        
        if (modelSelect && config.llm.model) {
            modelSelect.value = config.llm.model;
        }
        
        if (tempSlider && config.llm.temperature !== undefined) {
            tempSlider.value = config.llm.temperature;
            const tempValue = document.getElementById('llm-temperature-value');
            if (tempValue) tempValue.textContent = config.llm.temperature + '';
        }
        
        if (roleSelect && config.llm.role) {
            roleSelect.value = config.llm.role;
        }
    }
    
    // Interface
    if (config.interface) {
        const themeSelect = document.getElementById('interface-theme');
        const backgroundSelect = document.getElementById('interface-background');
        const opacitySlider = document.getElementById('background-opacity');
        
        if (themeSelect && config.interface.theme) {
            themeSelect.value = config.interface.theme;
            // Appliquer aussi au th√®me actuel
            currentTheme = config.interface.theme;
            document.body.className = `theme-${config.interface.theme}`;
        }
        
        if (backgroundSelect && config.interface.background) {
            backgroundSelect.value = config.interface.background;
        }
        
        if (opacitySlider && config.interface.background_opacity !== undefined) {
            opacitySlider.value = config.interface.background_opacity;
            const opacityValue = document.getElementById('background-opacity-value');
            if (opacityValue) opacityValue.textContent = config.interface.background_opacity + '%';
        }
    }
}

/**
 * üöÄ INITIALISATION au d√©marrage
 */
async function initializeConfig() {
    try {
        // Charger et appliquer la config au d√©marrage
        await loadSettings();
        addLogEntry('‚úÖ Configuration unifi√©e initialis√©e', 'success');
    } catch (error) {
        addLogEntry('‚ùå Erreur init config unifi√©e: ' + error.message, 'error');
    }
}

// Export des fonctions
if (typeof module !== 'undefined' && module.exports) {
    module.exports = {
        configAPI,
        setTheme,
        setBackground,
        setBackgroundOpacity,
        saveSettings,
        loadSettings,
        initializeConfig
    };
}

console.log('üéØ Frontend config unifi√© charg√©');


==================================================
FICHIER: .\web_interface\js\settings-modal.js
==================================================

/**
 * settings-modal.js - Gestion des param√®tres et modales
 * ‚öôÔ∏è Hypothalamus - Centre de contr√¥le et configuration
 * üöÄ OPTIMIS√â: Debounce + preview temps r√©el pour changements visuels
 */

// üöÄ Variables d'optimisation debounce
let previewTimer = null;
let batchedChanges = {};
let lastAppliedValues = {};
const PREVIEW_DEBOUNCE_DELAY = 250;      // 250ms pour preview fluide
const LOCALSTORAGE_DEBOUNCE_DELAY = 500; // 500ms pour localStorage

// Configuration des champs avec preview temps r√©el
const VISUAL_FIELDS = {
    'interface-background': { 
        key: 'background', 
        handler: 'setBackground',
        immediate: false
    },
    'background-opacity': { 
        key: 'background_opacity', 
        handler: 'setBackgroundOpacity',
        immediate: true   // Immediate pour feedback fluide opacit√©
    }
};

/**
 * Ouvre la modal des param√®tres
 */
function openSettings() {
    const modal = document.getElementById('settings-modal');
    if (modal) {
        modal.classList.add('show');
        interfaceState.settingsModalOpen = true;
        
        // Reset des valeurs de preview
        resetPreviewState();
        
        // Focus sur le premier √©l√©ment
        const firstInput = modal.querySelector('input, select');
        if (firstInput) {
            setTimeout(() => firstInput.focus(), 100);
        }
        
        addLogEntry('‚öôÔ∏è Param√®tres ouverts', 'info');
    }
}

/**
 * Ferme la modal des param√®tres
 */
function closeSettings() {
    // Flush final des changements en attente
    flushBatchedChanges();
    
    const modal = document.getElementById('settings-modal');
    if (modal) {
        modal.classList.remove('show');
        interfaceState.settingsModalOpen = false;
        addLogEntry('‚öôÔ∏è Param√®tres ferm√©s', 'info');
    }
}

/**
 * Ouvre la modal d'aide
 */
function showHelp() {
    const modal = document.getElementById('help-modal');
    if (modal) {
        modal.classList.add('show');
        interfaceState.helpModalOpen = true;
        addLogEntry('‚ùì Aide ouverte', 'info');
    }
}

/**
 * Ferme la modal d'aide
 */
function closeHelp() {
    const modal = document.getElementById('help-modal');
    if (modal) {
        modal.classList.remove('show');
        interfaceState.helpModalOpen = false;
        addLogEntry('‚ùì Aide ferm√©e', 'info');
    }
}

/**
 * Change d'onglet dans les param√®tres
 * @param {string} tabName - Nom de l'onglet
 * @param {Event} event - √âv√©nement du clic
 */
function switchSettingsTab(tabName, event) {
    // Masquer tous les onglets
    document.querySelectorAll('.settings-tab').forEach(tab => {
        tab.classList.remove('active');
    });
    
    // D√©sactiver tous les boutons
    document.querySelectorAll('.settings-tabs .tab-btn').forEach(btn => {
        btn.classList.remove('active');
    });
    
    // Activer l'onglet et le bouton s√©lectionn√©s
    const targetTab = document.getElementById(`settings-${tabName}`);
    if (targetTab) {
        targetTab.classList.add('active');
        interfaceState.currentSettingsTab = tabName;
    }
    
    if (event && event.target) {
        event.target.classList.add('active');
    }
    
    addLogEntry(`‚öôÔ∏è Onglet param√®tres: ${tabName}`, 'info');
}

/**
 * üöÄ OPTIMIS√â - Initialise les sliders avec preview temps r√©el debounced
 */
function initializeSliders() {
    const sliders = [
        { id: 'voice-speed', valueId: 'voice-speed-value', suffix: 'x', type: 'audio' },
        { id: 'voice-volume', valueId: 'voice-volume-value', suffix: '%', type: 'audio' },
        { id: 'audio-sensitivity', valueId: 'audio-sensitivity-value', suffix: '', type: 'audio' },
        { id: 'llm-temperature', valueId: 'llm-temperature-value', suffix: '', type: 'model' },
        { id: 'background-opacity', valueId: 'background-opacity-value', suffix: '%', type: 'visual' }
    ];
    
    sliders.forEach(slider => {
        const element = document.getElementById(slider.id);
        const valueElement = document.getElementById(slider.valueId);
        
        if (element && valueElement) {
            // üöÄ Event listener optimis√© avec preview
            element.addEventListener('input', function() {
                // Mise √† jour imm√©diate de l'affichage (pas de debounce sur l'UI)
                valueElement.textContent = this.value + slider.suffix;
                
                // Preview temps r√©el pour les champs visuels
                if (slider.type === 'visual') {
                    triggerVisualPreview(this.id, this.value);
                }
            });
            
            // Initialiser la valeur affich√©e
            valueElement.textContent = element.value + slider.suffix;
        }
    });
    
    // Event listeners pour les selects visuels
    Object.keys(VISUAL_FIELDS).forEach(fieldId => {
        const element = document.getElementById(fieldId);
        if (element) {
            element.addEventListener('change', function() {
                triggerVisualPreview(this.id, this.value);
            });
        }
    });
}

/**
 * üöÄ D√©clenche une preview visuelle avec debounce intelligent
 */
function triggerVisualPreview(elementId, value) {
    const field = VISUAL_FIELDS[elementId];
    if (!field) return;
    
    // Ajouter aux changements en batch
    batchedChanges[field.key] = value;
    
    // Application imm√©diate ou debounced selon le type
    if (field.immediate) {
        applyVisualChange(field.key, value, field.handler);
    } else {
        // Debounce pour √©viter les changements rapides
        if (previewTimer) clearTimeout(previewTimer);
        previewTimer = setTimeout(() => {
            flushVisualPreview();
        }, PREVIEW_DEBOUNCE_DELAY);
    }
}

/**
 * üöÄ Applique les changements visuels en batch (debounced)
 */
function flushVisualPreview() {
    Object.entries(batchedChanges).forEach(([key, value]) => {
        const field = Object.values(VISUAL_FIELDS).find(f => f.key === key);
        if (field && !field.immediate) {
            applyVisualChange(key, value, field.handler);
        }
    });
    
    // Sauvegarder en localStorage avec debounce
    debouncedLocalStorageSave();
}

/**
 * üöÄ Applique un changement visuel individuel (optimis√©)
 */
function applyVisualChange(key, value, handlerName) {
    // √âviter les applications redondantes
    if (lastAppliedValues[key] === value) return;
    lastAppliedValues[key] = value;
    
    try {
        switch(handlerName) {
            case 'setTheme':
                if (typeof setTheme === 'function') setTheme(value);
                break;
            case 'setBackground':
                if (typeof setBackground === 'function') setBackground(value);
                break;
            case 'setBackgroundOpacity':
                if (typeof setBackgroundOpacity === 'function') setBackgroundOpacity(value);
                break;
        }
    } catch (error) {
        console.warn(`Erreur application preview ${handlerName}:`, error);
    }
}

/**
 * üöÄ Sauvegarde localStorage debounced
 */
let localStorageTimer = null;
function debouncedLocalStorageSave() {
    if (localStorageTimer) clearTimeout(localStorageTimer);
    
    localStorageTimer = setTimeout(() => {
        try {
            const savedSettings = JSON.parse(localStorage.getItem('jarvis-settings') || '{}');
            Object.assign(savedSettings, batchedChanges);
            localStorage.setItem('jarvis-settings', JSON.stringify(savedSettings));
        } catch (error) {
            console.warn('Erreur sauvegarde localStorage:', error);
        }
    }, LOCALSTORAGE_DEBOUNCE_DELAY);
}

/**
 * üöÄ Flush final des changements en attente
 */
function flushBatchedChanges() {
    if (previewTimer) {
        clearTimeout(previewTimer);
        flushVisualPreview();
    }
    
    if (localStorageTimer) {
        clearTimeout(localStorageTimer);
        debouncedLocalStorageSave();
    }
    
    batchedChanges = {};
}

/**
 * üöÄ Reset l'√©tat de preview
 */
function resetPreviewState() {
    if (previewTimer) clearTimeout(previewTimer);
    if (localStorageTimer) clearTimeout(localStorageTimer);
    
    batchedChanges = {};
    lastAppliedValues = {};
}

/**
 * G√®re les √©v√©nements clavier des modales
 * @param {KeyboardEvent} event 
 */
function handleModalKeydown(event) {
    if (event.key === 'Escape') {
        if (interfaceState.settingsModalOpen) {
            closeSettings();
        } else if (interfaceState.helpModalOpen) {
            closeHelp();
        }
    }
}

/**
 * Initialise les √©v√©nements des param√®tres
 */
function initializeSettingsEvents() {
    // Boutons de modal
    const settingsBtn = document.getElementById('settings-btn');
    if (settingsBtn) {
        settingsBtn.addEventListener('click', openSettings);
    }
    
    const helpBtn = document.getElementById('help-btn');
    if (helpBtn) {
        helpBtn.addEventListener('click', showHelp);
    }
    
    // Boutons de fermeture
    const closeButtons = document.querySelectorAll('.modal-close, .close-settings, .close-help');
    closeButtons.forEach(btn => {
        btn.addEventListener('click', function() {
            const modal = this.closest('.modal');
            if (modal && modal.id === 'settings-modal') {
                closeSettings();
            } else if (modal && modal.id === 'help-modal') {
                closeHelp();
            }
        });
    });
    
    // Bouton de sauvegarde
    const saveBtn = document.getElementById('save-settings');
    if (saveBtn) {
        saveBtn.addEventListener('click', saveSettingsFromModal);
    }
    
    // Bouton de reset
    const resetBtn = document.getElementById('reset-settings');
    if (resetBtn) {
        resetBtn.addEventListener('click', resetSettings);
    }
    
    // Fermeture en cliquant sur l'overlay
    document.querySelectorAll('.modal').forEach(modal => {
        modal.addEventListener('click', function(e) {
            if (e.target === this) {
                if (this.id === 'settings-modal') {
                    closeSettings();
                } else if (this.id === 'help-modal') {
                    closeHelp();
                }
            }
        });
    });
    
    // Gestion des touches
    document.addEventListener('keydown', handleModalKeydown);
    
    // Initialiser les sliders
    initializeSliders();
}

async function saveSettingsFromModal() {
    try {
        addLogEntry('üíæ Sauvegarde s√©lective...', 'info');
        
        // Flush final des previews en attente
        flushBatchedChanges();
        
        // ‚úÖ R√©cup√©rer les nouvelles valeurs de la modal
        const currentSettings = await getCurrentServerConfig();
        const newSettings = JSON.parse(JSON.stringify(currentSettings));
        
        // Configuration VOIX (voice)
        newSettings.voice.personality = document.getElementById('voice-personality')?.value;
        // La vitesse et le volume sont dans audio.output
        newSettings.audio.output.speed = parseFloat(document.getElementById('voice-speed')?.value || 1.0);
        newSettings.audio.output.volume = parseInt(document.getElementById('voice-volume')?.value || 90);
        
        // Configuration LLM
        newSettings.llm.model = document.getElementById('llm-model')?.value;
        newSettings.llm.temperature = parseFloat(document.getElementById('llm-temperature')?.value || 0.7);
        
        // Configuration INTERFACE (interface)
        newSettings.interface.theme = document.getElementById('interface-theme')?.value;
        newSettings.interface.background = document.getElementById('interface-background')?.value;
        newSettings.interface.background_opacity = parseInt(document.getElementById('background-opacity')?.value || 30);
        
        // Configuration AUDIO INPUT (audio.input)
        newSettings.audio.input.sensitivity = parseInt(document.getElementById('audio-sensitivity')?.value || 5);
        
        const changes = getChangedSettings(currentSettings, newSettings);
        console.log('üîç [DEBUG] Changements d√©tect√©s:', changes);
        
        if (Object.keys(changes).length === 0) {
            addLogEntry('‚ÑπÔ∏è Aucun changement d√©tect√©', 'info');
            closeSettings();
            return;
        }
        
        console.log('üö® [DEBUG] ENVOI WebSocket:', {
            type: 'config_update',
            config: changes
        });
        // ‚úÖ Cat√©goriser les changements
        const lightChanges = {}; // Interface, theme, background - application imm√©diate
        const heavyChanges = {}; // Voice, LLM, STT - n√©cessitent rechargement modules
        
        Object.entries(changes).forEach(([key, value]) => {
            if (['theme', 'background', 'background_opacity', 'voice_speed', 'voice_volume'].includes(key)) {
                lightChanges[key] = value;
            } else {
                heavyChanges[key] = value;
            }
        });
        
        // ‚úÖ Appliquer les changements l√©gers IMM√âDIATEMENT (optimis√©)
        if (Object.keys(lightChanges).length > 0) {
            applyLightChanges(lightChanges);
        
            addLogEntry(`‚úÖ Changements visuels appliqu√©s`, 'success');
        }
        
        // ‚úÖ Sauvegarder TOUS les changements sur le serveur
        const allChangesToSave = {...lightChanges, ...heavyChanges};
        
        if (Object.keys(allChangesToSave).length > 0) {
        
            // Validation
            if (allChangesToSave.personality && !allChangesToSave.personality.trim()) {
                throw new Error('Personnalit√© requise');
            }
            
            // Envoyer au serveur via WebSocket
            if (!isConnected) {
                throw new Error('Connexion WebSocket requise');
            }
            
            // Cr√©er une promesse pour attendre la r√©ponse
            const success = await new Promise((resolve) => {
                const timeout = setTimeout(() => resolve(false), 5000);
                
                const handleResponse = (event) => {
                    try {
                        const data = JSON.parse(event.data);
                        if (data.type === 'config_updated') {
                            clearTimeout(timeout);
                            ws.removeEventListener('message', handleResponse);
                            resolve(data.success === true);
                        }
                    } catch (e) {
                        // Ignorer les autres messages
                    }
                };
                
                ws.addEventListener('message', handleResponse);
                
                console.log('üö® ENVOI WebSocket:', {
                    type: 'config_update',
                    config: allChangesToSave
                });
                
                const sent = sendWebSocketMessage({
                    type: 'config_update',
                    config: allChangesToSave
                });
                
                if (!sent) {
                    clearTimeout(timeout);
                    resolve(false);
                }
            });
            
            if (success) {
                addLogEntry(`‚úÖ Param√®tres sauvegard√©s (${Object.keys(allChangesToSave).length} changements)`, 'success');
            } else {
                throw new Error('Timeout ou erreur serveur');
            }
        }
        
        closeSettings();
        
    } catch (error) {
        console.error('Erreur sauvegarde:', error);
        addLogEntry('‚ùå Erreur sauvegarde: ' + error.message, 'error');
    }
}

/**
 * Reset des param√®tres
 */
async function resetSettings() {
    if (!confirm('Remettre tous les param√®tres par d√©faut ?')) return;
    
    // Reset preview state
    resetPreviewState();
    
    try {
        // Valeurs par d√©faut
        const defaultSettings = {
            personality: 'Jarvis',
            voice_speed: 1.0,
            voice_volume: 90,
            llm_model: 'qwen2.5:7b',
            llm_temperature: 0.7,
            theme: 'dark',
            background: 'none',
            background_opacity: 30,
            audio_sensitivity: 5
        };
        
        // Mettre √† jour l'interface imm√©diatement
        Object.entries(defaultSettings).forEach(([key, value]) => {
            const element = document.getElementById(key === 'theme' ? 'interface-theme' : 
                                                 key === 'background' ? 'interface-background' :
                                                 key === 'llm_model' ? 'llm-model' :
                                                 key === 'personality' ? 'voice-personality' :
                                                 key.replace('_', '-'));
            if (element) {
                element.value = value;
            }
        });
        
        // Recharger pour mettre √† jour les sliders
        await populateVoiceSelect();
        
        // Appliquer les changements visuels
        applyLightChanges({
            theme: defaultSettings.theme,
            background: defaultSettings.background,
            background_opacity: defaultSettings.background_opacity
        });
        
        // Supprimer du localStorage
        localStorage.removeItem('jarvis-settings');
        
        addLogEntry('üîÑ Param√®tres remis √† z√©ro', 'info');
        
    } catch (error) {
        addLogEntry('‚ùå Erreur reset: ' + error.message, 'error');
    }
}

/**
 * üöÄ OPTIMIS√â - Applique les changements l√©gers c√¥t√© client (avec anti-redondance)
 */
function applyLightChanges(changes) {
    console.log('üé® applyLightChanges optimis√© avec:', changes);
    
    // √âviter les applications redondantes
    Object.entries(changes).forEach(([key, value]) => {
        if (lastAppliedValues[key] === value) {
            delete changes[key];
        } else {
            lastAppliedValues[key] = value;
        }
    });
    
    if (Object.keys(changes).length === 0) {
        console.log('‚ö° Tous changements d√©j√† appliqu√©s - skip');
        return;
    }
    
    if (changes.theme) {
        console.log('üé® Application th√®me:', changes.theme);
        setTheme(changes.theme);
    }
    
    if (changes.background) {
        console.log('üñºÔ∏è Application background:', changes.background);
        setBackground(changes.background);
    }
    
    if (changes.background_opacity !== undefined) {
        setBackgroundOpacity(changes.background_opacity);
    }
    
    // Sauvegarder dans localStorage (sans debounce car c'est la sauvegarde finale)
    const savedSettings = JSON.parse(localStorage.getItem('jarvis-settings') || '{}');
    Object.assign(savedSettings, changes);
    localStorage.setItem('jarvis-settings', JSON.stringify(savedSettings));
    
    console.log('‚úÖ applyLightChanges optimis√© termin√©');
}


/**
 * Compare deux objets (ancien et nouveau) et retourne un objet contenant 
 * uniquement les parties du nouvel objet qui ont chang√©.
 * @param {Object} current - La configuration actuelle (serveur).
 * @param {Object} newSettings - La nouvelle configuration (modale).
 * @returns {Object} Un objet contenant uniquement les changements.
 */
function getChangedSettings(current, newSettings) {
    const changes = {};

    for (const key in newSettings) {
        // Ignorer les propri√©t√©s h√©rit√©es
        if (!Object.prototype.hasOwnProperty.call(newSettings, key)) continue;

        const currentValue = current[key];
        const newValue = newSettings[key];

        // 1. Si les deux sont des objets (mais pas null ou Array) -> Comparaison r√©cursive
        if (typeof newValue === 'object' && newValue !== null && !Array.isArray(newValue)) {
            
            // S'assurer que la cl√© existe dans l'objet actuel pour √©viter une erreur
            if (typeof currentValue !== 'object' || currentValue === null || Array.isArray(currentValue)) {
                 // Si la structure a chang√© (ex: object devient string), on consid√®re √ßa comme un changement complet
                 changes[key] = newValue;
                 continue;
            }

            // Appel r√©cursif pour d√©tecter les changements dans l'objet imbriqu√©
            const nestedChanges = getChangedSettings(currentValue, newValue);
            
            // Si des changements sont trouv√©s dans l'objet imbriqu√©,
            // on ajoute l'objet *complet* au bloc de changement.
            if (Object.keys(nestedChanges).length > 0) {
                changes[key] = newValue;
            }
            
        } 
        // 2. Si c'est une valeur primitive (string, number, boolean) ou Array/null -> Comparaison directe
        else if (currentValue !== newValue) {
            
            // üêõ Gestion des nombres √† virgule flottante (pour llm_temperature) :
            // Parfois, la conversion en float peut laisser des artefacts (ex: 0.7000000000000001)
            // On peut arrondir l√©g√®rement pour une comparaison fiable.
            let isNumeric = typeof currentValue === 'number' && typeof newValue === 'number';
            
            if (isNumeric && Math.abs(currentValue - newValue) < 0.00001) {
                // Les nombres sont "identiques" malgr√© une l√©g√®re diff√©rence binaire
                continue;
            }
            
            // C'est un changement r√©el de valeur (primitive ou Array/null)
            changes[key] = newValue;
        }
    }

    return changes;
}

/**
 * R√©cup√®re la config actuelle du serveur
 */
async function getCurrentServerConfig() {
    try {
        const response = await fetch('/api/config');
        const data = await response.json();
        return data;
    } catch (error) {
        console.error('Erreur r√©cup√©ration config serveur:', error);
        return {};
    }
}

// üöÄ Nettoyage automatique lors du d√©chargement de page
window.addEventListener('beforeunload', function() {
    flushBatchedChanges();
});

// Initialiser les √©v√©nements d√®s que le DOM est pr√™t
document.addEventListener('DOMContentLoaded', initializeSettingsEvents);

==================================================
FICHIER: .\web_interface\js\theme-manager.js
==================================================

/**
 * theme-manager.js - Gestion des th√®mes de l'interface
 * üé® Lobes Occipitaux - Traitement visuel et esth√©tique
 * üöÄ CORRIG√â: Background correct + opacit√© seulement sur l'image
 */

let themesConfig = null;

/**
 * Charge la configuration des th√®mes depuis themes.json
 */
async function loadThemesConfig() {
    try {
        const response = await fetch('config/themes.json');
        if (response.ok) {
            themesConfig = await response.json();
            addLogEntry('‚úÖ Configuration th√®mes charg√©e', 'success');
            return true;
        } else {
            throw new Error(`HTTP ${response.status}`);
        }
    } catch (error) {
        addLogEntry(`‚ùå Erreur chargement themes.json: ${error.message}`, 'error');
        return false;
    }
}

/**
 * Bascule vers le th√®me suivant dans le cycle
 */
function toggleTheme() {
    if (!themesConfig?.themes) {
        addLogEntry('Configuration des th√®mes non charg√©e', 'error');
        return;
    }
    
    // Utiliser l'ordre de cycle d√©fini ou les cl√©s par d√©faut
    const themes = themesConfig.config?.cycle_order || Object.keys(themesConfig.themes);
    const currentIndex = themes.indexOf(currentTheme);
    const nextIndex = (currentIndex + 1) % themes.length;
    
    addLogEntry(`üé® Passage du th√®me ${currentTheme} vers ${themes[nextIndex]}`, 'info');
    setTheme(themes[nextIndex]);
}

/**
 * Applique un th√®me sp√©cifique
 * @param {string} theme - ID du th√®me √† appliquer
 */
function setTheme(theme) {
    if (!themesConfig?.themes?.[theme]) {
        addLogEntry(`‚ùå Th√®me inconnu: ${theme}`, 'error');
        return;
    }
    
    // Mettre √† jour la variable globale
    currentTheme = theme;
    
    // Appliquer la classe CSS
    document.body.className = `theme-${theme}`;
    
    // Mettre √† jour le select des param√®tres si ouvert
    const themeSelect = document.getElementById('interface-theme');
    if (themeSelect) {
        themeSelect.value = theme;
    }
    
    // Mettre √† jour le bouton de navigation
    updateThemeButton();
    
    // Sauvegarder les pr√©f√©rences
    saveSettings();
    
    // Log du changement
    const themeConfig = themesConfig.themes[theme];
    addLogEntry(`‚úÖ Th√®me appliqu√©: ${themeConfig.current_name}`, 'success');
    
    // √âmettre un √©v√©nement personnalis√© pour les autres modules
    document.dispatchEvent(new CustomEvent('themeChanged', { 
        detail: { theme, config: themeConfig } 
    }));
}

/**
 * üöÄ CORRIG√â - Applique un arri√®re-plan avec stockage pour contr√¥le opacit√©
 * @param {string} backgroundPath - Chemin de l'arri√®re-plan
 */
function setBackground(backgroundPath) {
    console.log('üé® setBackground appel√© avec:', backgroundPath);
    
    // üöÄ FIX: S√©lectionner le bon dialogue-section (celui qui contient dialogue-container)
    const dialogueContainer = document.getElementById('dialogue-container');
    if (!dialogueContainer) {
        console.error('‚ùå dialogue-container introuvable');
        return;
    }
    
    const dialogueSection = dialogueContainer.closest('.dialogue-section');
    if (!dialogueSection) {
        console.error('‚ùå Zone dialogue parente introuvable');
        return;
    }
    
    console.log('‚úÖ Zone dialogue trouv√©e (bonne):', dialogueSection);
    
    // Nettoyer ancien arri√®re-plan
    dialogueSection.style.backgroundImage = '';
    dialogueSection.classList.remove('bg-image');
    dialogueSection.style.removeProperty('--bg-image-url');
    console.log('üßπ Ancien arri√®re-plan nettoy√©');
    
    if (backgroundPath && backgroundPath !== 'default') {
        // Construire le chemin
        let imagePath;
        if (backgroundPath.startsWith('images/')) {
            imagePath = `static/${backgroundPath}`;
        } else {
            imagePath = `static/images/${backgroundPath}`;
        }
        
        console.log('üñºÔ∏è Chemin image final:', imagePath);
        
        // üöÄ NOUVEAU: Stocker l'URL dans une CSS custom property
        dialogueSection.style.setProperty('--bg-image-url', `url('${imagePath}')`);
        dialogueSection.classList.add('bg-image');
        
        console.log('‚úÖ Styles CSS appliqu√©s avec custom property');
        
        // Mettre √† jour l'indicateur
        //updateBackgroundDisplay(backgroundPath, 'Image s√©lectionn√©e');
        
        addLogEntry(`üñºÔ∏è Arri√®re-plan dialogue: ${backgroundPath}`, 'info');
        localStorage.setItem('jarvis-background', backgroundPath);
        
    } else {
        console.log('üé® Arri√®re-plan par d√©faut');
        //updateBackgroundDisplay('default', 'Par d√©faut');
        addLogEntry('üé® Arri√®re-plan par d√©faut (dialogue)', 'info');
        localStorage.setItem('jarvis-background', 'default');
    }
}


/**
 * üöÄ CORRIG√â - Met √† jour la transparence de l'arri√®re-plan (utilise custom property)
 * @param {number} opacity - Opacit√© en pourcentage (10-100)
 */
function setBackgroundOpacity(opacity) {
    const style = document.createElement('style');
    style.id = 'background-opacity-override';
    
    const existing = document.getElementById('background-opacity-override');
    if (existing) existing.remove();
    
    // üöÄ FIX: Utilise la CSS custom property stock√©e par setBackground
    style.textContent = `
        .dialogue-section.bg-image::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-image: var(--bg-image-url);
            background-size: cover;
            background-position: center;
            background-repeat: no-repeat;
            opacity: ${opacity / 100};
            z-index: 1;
            pointer-events: none;
            border-radius: inherit;
        }
        
        .dialogue-section.bg-image .dialogue-header,
        .dialogue-section.bg-image .dialogue-container {
            position: relative;
            z-index: 2;
        }
    `;
    
    document.head.appendChild(style);
    localStorage.setItem('jarvis-background-opacity', opacity);
    
    console.log(`üé® Transparence background: ${opacity}%`);
}

/**
 * Charge l'arri√®re-plan sauvegard√© au d√©marrage (avec opacit√©)
 */
function loadSavedBackground() {
    const saved = localStorage.getItem('jarvis-background');
    if (saved && saved !== 'default') {
        setBackground(saved);
    }

    const savedopacity = localStorage.getItem('jarvis-background-opacity');
    if (savedopacity) {
        // Appliquer l'opacit√© apr√®s un court d√©lai pour s'assurer que setBackground est termin√©
        setTimeout(() => {
            setBackgroundOpacity(parseInt(savedopacity));
        }, 100);
        
        // Mettre √† jour le slider dans les param√®tres si ouvert
        const opacitySlider = document.getElementById('background-opacity');
        const opacityValue = document.getElementById('background-opacity-value');
        if (opacitySlider) {
            opacitySlider.value = savedopacity;
        }
        if (opacityValue) {
            opacityValue.textContent = savedopacity + '%';
        }
    }
}

/**
 * Met √† jour le bouton de th√®me dans la navigation
 */
function updateThemeButton() {
    const themeIcon = document.getElementById('theme-icon');
    const themeText = document.getElementById('theme-text');

    if (!themesConfig?.themes?.[currentTheme]) {
        addLogEntry('‚ö†Ô∏è Configuration th√®me manquante pour updateThemeButton', 'warning');
        return;
    }
    
    const themeConfig = themesConfig.themes[currentTheme];
    
    if (themeIcon && themeText) {
        // Afficher l'ic√¥ne et le nom du PROCHAIN th√®me
        themeIcon.textContent = themeConfig.next_icon || 'üé®';
        themeText.textContent = themeConfig.next_name || 'Changer th√®me';
        
        // Mettre √† jour le titre pour l'accessibilit√©
        const themeButton = themeText.closest('.nav-btn');
        if (themeButton) {
            themeButton.title = themeConfig.description || `Passer en ${themeConfig.next_name}`;
        }
    }
}

/**
 * Initialise le th√®me au d√©marrage de l'application
 * @param {string} defaultTheme - Th√®me par d√©faut si aucun n'est sauvegard√©
 */
function initializeTheme(defaultTheme = 'light') {
    // Charger le th√®me sauvegard√© ou utiliser le d√©faut
    const savedSettings = loadSavedSettings();
    const themeToApply = savedSettings?.theme || 
                        themesConfig?.config?.default_theme || 
                        defaultTheme;
    
    addLogEntry(`üé® Initialisation th√®me: ${themeToApply}`, 'info');
    setTheme(themeToApply);
}

/**
 * Applique un th√®me depuis les param√®tres
 * @param {string} theme - ID du th√®me s√©lectionn√©
 */
function applyThemeFromSettings(theme) {
    if (theme !== currentTheme) {
        addLogEntry(`üé® Changement th√®me depuis param√®tres: ${theme}`, 'info');
        setTheme(theme);
    }
}

/**
 * Retourne la configuration du th√®me actuel
 * @returns {Object|null} Configuration du th√®me actuel
 */
function getCurrentThemeConfig() {
    return themesConfig?.themes?.[currentTheme] || null;
}

/**
 * Retourne la liste des th√®mes disponibles
 * @returns {Array} Liste des th√®mes avec leurs informations
 */
function getAvailableThemes() {
    if (!themesConfig?.themes) return [];
    
    return Object.values(themesConfig.themes).map(theme => ({
        id: theme.id,
        name: theme.current_name,
        description: theme.description || `Th√®me ${theme.current_name}`
    }));
}

/**
 * V√©rifie si un th√®me existe
 * @param {string} themeId - ID du th√®me √† v√©rifier
 * @returns {boolean} True si le th√®me existe
 */
function themeExists(themeId) {
    return !!(themesConfig?.themes?.[themeId]);
}

/**
 * Applique un th√®me temporaire (pour pr√©visualisation)
 * @param {string} theme - ID du th√®me √† pr√©visualiser
 */
function previewTheme(theme) {
    if (!themeExists(theme)) return;
    
    // Sauvegarder le th√®me actuel
    const originalTheme = currentTheme;
    
    // Appliquer temporairement
    document.body.className = `theme-${theme}`;
    
    // Programmer le retour au th√®me original apr√®s 3 secondes
    setTimeout(() => {
        if (currentTheme === originalTheme) {
            document.body.className = `theme-${originalTheme}`;
        }
    }, 3000);
}

/**
 * Initialise les √©v√©nements li√©s aux th√®mes
 */
function initializeThemeEvents() {
    loadThemesConfig();
    // Gestion du changement depuis les param√®tres
    document.addEventListener('DOMContentLoaded', handleThemeSelectChange);
    
    // Gestion du raccourci clavier pour changer de th√®me
    document.addEventListener('keydown', function(event) {
        if (event.ctrlKey && event.key === 'f') {
            event.preventDefault();
            toggleTheme();
        }
    });
    
    // √âcouter les changements de pr√©f√©rences syst√®me (optionnel)
    if (window.matchMedia) {
        const mediaQuery = window.matchMedia('(prefers-color-scheme: dark)');
        mediaQuery.addEventListener('change', function(event) {
            // Optionnel: adapter automatiquement au th√®me syst√®me
            if (themesConfig?.config?.auto_follow_system) {
                const systemTheme = event.matches ? 'dark' : 'light';
                if (themeExists(systemTheme)) {
                    setTheme(systemTheme);
                }
            }
        });
    }
}

// Initialiser les √©v√©nements d√®s que le DOM est pr√™t
if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', initializeThemeEvents);
} else {
    initializeThemeEvents();
}


==================================================
FICHIER: .\web_interface\js\utils.js
==================================================

/**
 * utils.js - Fonctions utilitaires
 * üõ†Ô∏è Bo√Æte √† outils - Fonctions partag√©es et helpers
 */

/**
 * Sauvegarde les param√®tres dans le localStorage
 */
function saveSettings() {
    const settings = {
        theme: currentTheme,
        voiceVisible: voiceVisible,
        cameraVisible: cameraVisible,
        debugVisible: debugVisible,
        lastSave: new Date().toISOString()
    };
    
    try {
        localStorage.setItem('jarvis-settings', JSON.stringify(settings));
    } catch (error) {
        addLogEntry(`‚ö†Ô∏è Erreur sauvegarde param√®tres: ${error.message}`, 'warning');
    }
}

/**
 * Charge les param√®tres depuis le localStorage
 * @returns {Object|null} Param√®tres charg√©s ou null
 */
function loadSavedSettings() {
    try {
        const saved = localStorage.getItem('jarvis-settings');
        return saved ? JSON.parse(saved) : null;
    } catch (error) {
        addLogEntry(`‚ö†Ô∏è Erreur chargement param√®tres: ${error.message}`, 'warning');
        return null;
    }
}

/**
 * Charge les param√®tres sauvegard√©s et les applique
 */
function loadSettings() {
    const saved = localStorage.getItem('jarvis-settings');
    if (saved) {
        try {
            const settings = JSON.parse(saved);
            if (settings.theme) {
                setTheme(settings.theme);
            }
            voiceVisible = settings.voiceVisible || false;
            cameraVisible = settings.cameraVisible || false;
            debugVisible = settings.debugVisible || false;
        } catch (error) {
            addLogEntry(`‚ö†Ô∏è Erreur chargement param√®tres: ${error.message}`, 'warning');
        }
    }
    
    // Charger l'arri√®re-plan sauvegard√©
    loadSavedBackground();
}

/**
 * Met √† jour l'interface utilisateur globale
 */
function updateUI() {
    updateDebugVisibility();
    updateConnectionStatus();
    updateStatsDisplay();
    updateThemeButton();
    if (typeof initVoices === 'function') {
        initVoices();
    }
}

/**
 * Formatte une dur√©e en secondes en format lisible
 * @param {number} seconds - Dur√©e en secondes
 * @returns {string} Dur√©e format√©e
 */
function formatDuration(seconds) {
    if (seconds < 1) {
        return `${(seconds * 1000).toFixed(0)}ms`;
    } else if (seconds < 60) {
        return `${seconds.toFixed(1)}s`;
    } else {
        const minutes = Math.floor(seconds / 60);
        const remainingSeconds = (seconds % 60).toFixed(1);
        return `${minutes}m ${remainingSeconds}s`;
    }
}

/**
 * Formatte un nombre de tokens
 * @param {number} tokens - Nombre de tokens
 * @returns {string} Nombre format√©
 */
function formatTokenCount(tokens) {
    if (tokens < 1000) {
        return tokens.toString();
    } else if (tokens < 1000000) {
        return `${(tokens / 1000).toFixed(1)}K`;
    } else {
        return `${(tokens / 1000000).toFixed(1)}M`;
    }
}

/**
 * G√©n√®re un ID unique
 * @returns {string} ID unique
 */
function generateUniqueId() {
    return `jarvis-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
}

/**
 * D√©bounce une fonction
 * @param {Function} func - Fonction √† d√©bouncer
 * @param {number} wait - D√©lai en millisecondes
 * @returns {Function} Fonction debounc√©e
 */
function debounce(func, wait) {
    let timeout;
    return function executedFunction(...args) {
        const later = () => {
            clearTimeout(timeout);
            func(...args);
        };
        clearTimeout(timeout);
        timeout = setTimeout(later, wait);
    };
}

/**
 * Throttle une fonction
 * @param {Function} func - Fonction √† throttler
 * @param {number} limit - Limite en millisecondes
 * @returns {Function} Fonction throttl√©e
 */
function throttle(func, limit) {
    let inThrottle;
    return function(...args) {
        if (!inThrottle) {
            func.apply(this, args);
            inThrottle = true;
            setTimeout(() => inThrottle = false, limit);
        }
    };
}

/**
 * V√©rifie si un √©l√©ment est visible dans le viewport
 * @param {HTMLElement} element - √âl√©ment √† v√©rifier
 * @returns {boolean} True si visible
 */
function isElementVisible(element) {
    if (!element) return false;
    
    const rect = element.getBoundingClientRect();
    return (
        rect.top >= 0 &&
        rect.left >= 0 &&
        rect.bottom <= (window.innerHeight || document.documentElement.clientHeight) &&
        rect.right <= (window.innerWidth || document.documentElement.clientWidth)
    );
}

/**
 * Copie du texte dans le presse-papiers
 * @param {string} text - Texte √† copier
 * @returns {Promise<boolean>} Promise de succ√®s
 */
async function copyToClipboard(text) {
    try {
        if (navigator.clipboard && window.isSecureContext) {
            await navigator.clipboard.writeText(text);
            return true;
        } else {
            // Fallback pour les navigateurs plus anciens
            const textArea = document.createElement('textarea');
            textArea.value = text;
            textArea.style.position = 'fixed';
            textArea.style.left = '-999999px';
            textArea.style.top = '-999999px';
            document.body.appendChild(textArea);
            textArea.focus();
            textArea.select();
            
            const success = document.execCommand('copy');
            textArea.remove();
            return success;
        }
    } catch (error) {
        console.error('Erreur copie presse-papiers:', error);
        return false;
    }
}

/**
 * Valide une adresse email
 * @param {string} email - Email √† valider
 * @returns {boolean} True si valide
 */
function isValidEmail(email) {
    const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
    return emailRegex.test(email);
}

/**
 * Valide une URL
 * @param {string} url - URL √† valider
 * @returns {boolean} True si valide
 */
function isValidUrl(url) {
    try {
        new URL(url);
        return true;
    } catch {
        return false;
    }
}

/**
 * Nettoie une cha√Æne de caract√®res
 * @param {string} str - Cha√Æne √† nettoyer
 * @returns {string} Cha√Æne nettoy√©e
 */
function sanitizeString(str) {
    if (typeof str !== 'string') return '';
    
    return str
        .trim()
        .replace(/[<>]/g, '') // Supprimer les caract√®res HTML de base
        .replace(/\s+/g, ' '); // R√©duire les espaces multiples
}

/**
 * Formate une taille de fichier
 * @param {number} bytes - Taille en bytes
 * @returns {string} Taille format√©e
 */
function formatFileSize(bytes) {
    if (bytes === 0) return '0 B';
    
    const k = 1024;
    const sizes = ['B', 'KB', 'MB', 'GB', 'TB'];
    const i = Math.floor(Math.log(bytes) / Math.log(k));
    
    return `${parseFloat((bytes / Math.pow(k, i)).toFixed(2))} ${sizes[i]}`;
}

/**
 * Obtient les informations sur le navigateur
 * @returns {Object} Informations navigateur
 */
function getBrowserInfo() {
    const ua = navigator.userAgent;
    
    let browser = 'Unknown';
    let version = 'Unknown';
    
    if (ua.indexOf('Firefox') > -1) {
        browser = 'Firefox';
        version = ua.match(/Firefox\/(\d+)/)?.[1] || 'Unknown';
    } else if (ua.indexOf('Chrome') > -1) {
        browser = 'Chrome';
        version = ua.match(/Chrome\/(\d+)/)?.[1] || 'Unknown';
    } else if (ua.indexOf('Safari') > -1) {
        browser = 'Safari';
        version = ua.match(/Version\/(\d+)/)?.[1] || 'Unknown';
    } else if (ua.indexOf('Edge') > -1) {
        browser = 'Edge';
        version = ua.match(/Edge\/(\d+)/)?.[1] || 'Unknown';
    }
    
    return {
        browser,
        version,
        userAgent: ua,
        language: navigator.language,
        platform: navigator.platform,
        cookieEnabled: navigator.cookieEnabled,
        onLine: navigator.onLine
    };
}

/**
 * V√©rifie si le navigateur supporte une fonctionnalit√©
 * @param {string} feature - Nom de la fonctionnalit√©
 * @returns {boolean} True si support√©e
 */
function supportsFeature(feature) {
    switch (feature) {
        case 'websockets':
            return 'WebSocket' in window;
        case 'localstorage':
            return 'localStorage' in window;
        case 'clipboard':
            return 'clipboard' in navigator;
        case 'notifications':
            return 'Notification' in window;
        case 'speechrecognition':
            return 'SpeechRecognition' in window || 'webkitSpeechRecognition' in window;
        case 'speechsynthesis':
            return 'speechSynthesis' in window;
        default:
            return false;
    }
}

/**
 * G√®re les erreurs globalement
 * @param {Error} error - Erreur √† traiter
 * @param {string} context - Contexte de l'erreur
 */
function handleError(error, context = 'Application') {
    const errorMessage = `${context}: ${error.message}`;
    
    addLogEntry(errorMessage, 'error');
    console.error(`[${context}]`, error);
    
    // Optionnel: Envoyer l'erreur √† un service de monitoring
    // sendErrorToMonitoring(error, context);
}

/**
 * Cr√©e un √©l√©ment DOM avec des attributs
 * @param {string} tag - Tag HTML
 * @param {Object} attributes - Attributs de l'√©l√©ment
 * @param {string} textContent - Contenu texte
 * @returns {HTMLElement} √âl√©ment cr√©√©
 */
function createElement(tag, attributes = {}, textContent = '') {
    const element = document.createElement(tag);
    
    Object.entries(attributes).forEach(([key, value]) => {
        if (key === 'className') {
            element.className = value;
        } else if (key === 'style' && typeof value === 'object') {
            Object.assign(element.style, value);
        } else {
            element.setAttribute(key, value);
        }
    });
    
    if (textContent) {
        element.textContent = textContent;
    }
    
    return element;
}

/**
 * Affiche une notification toast
 * @param {string} message - Message √† afficher
 * @param {string} type - Type (success, warning, error, info)
 * @param {number} duration - Dur√©e en ms
 */
function showToast(message, type = 'info', duration = 3000) {
    // Cr√©er le conteneur de toasts s'il n'existe pas
    let toastContainer = document.getElementById('toast-container');
    if (!toastContainer) {
        toastContainer = createElement('div', {
            id: 'toast-container',
            style: {
                position: 'fixed',
                top: '20px',
                right: '20px',
                zIndex: '10000',
                display: 'flex',
                flexDirection: 'column',
                gap: '10px'
            }
        });
        document.body.appendChild(toastContainer);
    }
    
    // Cr√©er le toast
    const toast = createElement('div', {
        className: `toast toast-${type}`,
        style: {
            padding: '12px 16px',
            borderRadius: '8px',
            color: 'white',
            fontWeight: '500',
            boxShadow: '0 4px 12px rgba(0,0,0,0.3)',
            transform: 'translateX(100%)',
            transition: 'transform 0.3s ease',
            maxWidth: '300px',
            backgroundColor: type === 'success' ? '#10b981' :
                           type === 'warning' ? '#f59e0b' :
                           type === 'error' ? '#ef4444' : '#3b82f6'
        }
    }, message);
    
    toastContainer.appendChild(toast);
    
    // Animation d'entr√©e
    setTimeout(() => {
        toast.style.transform = 'translateX(0)';
    }, 10);
    
    // Suppression automatique
    setTimeout(() => {
        toast.style.transform = 'translateX(100%)';
        setTimeout(() => {
            if (toast.parentNode) {
                toast.parentNode.removeChild(toast);
            }
        }, 300);
    }, duration);
}

/**
 * Gestionnaire d'erreurs globales
 */
window.addEventListener('error', function(event) {
    handleError(event.error, 'Global Error');
});

window.addEventListener('unhandledrejection', function(event) {
    handleError(new Error(event.reason), 'Unhandled Promise');
});

// Export des fonctions pour utilisation dans d'autres modules
if (typeof module !== 'undefined' && module.exports) {
    module.exports = {
        saveSettings, loadSavedSettings, loadSettings, updateUI,
        formatDuration, formatTokenCount, generateUniqueId,
        debounce, throttle, isElementVisible, copyToClipboard,
        isValidEmail, isValidUrl, sanitizeString, formatFileSize,
        getBrowserInfo, supportsFeature, handleError, createElement, showToast
    };
}

==================================================
FICHIER: .\web_interface\js\variables-globals.js
==================================================

/**
 * variables-globals.js - Variables globales et √©tat de l'application Jarvis
 * üß† Cortex Pr√©frontal - √âtat central de l'application
 */

// √âtat de connexion
let ws = null;
let isConnected = false;
let isListening = false;

// Configuration interface
let currentTheme = 'light';
let cameraVisible = false;
let voiceVisible = false;
let currentVoiceId = 0;
let debugVisible = false;

// Statistiques de session
let stats = {
    messages: 0,
    tokens: 0,
    totalTime: 0,
    ttsEfficiency: 100
};

// Configuration charg√©e depuis les fichiers JSON
let config = {
    themes: {},
    voices: {},
    backgrounds: {},
    models: {}
};

// √âtat de l'interface
let interfaceState = {
    settingsModalOpen: false,
    helpModalOpen: false,
    currentSettingsTab: 'voice',
    currentDebugTab: 'logs'
};

/**
 * R√©initialise les statistiques de session
 */
function resetStats() {
    stats = {
        messages: 0,
        tokens: 0,
        totalTime: 0,
        ttsEfficiency: 100
    };
}

/**
 * Met √† jour les statistiques
 * @param {Object} metadata - M√©tadonn√©es de performance
 */
function updateStats(metadata) {
    if (metadata.total_time) {
        stats.totalTime = (stats.totalTime + metadata.total_time) / 2; // Moyenne mobile
    }
    if (metadata.token_count) {
        stats.tokens += metadata.token_count;
    }
    updateStatsDisplay();
}

/**
 * Met √† jour l'affichage des statistiques
 */
function updateStatsDisplay() {
    const elements = {
        'stat-messages': stats.messages,
        'stat-tokens': stats.tokens,
        'stat-avgtime': `${stats.totalTime.toFixed(1)}s`,
        'stat-tts-efficiency': `${stats.ttsEfficiency.toFixed(0)}%`
    };
    
    Object.entries(elements).forEach(([id, value]) => {
        const element = document.getElementById(id);
        if (element) element.textContent = value;
    });
}

/**
 * Met √† jour l'efficacit√© TTS
 * @param {boolean} success - Succ√®s de l'op√©ration TTS
 */
function updateTTSEfficiency(success) {
    if (success) {
        stats.ttsEfficiency = Math.min(100, stats.ttsEfficiency + 0.1);
    } else {
        stats.ttsEfficiency = Math.max(0, stats.ttsEfficiency - 1);
    }
}

// Export des variables pour les autres modules (si utilisation de modules ES6)
if (typeof module !== 'undefined' && module.exports) {
    module.exports = {
        ws, isConnected, isListening, currentTheme, cameraVisible, voiceVisible, debugVisible,
        stats, config, interfaceState, currentVoiceId, 
        resetStats, updateStats, updateStatsDisplay, updateTTSEfficiency
    };
}

==================================================
FICHIER: .\web_interface\js\voice-lab.js
==================================================

/**
 * voice-lab-complete.js - Laboratoire de clonage vocal complet
 * üé≠ Interface compl√®te pour cr√©er, g√©rer et utiliser des voix clon√©es
 */

// √âtat du Voice Lab
let isRecording = false;
let mediaRecorder = null;
let audioChunks = [];
let recordingStartTime = null;
let recordingTimer = null;
let currentEditingVoice = null;
let pendingAudioData = null;

/**
 * Toggle le panneau Voice Lab
 */
function toggleVoiceLab() {
    voiceVisible = !voiceVisible;
    updateVoiceVisibility();

    addLogEntry(`üîç Voice Lab : ${voiceVisible ? 'activ√©' : 'd√©sactiv√©'}`, 'info');
    saveSettings();
}

/**
 * Met √† jour la visibilit√© du panneau de camera
 */
function updateVoiceVisibility() {
    const voiceSection = document.getElementById('voice-section');
    const mainContent = document.querySelector('.main-content');
    
    if (!voiceSection || !mainContent) return;
    
    if (voiceVisible) {
        voiceSection.classList.remove('hidden');
        mainContent.classList.remove('voice-hidden');        
        loadClonedVoices();
        updateVoiceStats();
    } else {
        voiceSection.classList.add('hidden');
        mainContent.classList.add('voice-hidden');
        // Arr√™ter l'enregistrement si en cours
        if (isRecording) {
            stopVoiceRecording();
        }
    }
}

/**
 * D√©marre l'enregistrement audio pour le clonage
 */
async function startVoiceRecording() {
    if (isRecording) {
        addLogEntry('‚ö†Ô∏è Enregistrement d√©j√† en cours', 'warning');
        return;
    }
    
    try {
        // Configuration audio optimis√©e pour le clonage vocal
        const stream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
                channelCount: 1,
                sampleRate: 16000,
                sampleSize: 16,
                echoCancellation: true,
                noiseSuppression: true,
                autoGainControl: true
            } 
        });
        
        // Cr√©er le MediaRecorder
        const mimeType = MediaRecorder.isTypeSupported('audio/webm') 
            ? 'audio/webm' 
            : 'audio/ogg';
        
        mediaRecorder = new MediaRecorder(stream, { 
            mimeType,
            audioBitsPerSecond: 128000
        });
        
        audioChunks = [];
        
        mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
                audioChunks.push(event.data);
            }
        };
        
        mediaRecorder.onstop = async () => {
            const audioBlob = new Blob(audioChunks, { type: mimeType });
            const duration = (Date.now() - recordingStartTime) / 1000;
            
            if (duration < 6) {
                showToast('‚ö†Ô∏è Enregistrement trop court (minimum 6 secondes)', 'warning');
                addLogEntry(`‚ö†Ô∏è Audio rejet√©: ${duration.toFixed(1)}s (min 6s)`, 'warning');
                pendingAudioData = null;
            } else {
                // Stocker l'audio en attente
                pendingAudioData = {
                    blob: audioBlob,
                    duration: duration,
                    type: 'recording'
                };
                
                showAudioPreview(audioBlob, duration);
                enableSaveButton(true);
                
                addLogEntry(`‚úÖ Enregistrement termin√©: ${duration.toFixed(1)}s`, 'success');
                showToast('‚úÖ Enregistrement pr√™t √† √™tre sauvegard√©', 'success');
            }
            
            // Lib√©rer le stream
            stream.getTracks().forEach(track => track.stop());
        };
        
        isRecording = true;
        recordingStartTime = Date.now();
        
        mediaRecorder.start();
        startRecordingTimer();
        updateRecordingUI(true);
        
        addLogEntry('üé§ Enregistrement d√©marr√©...', 'info');
        showToast('üé§ Parlez clairement pendant 6-30 secondes', 'info');
        
        // Arr√™t automatique apr√®s 30 secondes
        setTimeout(() => {
            if (isRecording) {
                stopVoiceRecording();
                showToast('‚è∞ Arr√™t automatique (30s max)', 'warning');
            }
        }, 30000);
        
    } catch (error) {
        addLogEntry(`‚ùå Erreur microphone: ${error.message}`, 'error');
        showToast('‚ùå Impossible d\'acc√©der au microphone', 'error');
    }
}

/**
 * Arr√™te l'enregistrement vocal
 */
function stopVoiceRecording() {
    if (!isRecording || !mediaRecorder) {
        addLogEntry('‚ö†Ô∏è Aucun enregistrement en cours', 'warning');
        return;
    }
    
    try {
        mediaRecorder.stop();
        isRecording = false;
        
        stopRecordingTimer();
        updateRecordingUI(false);
        
        addLogEntry('üõë Enregistrement arr√™t√©', 'info');
        
    } catch (error) {
        addLogEntry(`‚ùå Erreur arr√™t: ${error.message}`, 'error');
    }
}

/**
 * G√®re l'upload de fichier audio/vid√©o
 */
async function handleVoiceFileUpload(event) {
    const file = event.target.files[0];
    if (!file) return;
    
    // V√©rifier le type de fichier
    const isVideo = file.type.startsWith('video/');
    const isAudio = file.type.startsWith('audio/');
    
    if (!isVideo && !isAudio) {
        showToast('‚ö†Ô∏è Veuillez s√©lectionner un fichier audio ou vid√©o', 'warning');
        event.target.value = ''; // Reset input
        return;
    }
    
    // V√©rifier la taille (max 50MB)
    const maxSize = 50 * 1024 * 1024;
    if (file.size > maxSize) {
        showToast('‚ö†Ô∏è Fichier trop volumineux (max 50MB)', 'warning');
        event.target.value = '';
        return;
    }
    
    try {
        addLogEntry(`üìÅ Fichier charg√©: ${file.name} (${(file.size / 1024 / 1024).toFixed(2)}MB)`, 'info');
        
        // Lire le fichier en tant qu'ArrayBuffer
        const arrayBuffer = await file.arrayBuffer();
        
        // Stocker en attente de sauvegarde
        pendingAudioData = {
            data: arrayBuffer,
            filename: file.name,
            type: isVideo ? 'video' : 'audio',
            size: file.size
        };
        
        showFileInfo(file.name, file.size, isVideo);
        
        const voiceName = document.getElementById('voice-name-input')?.value?.trim();
        enableSaveButton(voiceName && pendingAudioData);
        
        showToast(
            isVideo 
                ? '‚úÖ Vid√©o charg√©e - audio sera extrait' 
                : '‚úÖ Audio charg√© et pr√™t', 
            'success'
        );
        
    } catch (error) {
        addLogEntry(`‚ùå Erreur lecture fichier: ${error.message}`, 'error');
        showToast('‚ùå Erreur lors du chargement du fichier', 'error');
        event.target.value = '';
    }
}

/**
 * Sauvegarde la voix clon√©e
 */

function arrayBufferToBase64(buffer) {
    const bytes = new Uint8Array(buffer);
    let binary = '';
    
    for (let i = 0; i < bytes.length; i++) {
        binary += String.fromCharCode(bytes[i]);
    }
    
    return btoa(binary);
}

async function saveClonedVoice() {
    const voiceName = document.getElementById('voice-name-input')?.value?.trim();
    const voiceDescription = document.getElementById('voice-description-input')?.value?.trim();
    
    // Validation
    if (!voiceName) {
        showToast('‚ö†Ô∏è Nom de la voix requis', 'warning');
        return;
    }
    
    if (!pendingAudioData) {
        showToast('‚ö†Ô∏è Aucun audio en attente de sauvegarde', 'warning');
        return;
    }
    
    // D√©sactiver l'interface pendant le traitement
    const saveBtn = document.getElementById('save-voice-btn');
    const originalText = saveBtn?.textContent;
    if (saveBtn) saveBtn.textContent = 'üîÑ Traitement...';

    try {
        addLogEntry(`üíæ Cr√©ation voix: ${voiceName}...`, 'info');
        showToast('üîÑ Traitement en cours...', 'info');
        
        let audioData;
        let fileType;
        
        if (pendingAudioData.type === 'recording') {
            // Enregistrement direct
            audioData = await pendingAudioData.blob.arrayBuffer();
            fileType = 'audio';
        } else {
            // Fichier upload√©
            audioData = pendingAudioData.data;
            fileType = pendingAudioData.type;
        }
        
        
        // Encoder en base64
        const base64Audio = arrayBufferToBase64(audioData);
        
        // Envoyer au serveur
        const response = await fetch('/api/voice/clone', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                audio_data: base64Audio,
                voice_name: voiceName,
                description: voiceDescription,
                file_type: fileType
            })
        });
        
        const result = await response.json();
        
        if (result.success) {
            addLogEntry(`‚úÖ Voix cr√©√©e: ${voiceName} (ID: ${result.voice_id})`, 'success');
            showToast(`‚úÖ Voix "${voiceName}" cr√©√©e avec succ√®s !`, 'success');
            
            // ‚úÖ NOUVEAU - Utiliser le gestionnaire centralis√©
            await refreshVoices();
            
            // Reset du formulaire
            resetVoiceForm();
            
        } else {
            throw new Error(result.error || 'Erreur inconnue');
        }
        
    } catch (error) {
        addLogEntry(`‚ùå Erreur cr√©ation voix: ${error.message}`, 'error');
        showToast(`‚ùå Erreur: ${error.message}`, 'error');
    } finally {
        if (saveBtn && originalText) {
            saveBtn.textContent = originalText;
        }
    }
}

/**
 * Charge la liste des voix clon√©es
 */
async function loadClonedVoices() {
    try {
        const response = await fetch('/api/voice/cloned/list');
        const data = await response.json();
        
        if (data.success) {
            displayClonedVoices(data.voices);
            addLogEntry(`üìã ${data.voices.length} voix clon√©es charg√©es`, 'info');
        } else {
            console.error('Erreur API:', data.error);
            displayClonedVoices([]);
        }
    } catch (error) {
        console.error('Erreur chargement voix clon√©es:', error);
        displayClonedVoices([]);
        addLogEntry(`‚ùå Erreur: ${error.message}`, 'error');
    }
}

/**
 * Affiche la liste des voix clon√©es
 */
function displayClonedVoices(voices) {
    const container = document.getElementById('cloned-voices-list');
    if (!container) return;
    
    if (!voices || voices.length === 0) {
        container.innerHTML = `
            <div class="empty-state">
                <p>üé≠ Aucune voix clon√©e</p>
                <small>Utilisez l'enregistrement ou l'upload pour cr√©er votre premi√®re voix</small>
            </div>
        `;
        return;
    }
    
    // Trier par date de cr√©ation (plus r√©cent en premier)
    const sortedVoices = voices.sort((a, b) => (b.created_at || 0) - (a.created_at || 0));
    
    container.innerHTML = sortedVoices.map(voice => {
        const isDefault = voice.id === currentVoiceId;
        const statusIcon = voice.status === 'ready' ? '‚úÖ' : '‚è≥';
        const duration = voice.duration ? `${voice.duration.toFixed(1)}s` : 'N/A';
        
        return `
            <div class="voice-item ${isDefault ? 'is-default' : ''}" data-voice-id="${voice.id}">
                <div class="voice-header">
                    <h4>${statusIcon} ${voice.name}</h4>
                    <span class="voice-duration">${duration}</span>
                </div>
                <p class="voice-description">${voice.description || 'Aucune description'}</p>
                <div class="voice-meta">
                    <small>üéØ Mod√®le: ${voice.model}</small>
                    <small>üìÖ ${new Date(voice.created_at * 1000).toLocaleDateString()}</small>
                    ${voice.has_embedding ? '<small>‚ö° Optimis√©e</small>' : ''}
                </div>
                <div class="voice-actions">
                    <button onclick="testClonedVoice('${voice.id}')" 
                            class="voice-btn test-btn" 
                            title="Tester cette voix"
                            ${voice.status !== 'ready' ? 'disabled' : ''}>
                        üîä
                    </button>
                    <button onclick="selectClonedVoice('${voice.id}')" 
                            class="voice-btn select-btn" 
                            title="Utiliser cette voix"
                            ${voice.status !== 'ready' || isDefault ? 'disabled' : ''}>
                        ${isDefault ? '‚úÖ' : '‚òëÔ∏è'}
                    </button>
                    <button onclick="editClonedVoice('${voice.id}')" 
                            class="voice-btn edit-btn" 
                            title="Renommer">
                        ‚úèÔ∏è
                    </button>
                    <button onclick="exportClonedVoice('${voice.id}')" 
                            class="voice-btn export-btn" 
                            title="Exporter">
                        üíæ
                    </button>
                    <button onclick="deleteClonedVoice('${voice.id}')" 
                            class="voice-btn delete-btn" 
                            title="Supprimer">
                        üóëÔ∏è
                    </button>
                </div>
            </div>
        `;
    }).join('');
}

/**
 * √âdite une voix clon√©e (renommer)
 */
async function editClonedVoice(voiceId) {
    const newName = prompt('Nouveau nom de la voix:');
    const newDesc = prompt('Nouvelle description (optionnel):');
    
    if (!newName || newName.trim() === '') {
        showToast('‚ö†Ô∏è Nom requis', 'warning');
        return;
    }
    
    try {
        const response = await fetch(`/api/voice/rename/${voiceId}`, {
            method: 'PUT',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                new_name: newName.trim(),
                new_description: newDesc?.trim() || ''
            })
        });
        
        const result = await response.json();
        
        if (result.success) {
            showToast(`‚úÖ Voix renomm√©e: ${newName}`, 'success');
            await loadClonedVoices();  // Recharger la liste
            // ‚úÖ NOUVEAU - Utiliser le gestionnaire centralis√©
            await refreshVoices();
        } else {
            showToast(`‚ùå Erreur: ${result.error}`, 'error');
        }
    } catch (error) {
        showToast(`‚ùå Erreur: ${error.message}`, 'error');
    }
}

/**
 * Supprime une voix clon√©e
 */
async function deleteClonedVoice(voiceId) {
    if (!confirm('‚ö†Ô∏è √ätes-vous s√ªr de vouloir supprimer cette voix ?')) {
        return;
    }
    
    try {
        const response = await fetch(`/api/voice/delete/${voiceId}`, {
            method: 'DELETE'
        });
        
        const result = await response.json();
        
        if (result.success) {
            showToast('‚úÖ Voix supprim√©e', 'success');
            await loadClonedVoices();  // Recharger la liste
            updateVoiceStats();        // Mettre √† jour les stats
            
            // ‚úÖ NOUVEAU - Utiliser le gestionnaire centralis√©
            await refreshVoices();
        } else {
            showToast(`‚ùå Erreur: ${result.error}`, 'error');
        }
    } catch (error) {
        showToast(`‚ùå Erreur: ${error.message}`, 'error');
    }
}

/**
 * Teste une voix clon√©e
 */
async function testClonedVoice(voiceId) {
    try {
        showToast('üîä Test de la voix...', 'info');
        
        const response = await fetch('/api/voice/test', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                voice_id: voiceId,
                text: document.getElementById('test-text-input')?.value || 
                      "Bonjour, ceci est un test de ma voix clon√©e. Je peux maintenant parler avec cette voix personnalis√©e."
            })
        });
        
        if (response.ok) {
            addLogEntry(`üîä Test voix: ${voiceId}`, 'info');
        } else {
            const error = await response.json();
            showToast(`‚ùå Erreur: ${error.message || 'Test √©chou√©'}`, 'error');
        }
        
    } catch (error) {
        showToast(`‚ùå Erreur: ${error.message}`, 'error');
    }
}

/**
 * S√©lectionne une voix clon√©e pour l'utiliser
 */
async function selectClonedVoice(voiceId) {
    try {
        const response = await fetch('/api/voice/set-default', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                voice_id: voiceId
            })
        });
        
        const result = await response.json();
        
        if (result.success) {
            showToast(`‚úÖ Voix active: ${result.voice_name}`, 'success');
            
            // Mettre √† jour la variable globale
            currentVoiceId = voiceId;
            
            // Mettre √† jour l'affichage dans les param√®tres
            updatePersonality(`üé≠ ${result.voice_name}`);
            
            // Recharger la liste pour mettre √† jour l'UI
            await loadClonedVoices();
            
            // ‚úÖ NOUVEAU - Utiliser le gestionnaire centralis√©
            await refreshVoices();
            
            // Mettre √† jour le s√©lecteur de voix dans les param√®tres
            const voiceSelect = document.getElementById('voice-personality');
            for (let option of voiceSelect.options) {
                const voiceData = voices[option.value];
                if (voiceData && voiceData.voice_id === currentVoiceId) {
                    voiceSelect.value = option.value;
                    break;
                }
            }
            
        } else {
            showToast(`‚ùå ${result.error}`, 'error');
        }
        
    } catch (error) {
        showToast(`‚ùå Erreur: ${error.message}`, 'error');
    }
}

/**
 * √âdite une voix clon√©e (renommer)
 */
async function editClonedVoice(voiceId) {
    const newName = prompt('Nouveau nom de la voix:');
    const newDesc = prompt('Nouvelle description (optionnel):');
    
    if (!newName || newName.trim() === '') {
        showToast('‚ö†Ô∏è Nom requis', 'warning');
        return;
    }
    
    try {
        const response = await fetch(`/api/voice/rename/${voiceId}`, {
            method: 'PUT',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify({
                new_name: newName.trim(),
                new_description: newDesc?.trim() || ''
            })
        });
        
        const result = await response.json();
        
        if (result.success) {
            showToast(`‚úÖ Voix renomm√©e: ${newName}`, 'success');
            await loadClonedVoices();  // Recharger la liste
            updateVoiceStats();        // Mettre √† jour les stats
        } else {
            showToast(`‚ùå Erreur: ${result.error}`, 'error');
        }
    } catch (error) {
        showToast(`‚ùå Erreur: ${error.message}`, 'error');
    }
}

/**
 * Supprime une voix clon√©e
 */
async function deleteClonedVoice(voiceId) {
    if (!confirm('‚ö†Ô∏è √ätes-vous s√ªr de vouloir supprimer cette voix ?')) {
        return;
    }
    
    try {
        const response = await fetch(`/api/voice/delete/${voiceId}`, {
            method: 'DELETE'
        });
        
        const result = await response.json();
        
        if (result.success) {
            showToast('‚úÖ Voix supprim√©e', 'success');
            await loadClonedVoices();  // Recharger la liste
            updateVoiceStats();        // Mettre √† jour les stats
        } else {
            showToast(`‚ùå Erreur: ${result.error}`, 'error');
        }
    } catch (error) {
        showToast(`‚ùå Erreur: ${error.message}`, 'error');
    }
}

/**
 * Met √† jour les statistiques des voix
 */
async function updateVoiceStats() {
    try {
        const response = await fetch('/api/voice/stats');
        const data = await response.json();
        
        const statsDiv = document.getElementById('voice-stats');
        if (statsDiv && data.success) {
            statsDiv.innerHTML = `
                <div class="stats-grid">
                    <div class="stat-item">
                        <span class="stat-label">Total</span>
                        <span class="stat-value">${data.total_voices}</span>
                    </div>
                    <div class="stat-item">
                        <span class="stat-label">Clon√©es</span>
                        <span class="stat-value">${data.cloned_voices}</span>
                    </div>
                    <div class="stat-item">
                        <span class="stat-label">Stockage</span>
                        <span class="stat-value">${data.storage_used}</span>
                    </div>
                    <div class="stat-item">
                        <span class="stat-label">Mod√®le</span>
                        <span class="stat-value">${data.xtts_loaded ? '‚úÖ XTTS' : '‚ö†Ô∏è Edge-TTS'}</span>
                    </div>
                </div>
            `;
        }
    } catch (error) {
        console.error('Erreur stats:', error);
    }
}

// Fonctions utilitaires

/**
 * Active/d√©sactive le bouton de sauvegarde
 */
function enableSaveButton(enabled) {
    console.log('üîç enableSaveButton appel√©e');
    const saveBtn = document.getElementById('save-voice-btn');
    if (saveBtn) {
        saveBtn.disabled = !enabled;
        saveBtn.style.opacity = enabled ? '1' : '0.5';
    }
}

/**
 * Affiche l'aper√ßu audio
 */
function showAudioPreview(blob, duration) {
    const previewDiv = document.getElementById('audio-preview');
    if (!previewDiv) return;
    
    const url = URL.createObjectURL(blob);
    
    previewDiv.innerHTML = `
        <div class="audio-preview-card">
            <h5>üìä Aper√ßu de l'enregistrement</h5>
            <audio controls src="${url}"></audio>
            <p>Dur√©e: ${duration.toFixed(1)} secondes</p>
        </div>
    `;
    
    previewDiv.style.display = 'block';
}

/**
 * Affiche les infos du fichier upload√©
 */
function showFileInfo(filename, size, isVideo) {
    const previewDiv = document.getElementById('audio-preview');
    if (!previewDiv) return;
    
    const sizeKB = (size / 1024).toFixed(1);
    const sizeMB = (size / (1024 * 1024)).toFixed(2);
    const displaySize = size > 1024 * 1024 ? `${sizeMB} MB` : `${sizeKB} KB`;
    
    previewDiv.innerHTML = `
        <div class="file-info-card">
            <h5>${isVideo ? 'üé•' : 'üéµ'} Fichier charg√©</h5>
            <p><strong>Nom:</strong> ${filename}</p>
            <p><strong>Taille:</strong> ${displaySize}</p>
            <p><strong>Type:</strong> ${isVideo ? 'Vid√©o' : 'Audio'}</p>
            ${isVideo ? '<p class="info-note">‚ÑπÔ∏è L\'audio sera extrait de la vid√©o</p>' : ''}
        </div>
    `;
    
    previewDiv.style.display = 'block';
}

/**
 * Lance le timer d'enregistrement
 */
function startRecordingTimer() {
    let seconds = 0;
    const timerDisplay = document.getElementById('recording-timer');
    
    if (timerDisplay) {
        timerDisplay.style.display = 'inline-block';
    }
    
    recordingTimer = setInterval(() => {
        seconds++;
        if (timerDisplay) {
            timerDisplay.textContent = `${seconds}s / 30s`;
            
            // Changer la couleur selon la dur√©e
            if (seconds < 6) {
                timerDisplay.style.color = '#ff9800'; // Orange
            } else {
                timerDisplay.style.color = '#4caf50'; // Vert
            }
        }
        
        // Indicateur √† 6s
        if (seconds === 6) {
            showToast('‚úÖ Dur√©e minimale atteinte', 'success');
        }
    }, 1000);
}

/**
 * Arr√™te le timer d'enregistrement
 */
function stopRecordingTimer() {
    if (recordingTimer) {
        clearInterval(recordingTimer);
        recordingTimer = null;
    }
    
    const timerDisplay = document.getElementById('recording-timer');
    if (timerDisplay) {
        timerDisplay.style.display = 'none';
        timerDisplay.textContent = '';
        timerDisplay.style.color = '';
    }
}

/**
 * Met √† jour l'interface d'enregistrement
 */
function updateRecordingUI(recording) {
    const recordBtn = document.getElementById('record-btn');
    const stopBtn = document.getElementById('stop-record-btn');
    const indicator = document.getElementById('recording-indicator');
    
    if (recording) {
        if (recordBtn) recordBtn.style.display = 'none';
        if (stopBtn) stopBtn.style.display = 'inline-block';
        if (indicator) indicator.classList.add('hidden');
        
        // D√©sactiver les autres contr√¥les
        document.querySelectorAll('#cloned-voices-list input, #cloned-voices-list textarea')
            .forEach(input => input.disabled = true);
        
    } else {
        if (recordBtn) recordBtn.style.display = 'inline-block';
        if (stopBtn) stopBtn.style.display = 'none';
        if (indicator) indicator.classList.remove('hidden');
        
        // R√©activer les contr√¥les
        document.querySelectorAll('#cloned-voices-list input, #cloned-voices-list textarea')
            .forEach(input => input.disabled = false);
    }
}

/**
 * Remet le formulaire √† z√©ro
 */
function resetVoiceForm() {
    // Reset des champs texte
    const nameInput = document.getElementById('voice-name-input');
    const descInput = document.getElementById('voice-description-input');
    const fileInput = document.getElementById('voice-file-input');
    
    if (nameInput) nameInput.value = '';
    if (descInput) descInput.value = '';
    if (fileInput) fileInput.value = '';
    
    // Reset de l'aper√ßu
    const previewDiv = document.getElementById('audio-preview');
    if (previewDiv) {
        previewDiv.style.display = 'none';
        previewDiv.innerHTML = '';
    }
    
    // Reset des donn√©es en attente
    pendingAudioData = null;
    enableSaveButton(false);
    
    addLogEntry('üîÑ Formulaire r√©initialis√©', 'info');
}


/**
 * Initialise les √©v√©nements du Voice Lab
 */
function initializeVoiceLab() {
    // Connecter l'upload de fichier
    const fileInput = document.getElementById('voice-upload');
    if (fileInput) {
        fileInput.addEventListener('change', handleVoiceFileUpload);
        console.log('‚úÖ Event file upload connect√©');
    }
    
    // Validation simple sur le nom
    const nameInput = document.getElementById('voice-name-input');
    if (nameInput) {
        nameInput.addEventListener('input', function() {
            const voiceName = this.value.trim();
            enableSaveButton(voiceName && pendingAudioData);
        });
    }
}

/**
 * V√©rifie si le formulaire est valide
 */
function checkFormValidation() {
    const voiceName = document.getElementById('voice-name-input')?.value?.trim();
    const hasAudio = pendingAudioData !== null;
    
    console.log('üîç Validation:', { voiceName, hasAudio });
    
    const shouldEnable = voiceName && hasAudio;
    
    // ‚úÖ Appel direct sans r√©cursion
    const saveBtn = document.getElementById('save-voice-btn');
    if (saveBtn) {
        saveBtn.disabled = !shouldEnable;
        saveBtn.style.opacity = shouldEnable ? '1' : '0.5';
    }
}

// Chargement initial
document.addEventListener('DOMContentLoaded', () => {
    console.log('üé≠ Voice Lab DOM loaded');
    
    // Initialiser les √©v√©nements
    initializeVoiceLab();
    
    if (voiceVisible) {
        loadClonedVoices();
        updateVoiceStats();
    }
});

console.log('üé≠ Voice Lab charg√©');


==================================================
FICHIER: .\web_interface\js\voice-manager.js
==================================================

/**
 * voice-manager.js - Gestionnaire centralis√© des voix
 * üé§ Module unique pour toute la gestion des voix (standard + clon√©es)
 */

class VoiceManager {
    constructor() {
        this.voices = {
            standard: {},
            cloned: {}
        };
        this.isLoaded = false;
        this.listeners = new Set();
    }

    /**
     * Charge TOUTES les voix depuis le serveur
     */
    async loadAllVoices() {
        try {
            console.log('üîÑ Chargement des voix...');
            
            const response = await fetch('/api/voice/all/list');
            const data = await response.json();
            
            if (data.success || data.voices) {  // Parfois pas de champ "success"
                // Nettoyer et r√©organiser
                this.voices.standard = data.voices || {};
                this.voices.cloned = {};
                
                // ‚úÖ CORRECTION - Prendre TOUTES les voix clon√©es pr√™tes
                if (data.cloned_voices) {
                    Object.entries(data.cloned_voices).forEach(([id, voice]) => {
                        console.log(`üé≠ Voix clon√©e trouv√©e: ${id} - ${voice.name} - Status: ${voice.processing_status}`);
                        
                        // ‚úÖ Filtre corrig√©
                        if (voice.processing_status === 'ready') {
                            this.voices.cloned[id] = voice;
                            console.log(`‚úÖ Voix ajout√©e: ${voice.name}`);
                        } else {
                            console.log(`‚ö†Ô∏è Voix ignor√©e (status: ${voice.processing_status}): ${voice.name}`);
                        }
                    });
                }
                
                this.isLoaded = true;
                this.notifyListeners('voices_loaded');
                
                console.log(`‚úÖ ${Object.keys(this.voices.standard).length} voix standard, ${Object.keys(this.voices.cloned).length} voix clon√©es`);
                console.log(`üé≠ Voix clon√©es charg√©es:`, Object.keys(this.voices.cloned));
                
                return true;
            } else {
                console.error('‚ùå Format API inattendu:', data);
                return false;
            }
        } catch (error) {
            console.error('‚ùå Erreur chargement voix:', error);
            return false;
        }
    }

    /**
     * Met √† jour une interface avec les voix
     */
    populateSelect(selectId) {
        const select = document.getElementById(selectId);
        if (!select || !this.isLoaded) return false;

        // Vider la liste
        select.innerHTML = '';

        // Ajouter voix standard
        if (Object.keys(this.voices.standard).length > 0) {
            const standardGroup = document.createElement('optgroup');
            standardGroup.label = 'üé§ Voix standard';
            
            Object.entries(this.voices.standard).forEach(([id, voice]) => {
                const option = document.createElement('option');
                option.value = id;
                option.textContent = voice.display_name || voice.name;
                standardGroup.appendChild(option);
            });
            
            select.appendChild(standardGroup);
        }

        // Ajouter voix clon√©es
        if (Object.keys(this.voices.cloned).length > 0) {
            const clonedGroup = document.createElement('optgroup');
            clonedGroup.label = 'üé≠ Voix clon√©es';
            
            Object.entries(this.voices.cloned).forEach(([id, voice]) => {
                const option = document.createElement('option');
                option.value = id;
                option.textContent = voice.display_name || voice.name;
                clonedGroup.appendChild(option);
            });
            
            select.appendChild(clonedGroup);
        }

        return true;
    }

    /**
     * Rechargement apr√®s modification
     */
    async refresh() {
        await this.loadAllVoices();
        this.notifyListeners('voices_updated');
    }

    /**
     * Syst√®me d'√©v√©nements simple
     */
    addListener(callback) {
        this.listeners.add(callback);
    }

    removeListener(callback) {
        this.listeners.delete(callback);
    }

    notifyListeners(event) {
        this.listeners.forEach(callback => {
            try {
                callback(event, this.voices);
            } catch (e) {
                console.error('Erreur listener:', e);
            }
        });
    }

    /**
     * Getters utiles
     */
    getVoiceById(id) {
        return this.voices.standard[id] || this.voices.cloned[id] || null;
    }

    getAllVoices() {
        return {...this.voices.standard, ...this.voices.cloned};
    }

    getClonedVoices() {
        return this.voices.cloned;
    }

    getStats() {
        return {
            standard: Object.keys(this.voices.standard).length,
            cloned: Object.keys(this.voices.cloned).length,
            total: Object.keys(this.voices.standard).length + Object.keys(this.voices.cloned).length
        };
    }
}

// Instance globale unique
window.voiceManager = new VoiceManager();

/**
 * Fonctions publiques simplifi√©es
 */

// Charger les voix au d√©marrage
async function initVoices() {
    return await window.voiceManager.loadAllVoices();
}

// Mettre √† jour un select avec les voix
function updateVoiceSelect(selectId) {
    return window.voiceManager.populateSelect(selectId);
}

// Rafra√Æchir apr√®s modification
async function refreshVoices() {
    await window.voiceManager.refresh();
}

// S'abonner aux changements
function onVoicesChange(callback) {
    window.voiceManager.addListener(callback);
}

console.log('üé§ Voice Manager initialis√©');


==================================================
FICHIER: .\web_interface\js\websocket-manager.js
==================================================

/**
 * websocket-manager.js - Gestion de la communication WebSocket
 * üîå Thalamus - Centre de communication et routage des messages
 * üöÄ OPTIMIS√â: Map routing O(1) au lieu de switch O(n)
 */

// üöÄ Map de routage optimis√© (initialisation unique)
const messageRoutes = new Map();

// üìä M√©triques de performance WebSocket
let routingMetrics = {
    totalMessages: 0,
    routingTimeSum: 0,
    unknownTypes: 0,
    mostFrequent: new Map()
};

/**
 * üöÄ Initialise le Map de routage (performance O(1))
 */
function initializeMessageRoutes() {
    // Handlers ordonn√©s par fr√©quence (d'apr√®s tes logs: llm_token = le plus fr√©quent)
    messageRoutes.set('llm_token', handleLLMToken);
    messageRoutes.set('tts_queued', handleTTSQueued);
    messageRoutes.set('first_token', handleFirstToken);
    messageRoutes.set('llm_complete', handleLLMComplete);
    messageRoutes.set('transcription', handleTranscription);
    messageRoutes.set('status', handleStatusMessage);
    messageRoutes.set('listening_start', handleListeningStart);
    messageRoutes.set('listening_end', handleListeningEnd);
    messageRoutes.set('config_updated', handleConfigUpdated);
    messageRoutes.set('message_processing_start', () => startNewAssistantResponse());
    messageRoutes.set('error', (data) => addLogEntry(`‚ùå Erreur: ${data.content}`, 'error'));
    messageRoutes.set('pong', () => { /* Keep-alive response - rien √† faire */ });
    
    addLogEntry('üöÄ Map routing initialis√© (11 routes)', 'info');
}

/**
 * Initialise la connexion WebSocket
 */
function initializeWebSocket() {
    const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    const wsUrl = `${protocol}//${window.location.host}/ws`;
    
    addLogEntry(`üîå Connexion WebSocket: ${wsUrl}`, 'info');
    
    ws = new WebSocket(wsUrl);
    
    ws.onopen = handleWebSocketOpen;
    ws.onmessage = handleWebSocketMessage;
    ws.onclose = handleWebSocketClose;
    ws.onerror = handleWebSocketError;
}

/**
 * G√®re l'ouverture de la connexion WebSocket
 */
function handleWebSocketOpen() {
    isConnected = true;
    updateConnectionStatus();
    addLogEntry('‚úÖ Connexion WebSocket √©tablie', 'success');
    
    // Charger la configuration actuelle depuis le serveur
    loadCurrentConfig();
    
    // √âmettre un √©v√©nement de connexion
    document.dispatchEvent(new CustomEvent('websocketConnected'));
}

/**
 * G√®re les messages re√ßus via WebSocket
 * @param {MessageEvent} event - √âv√©nement message WebSocket
 */
function handleWebSocketMessage(event) {
    try {
        const data = JSON.parse(event.data);
        routeWebSocketMessage(data);
    } catch (error) {
        addLogEntry(`‚ùå Erreur parsing message WebSocket: ${error.message}`, 'error');
    }
}

/**
 * üöÄ OPTIMIS√â - Route les messages WebSocket avec Map O(1)
 * @param {Object} data - Donn√©es du message
 */
function routeWebSocketMessage(data) {
    const startTime = performance.now();
    
    // M√©triques
    routingMetrics.totalMessages++;
    const count = routingMetrics.mostFrequent.get(data.type) || 0;
    routingMetrics.mostFrequent.set(data.type, count + 1);
    
    // üöÄ Routage O(1) avec Map
    const handler = messageRoutes.get(data.type);
    
    if (handler) {
        // Ex√©cution directe du handler
        handler(data);
    } else {
        // Type inconnu
        routingMetrics.unknownTypes++;
        addLogEntry(`‚ö†Ô∏è Type de message WebSocket inconnu: ${data.type}`, 'warning');
    }
    
    // M√©triques de performance
    const routingTime = performance.now() - startTime;
    routingMetrics.routingTimeSum += routingTime;
    
    // Log p√©riodique des performances (tous les 100 messages)
    if (routingMetrics.totalMessages % 100 === 0) {
        logRoutingMetrics();
    }
}

/**
 * üìä Affiche les m√©triques de routage
 */
function logRoutingMetrics() {
    const avgTime = (routingMetrics.routingTimeSum / routingMetrics.totalMessages).toFixed(3);
    const top3 = [...routingMetrics.mostFrequent.entries()]
        .sort((a, b) => b[1] - a[1])
        .slice(0, 3)
        .map(([type, count]) => `${type}:${count}`)
        .join(', ');
    
    addLogEntry(`üìä Routing: ${routingMetrics.totalMessages} msgs, ${avgTime}ms avg, top: ${top3}`, 'info');
}

/**
 * G√®re les messages de statut
 * @param {Object} data - Donn√©es du message
 */
function handleStatusMessage(data) {
    if (data.personality) {
        updatePersonality(data.personality);
        addSystemMessage(`‚úÖ ${data.personality} est pr√™t !`);
    }
    
    if (data.content) {
        addSystemMessage(data.content);
    }
}

/**
 * G√®re les tokens LLM en streaming
 * @param {Object} data - Donn√©es du token
 */
function handleLLMToken(data) {
    appendToCurrentResponse(data.content);
    
    // Mettre √† jour les stats en temps r√©el
    stats.tokens++;
    if (stats.tokens % 10 === 0) {
        updateStatsDisplay();
    }
}

/**
 * G√®re les √©v√©nements TTS
 * @param {Object} data - Donn√©es TTS
 */
function handleTTSQueued(data) {
    if (data.metadata?.success) {
        addLogEntry(`üîä TTS: Phrase ajout√©e (${data.metadata.length} chars)`, 'info');
        updateTTSEfficiency(true);
    } else {
        addLogEntry('‚ö†Ô∏è TTS: Queue pleine, phrase ignor√©e', 'warning');
        updateTTSEfficiency(false);
    }
}

/**
 * G√®re le premier token (Time To First Token)
 * @param {Object} data - Donn√©es du premier token
 */
function handleFirstToken(data) {
    if (data.metadata?.ttft) {
        addLogEntry(`‚ö° Premier token LLM: ${data.metadata.ttft.toFixed(2)}s`, 'info');
    }
}

/**
 * G√®re la fin de g√©n√©ration LLM
 * @param {Object} data - Donn√©es de fin de g√©n√©ration
 */
function handleLLMComplete(data) {
    finishCurrentResponse();
    
    if (data.metadata) {
        updateStats(data.metadata);
        
        // Log des performances
        const { total_time, token_count, tokens_per_second } = data.metadata;
        if (total_time && token_count) {
            addLogEntry(`üìä G√©n√©ration: ${token_count} tokens en ${total_time.toFixed(2)}s (${tokens_per_second?.toFixed(1) || 'N/A'} tok/s)`, 'info');
        }
    }
}

/**
 * G√®re les transcriptions vocales
 * @param {Object} data - Donn√©es de transcription
 */
function handleTranscription(data) {
    const testMessage = document.getElementById('mic-test-message');
    
    if (testMessage) {
        // Mode test microphone
        testMessage.innerHTML = `
            <div class="message-content">
                <p>‚úÖ Test r√©ussi : "${data.content}"</p>
                <p style="font-size: 0.9em; opacity: 0.8;">Votre micro fonctionne parfaitement !</p>
            </div>
        `;
        addLogEntry(`üé§ Test micro OK: ${data.content}`, 'success');
    } else {
        addUserMessage(data.content);
        addLogEntry(`üé§ Transcription: ${data.content}`, 'info');
    }
}

/**
 * G√®re le d√©but d'√©coute
 */
function handleListeningStart() {
    setListeningState(true);
    addLogEntry('üëÇ √âcoute vocale d√©marr√©e', 'info');
}

/**
 * G√®re la fin d'√©coute
 */
function handleListeningEnd() {
    setListeningState(false);
    addLogEntry('üîá √âcoute vocale arr√™t√©e', 'info');
}

/**
 * G√®re les confirmations de mise √† jour de config
 * @param {Object} data - Donn√©es de confirmation
 */
function handleConfigUpdated(data) {
    console.log('üîç [DEBUG] Confirmation re√ßue:', data);
    
    if (data.success) {
        addLogEntry(`‚úÖ ${data.message}`, 'success');
    } else {
        addLogEntry(`‚ùå Erreur param√®tres: ${data.message}`, 'error');
    }
}

/**
 * G√®re la fermeture de la connexion WebSocket
 */
function handleWebSocketClose() {
    isConnected = false;
    updateConnectionStatus();
    addLogEntry('üîå Connexion WebSocket ferm√©e', 'warning');
    
    // √âmettre un √©v√©nement de d√©connexion
    document.dispatchEvent(new CustomEvent('websocketDisconnected'));
    
    // Tentative de reconnexion apr√®s 3 secondes
    setTimeout(() => {
        if (!isConnected) {
            addLogEntry('üîÑ Tentative de reconnexion...', 'info');
            initializeWebSocket();
        }
    }, 3000);
}

/**
 * G√®re les erreurs WebSocket
 * @param {Event} error - √âv√©nement d'erreur
 */
function handleWebSocketError(error) {
    addLogEntry(`‚ùå Erreur WebSocket: ${error.message || 'Erreur inconnue'}`, 'error');
    console.error('Erreur WebSocket:', error);
}

/**
 * Envoie un message via WebSocket
 * @param {Object} message - Message √† envoyer
 * @returns {boolean} Succ√®s de l'envoi
 */
function sendWebSocketMessage(message) {
    console.log('üì§ [WS] Envoi message:', message); 
    if (!ws || ws.readyState !== WebSocket.OPEN) {
        addLogEntry('‚ùå WebSocket non connect√©', 'error');
        return false;
    }
    
    try {
        ws.send(JSON.stringify(message));
        return true;
    } catch (error) {
        addLogEntry(`‚ùå Erreur envoi WebSocket: ${error.message}`, 'error');
        return false;
    }
}

/**
 * Envoie un message texte
 * @param {string} content - Contenu du message
 */
function sendTextMessage(content) {
    return sendWebSocketMessage({
        type: 'text_message',
        text: content
    });
}

/**
 * Lance l'√©coute vocale
 */
function requestVoiceInput() {
    return sendWebSocketMessage({
        type: 'voice_input'
    });
}

/**
 * Met √† jour la configuration
 * @param {Object} config - Nouvelle configuration
 */
function updateServerConfig(config) {
    return sendWebSocketMessage({
        type: 'config_update',
        config: config
    });
}

/**
 * Met √† jour l'indicateur de statut de connexion
 */
function updateConnectionStatus() {
    const indicator = document.getElementById('status-indicator');
    
    if (!indicator) return;
    
    if (isConnected) {
        indicator.classList.remove('offline');
        indicator.title = 'Connect√©';
    } else {
        indicator.classList.add('offline');
        indicator.title = 'D√©connect√©';
    }
}

/**
 * D√©marre le syst√®me de keep-alive
 */
function startKeepAlive() {
    setInterval(() => {
        if (isConnected) {
            sendWebSocketMessage({ type: 'ping' });
        }
    }, 30000); // Ping toutes les 30 secondes
}

/**
 * üöÄ Retourne les m√©triques de routage actuelles
 */
function getRoutingMetrics() {
    return {
        ...routingMetrics,
        avgRoutingTime: routingMetrics.totalMessages > 0 
            ? (routingMetrics.routingTimeSum / routingMetrics.totalMessages).toFixed(3)
            : 0
    };
}

/**
 * üöÄ Reset des m√©triques (pour tests de performance)
 */
function resetRoutingMetrics() {
    routingMetrics = {
        totalMessages: 0,
        routingTimeSum: 0,
        unknownTypes: 0,
        mostFrequent: new Map()
    };
    addLogEntry('üìä M√©triques routing reset', 'info');
}

// Initialiser le routing et le keep-alive d√®s le chargement
document.addEventListener('DOMContentLoaded', () => {
    initializeMessageRoutes();
    startKeepAlive();
});

==================================================
FICHIER: .\web_interface\styles\base.css
==================================================

/* Variables CSS pour les th√®mes */
:root {
    /* Th√®me Light */
    --bg-primary: #ffffff;
    --bg-secondary: #f8f9fa;
    --bg-accent: #e9ecef;
    --accent-color: #0d6efd;
    --text-primary: #212529;
    --text-secondary: #6c757d;
    --text-accent: #0d6efd;
    --border: #dee2e6;
    --shadow: rgba(0,0,0,0.1);
    --success: #198754;
    --warning: #ffc107;
    --danger: #dc3545;
    --info: #0dcaf0;
    --text-accent-rgb: 13, 110, 253;
    --bg-accent-rgb: 233, 236, 239;
    --border-rgb: 222, 226, 230;
}

.theme-dark {
    /* Th√®me Gray - blue */
    --bg-primary: #1a1a1a;
    --bg-primary-rgb: 26, 26, 46;
    --bg-secondary: #2d2d2d;
    --bg-secondary-rgb: 22, 33, 62;
    --bg-accent: #3d3d3d;
    --accent-color: #4dabf7; 
    --text-primary: #ffffff;
    --text-secondary: #b0b0b0;
    --text-accent: #4dabf7;
    --border: #4d4d4d;
    --shadow: rgba(0,0,0,0.3);
    --success: #51cf66;
    --warning: #ffd43b;
    --danger: #ff6b6b;
    --info: #74c0fc;
    --text-accent-rgb: 74, 171, 247;
    --bg-accent-rgb: 61, 61, 61;
}

.theme-jarvis {
    /* Th√®me Dark - Green*/
    --bg-primary: #0a0e1a;
    --bg-secondary: #1a2332;
    --bg-accent: #2a3441;
    --accent-color: #00ff88;
    --text-primary: #00d4ff;
    --text-secondary: #7fb8d3;
    --text-accent: #00ff88;
    --border: #3a4451;
    --shadow: rgba(0,212,255,0.2);
    --success: #00ff88;
    --warning: #ffb347;
    --danger: #ff6b6b;
    --info: #00d4ff;
    --text-accent-rgb: 0, 255, 136;
    --bg-accent-rgb: 42, 52, 65;
}

/* Reset et base */
* {
    margin: 0;
    padding: 0;
    box-sizing: border-box;
}

body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    background-color: var(--bg-primary);
    color: var(--text-primary);
    transition: all 0.3s ease;
    height: 100vh;
    overflow: hidden;
}

kbd {
    background: rgba(30, 131, 92, 0.15);
    color: var(--text-color);
    padding: 2px 6px;
    border-radius: 3px;
    font-family: monospace;
    font-size: 0.85em;
    border: 1px solid rgba(255, 255, 255, 0.2);
    box-shadow: 0 1px 2px rgba(0, 0, 0, 0.2);
}

/* Animations */
@keyframes pulse {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.5; }
}

@keyframes blink {
    0%, 50% { opacity: 1; }
    51%, 100% { opacity: 0.3; }
}

@keyframes slideIn {
    from {
        opacity: 0;
        transform: translateY(10px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

@keyframes fadeIn {
    from { opacity: 0; }
    to { opacity: 1; }
}

@keyframes slideInScale {
    from {
        opacity: 0;
        transform: scale(0.9) translateY(-20px);
    }
    to {
        opacity: 1;
        transform: scale(1) translateY(0);
    }
}

@keyframes flash {
    0% { opacity: 0; }
    50% { opacity: 1; }
    100% { opacity: 0; }
}

/* Scrollbars personnalis√©es */
::-webkit-scrollbar {
    width: 8px;
}

::-webkit-scrollbar-track {
    background: var(--bg-secondary);
}

::-webkit-scrollbar-thumb {
    background: var(--border);
    border-radius: 4px;
}

::-webkit-scrollbar-thumb:hover {
    background: var(--text-secondary);
}


==================================================
FICHIER: .\web_interface\styles\components.css
==================================================

/* Boutons de navigation */
.nav-btn {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    padding: 0.5rem 1rem;
    background: var(--bg-accent);
    border: 1px solid var(--border);
    border-radius: 8px;
    color: var(--text-primary);
    cursor: pointer;
    transition: all 0.2s ease;
    font-size: 0.9rem;
}

.nav-btn:hover {
    background: var(--text-accent);
    color: var(--bg-primary);
    transform: translateY(-1px);
}

.icon {
    font-size: 1.1rem;
}

/* Boutons de contr√¥le */
.control-btn {
    padding: 0.5rem;
    background: transparent;
    border: 1px solid var(--border);
    border-radius: 6px;
    color: var(--text-secondary);
    cursor: pointer;
    transition: all 0.2s ease;
}

.control-btn:hover {
    background: var(--bg-primary);
    color: var(--text-primary);
}

/* Modals */
.modal {
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0, 0, 0, 0.5);
    display: none;
    justify-content: center;
    align-items: center;
    z-index: 2000;
    animation: fadeIn 0.3s ease;
}

.modal.show {
    display: flex;
}

.modal-content {
    background: var(--bg-primary);
    border-radius: 12px;
    border: 1px solid var(--border);
    width: 90%;
    max-width: 600px;
    max-height: 80vh;
    display: flex;
    flex-direction: column;
    animation: slideInScale 0.3s ease;
}

.modal-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1.5rem;
    border-bottom: 1px solid var(--border);
    background: var(--bg-secondary);
    border-radius: 12px 12px 0 0;
}

.modal-header h2 {
    color: var(--text-accent);
    font-size: 1.3rem;
}

.modal-close {
    background: transparent;
    border: none;
    color: var(--text-secondary);
    cursor: pointer;
    font-size: 1.5rem;
    padding: 0.25rem;
    border-radius: 4px;
    transition: all 0.2s ease;
}

.modal-close:hover {
    background: var(--bg-accent);
    color: var(--text-primary);
}

.modal-body {
    flex: 1;
    overflow-y: auto;
    padding: 1.5rem;
}

.modal-footer {
    display: flex;
    justify-content: flex-end;
    gap: 1rem;
    padding: 1.5rem;
    border-top: 1px solid var(--border);
    background: var(--bg-secondary);
}

/* Inputs g√©n√©raux */
.setting-group input,
.setting-group textarea,
.setting-group select {
    width: 100%;
    padding: 0.75rem;
    border: 1px solid var(--border);
    border-radius: 6px;
    background: var(--bg-secondary);
    color: var(--text-primary);
    font-size: 0.9rem;
}

.setting-group input[type="range"] {
    width: calc(100% - 60px);
    margin-right: 10px;
}

.setting-group input[type="checkbox"] {
    width: auto;
    margin-right: 0.5rem;
}

/* Boutons g√©n√©raux */
.btn-primary {
    background: var(--text-accent);
    color: white;
    border: none;
    padding: 0.75rem 1.5rem;
    border-radius: 6px;
    cursor: pointer;
    font-weight: 500;
    transition: all 0.2s ease;
}

.btn-primary:hover {
    background: #0b5ed7;
    transform: translateY(-1px);
}

.btn-secondary {
    background: var(--bg-accent);
    color: var(--text-primary);
    border: 1px solid var(--border);
    padding: 0.75rem 1.5rem;
    border-radius: 6px;
    cursor: pointer;
    font-weight: 500;
    transition: all 0.2s ease;
}

.btn-secondary:hover {
    background: var(--bg-secondary);
}

.test-btn {
    background: var(--success);
    color: white;
    border: none;
    padding: 0.5rem 1rem;
    border-radius: 6px;
    cursor: pointer;
    font-size: 0.9rem;
    transition: all 0.2s ease;
}

.test-btn:hover {
    background: #157347;
}

/* Form groups */
.form-group {
    margin-bottom: 20px;
}

.form-group label {
    display: block;
    margin-bottom: 8px;
    font-weight: 600;
    color: var(--text-primary);
}

.form-group input,
.form-group textarea {
    width: 100%;
    padding: 12px;
    background: var(--bg-secondary);
    border: 2px solid var(--border-color);
    border-radius: 8px;
    color: var(--text-primary);
    font-size: 14px;
    transition: all 0.3s;
}

.form-group input:focus,
.form-group textarea:focus {
    outline: none;
    border-color: var(--accent-color);
    box-shadow: 0 0 0 3px rgba(var(--accent-rgb), 0.2);
}

/* Variantes de boutons suppl√©mentaires */
.btn-primary, .btn-success, .btn-danger, .btn-secondary {
    padding: 12px 24px;
    border: none;
    border-radius: 8px;
    font-size: 16px;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.3s;
}

.btn-primary {
    background: var(--accent-color);
    color: white;
}

.btn-success {
    background: #4caf50;
    color: white;
}

.btn-danger {
    background: #f44336;
    color: white;
}

.btn-secondary {
    background: var(--bg-secondary);
    color: var(--text-primary);
}

.btn-success:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

/* Stats g√©n√©riques */
.stat-item {
    text-align: center;
}

.stat-label {
    display: block;
    font-size: 12px;
    color: var(--text-secondary);
    margin-bottom: 5px;
}

.stat-value {
    display: block;
    font-size: 20px;
    font-weight: bold;
    color: var(--accent-color);
}

/* Cards g√©n√©riques */
.audio-preview-card,
.file-info-card {
    background: rgba(var(--accent-rgb), 0.1);
    border: 2px solid var(--accent-color);
    padding: 20px;
    border-radius: 10px;
    margin-top: 15px;
}

.no-voices {
    text-align: center;
    padding: 40px 20px;
    color: var(--text-secondary);
}

.no-voices p {
    font-size: 18px;
    margin-bottom: 10px;
}

.xtts-info {
    background: var(--bg-primary);
    padding: 20px;
    border-radius: 12px;
    margin-top: 30px;
}

.xtts-info summary {
    cursor: pointer;
    font-weight: 600;
    padding: 10px;
}

.info-content {
    padding: 20px 10px 10px;
}

/* Dev status */
.dev-status {
    background: linear-gradient(135deg, rgba(255, 193, 7, 0.1), rgba(255, 152, 0, 0.1));
    border: 2px dashed #ffc107;
    border-radius: 12px;
    padding: 20px;
    margin: 20px 0;
}

.feature-roadmap {
    margin-top: 20px;
}

.feature-section {
    margin: 15px 0;
    padding: 15px;
    background: var(--bg-primary);
    border-radius: 8px;
}

.future-btn {
    opacity: 0.5;
    cursor: not-allowed;
    background: #555 !important;
}

/* File upload */
.file-upload-label {
    display: inline-block;
    padding: 12px 24px;
    background: var(--bg-primary);
    border: 2px dashed var(--border);
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.3s;
}

.file-upload-label:hover {
    background: var(--bg-secondary);
    border-color: var(--accent-color);
}

/* Form actions */
.form-actions {
    display: flex;
    gap: 10px;
    margin-top: 20px;
}

/* Recording indicator */
.recording-indicator {
    width: 20px;
    height: 20px;
    background: #ff4444;
    border-radius: 50%;
    opacity: 0;
    display: inline-block;
    margin-left: 10px;
}

.recording-indicator.active {
    animation: pulse 1s infinite;
}

.recording-timer {
    font-family: 'Courier New', monospace;
    font-size: 16px;
    font-weight: bold;
}

/* Panels content */
.panel-content {
    flex: 1;
    overflow-y: auto;
    padding: 20px;
}


==================================================
FICHIER: .\web_interface\styles\index.css
==================================================

/* ===========================
   Main CSS - Import all modular styles
   =========================== */

/* 1. Base styles (reset, variables, typography) */
@import url('./base.css');

/* 2. Layout and grid structure */
@import url('./layout.css');

/* 3. Reusable components */
@import url('./components.css');

/* 4. Panel-specific styles */
@import url('./panels/dialogue.css');
@import url('./panels/voice.css');
@import url('./panels/camera.css');
@import url('./panels/debug.css');
@import url('./panels/settings.css');

/* 
   Structure CSS modulaire:
   
   styles/
   ‚îú‚îÄ‚îÄ index.css       # Fichier principal (ce fichier)
   ‚îú‚îÄ‚îÄ base.css        # Reset, variables, typographie
   ‚îú‚îÄ‚îÄ layout.css      # Grille, structure
   ‚îú‚îÄ‚îÄ components.css  # Composants r√©utilisables
   ‚îú‚îÄ‚îÄ panels/
   ‚îÇ   ‚îú‚îÄ‚îÄ voice.css     # Panel vocal
   ‚îÇ   ‚îú‚îÄ‚îÄ camera.css    # Panel cam√©ra
   ‚îÇ   ‚îî‚îÄ‚îÄ settings.css  # Panel param√®tres
   ‚îî‚îÄ‚îÄ themes/
       ‚îú‚îÄ‚îÄ dark.css    # Th√®me sombre
       ‚îî‚îÄ‚îÄ light.css   # Th√®me clair + Jarvis
       
   Pour utiliser un th√®me sp√©cifique, ajoutez la classe correspondante
   au body ou √† l'√©l√©ment racine:
   - .theme-light (par d√©faut)
   - .theme-dark
   - .theme-jarvis
*/


==================================================
FICHIER: .\web_interface\styles\layout.css
==================================================

/* Header */
.header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1rem 2rem;
    background-color: var(--bg-secondary);
    border-bottom: 1px solid var(--border);
    box-shadow: 0 2px 4px var(--shadow);
    z-index: 1000;
}

.header-left {
    display: flex;
    align-items: center;
}

.logo {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    font-size: 1.5rem;
    font-weight: bold;
    color: var(--text-accent);
}

.logo-icon {
    font-size: 2rem;
    animation: pulse 2s infinite;
}

.status-indicator {
    font-size: 0.8rem;
    margin-left: 0.5rem;
    color: var(--success);
    animation: blink 1.5s infinite;
}

.status-indicator.offline {
    color: var(--danger);
}

.header-nav {
    display: flex;
    gap: 1rem;
}

/* Layout principal */
.main-content {
    display: grid;
    grid-template-columns: 1fr 350px;
    grid-template-rows: 1fr auto;
    height: calc(100vh - 80px);
    gap: 1rem;
    padding: 1rem;
    position: relative;
}

.main-content.voice-hidden,
.main-content.camera-hidden,
.main-content.debug-hidden {
    grid-template-columns: 1fr;
}

.voice-section:not(.hidden),
.camera-section:not(.hidden),
.debug-section:not(.hidden) {
    position: relative;
    z-index: 1;
}


/* Responsive */
@media (max-width: 768px) {
    .main-content {
        grid-template-columns: 1fr;
        grid-template-rows: 1fr auto 300px;
    }
    
    .debug-section {
        grid-row: 3;
    }
    
    .header {
        padding: 0.75rem 1rem;
    }
    
    .header-nav {
        gap: 0.5rem;
    }
    
    .nav-btn {
        padding: 0.5rem;
        font-size: 0.8rem;
    }
    
    .nav-btn span:not(.icon) {
        display: none;
    }
    
    .modal-content {
        width: 95%;
        margin: 1rem;
    }
}


==================================================
FICHIER: .\web_interface\styles\panels\camera.css
==================================================

.camera-section {
    grid-row: 1;
    grid-column: 3; 
    background: var(--bg-secondary);
    border-radius: 12px;
    border: 1px solid var(--border);
    display: flex;
    flex-direction: column;
    overflow: hidden;
    transition: all 0.3s ease;
}

.camera-section.hidden {
    display: none;
}

.camera-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1rem;
    background: var(--bg-accent);
    border-bottom: 1px solid var(--border);
}

.camera-header h3 {
    font-size: 1.1rem;
    color: var(--text-accent);
}

.close-camera {
    background: transparent;
    border: none;
    color: var(--text-secondary);
    cursor: pointer;
    font-size: 1.2rem;
    padding: 0.25rem;
    border-radius: 4px;
    transition: all 0.2s ease;
}

.close-camera:hover {
    background: var(--bg-primary);
    color: var(--text-primary);
}

/* Camera Panel */
#camera-feed {
    width: 100%;
    height: auto;
    border-radius: 12px;
    background: #000;
}

.camera-container {
    position: relative;
    margin-bottom: 20px;
}

.camera-flash {
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: white;
    opacity: 0;
    animation: flash 0.3s;
    pointer-events: none;
}

.captured-images-grid {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(100px, 1fr));
    gap: 10px;
    margin: 20px 0;
}

.captured-image-card {
    position: relative;
    border-radius: 8px;
    overflow: hidden;
}

.captured-image-card img {
    width: 100%;
    height: auto;
    display: block;
}

.image-overlay {
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background: rgba(0,0,0,0.7);
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 5px;
    opacity: 0;
    transition: opacity 0.2s;
}

.captured-image-card:hover .image-overlay {
    opacity: 1;
}


==================================================
FICHIER: .\web_interface\styles\panels\debug.css
==================================================

/* Section debug */
.debug-section {
    grid-row: 1;
    grid-column: 2; 
    background: var(--bg-secondary);
    border-radius: 12px;
    border: 1px solid var(--border);
    display: flex;
    flex-direction: column;
    overflow: hidden;
    transition: all 0.3s ease;
}

.debug-section.hidden {
    display: none;
}

.debug-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1rem;
    background: var(--bg-accent);
    border-bottom: 1px solid var(--border);
}

.debug-header h3 {
    font-size: 1.1rem;
    color: var(--text-accent);
}

.close-debug {
    background: transparent;
    border: none;
    color: var(--text-secondary);
    cursor: pointer;
    font-size: 1.2rem;
    padding: 0.25rem;
    border-radius: 4px;
    transition: all 0.2s ease;
}

.close-debug:hover {
    background: var(--bg-primary);
    color: var(--text-primary);
}

.debug-tabs {
    display: flex;
    background: var(--bg-primary);
    border-bottom: 1px solid var(--border);
}

.tab-btn {
    flex: 1;
    padding: 0.75rem;
    background: transparent;
    border: none;
    color: var(--text-secondary);
    cursor: pointer;
    transition: all 0.2s ease;
    font-size: 0.9rem;
}

.tab-btn.active {
    background: var(--bg-accent);
    color: var(--text-accent);
    border-bottom: 2px solid var(--text-accent);
}

.tab-btn:hover:not(.active) {
    background: var(--bg-secondary);
    color: var(--text-primary);
}

.debug-content {
    flex: 1;
    overflow: hidden;
}

.debug-tab {
    height: 100%;
    overflow-y: auto;
    padding: 1rem;
    display: none;
}

.debug-tab.active {
    display: block;
}

/* Logs */
.log-container {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}

.log-entry {
    display: flex;
    align-items: flex-start;
    gap: 0.5rem;
    padding: 0.5rem;
    border-radius: 6px;
    font-size: 0.8rem;
    line-height: 1.3;
}

.log-entry.info {
    background: rgba(13, 202, 240, 0.1);
    border-left: 3px solid var(--info);
}

.log-entry.success {
    background: rgba(25, 135, 84, 0.1);
    border-left: 3px solid var(--success);
}

.log-entry.warning {
    background: rgba(255, 193, 7, 0.1);
    border-left: 3px solid var(--warning);
}

.log-entry.error {
    background: rgba(220, 53, 69, 0.1);
    border-left: 3px solid var(--danger);
}

.log-time {
    color: var(--text-secondary);
    font-weight: 500;
    min-width: 60px;
}

/* Stats */
.stats-grid {
    display: grid;
    grid-template-columns: 1fr 1fr;
    gap: 1rem;
}

.stat-card {
    background: var(--bg-accent);
    padding: 1rem;
    border-radius: 8px;
    text-align: center;
    border: 1px solid var(--border);
}

.stat-value {
    font-size: 1.5rem;
    font-weight: bold;
    color: var(--text-accent);
    margin-bottom: 0.25rem;
}

.stat-label {
    font-size: 0.8rem;
    color: var(--text-secondary);
}

/* Config */
.config-item {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 0.75rem 0;
    border-bottom: 1px solid var(--border);
}

.config-item:last-child {
    border-bottom: none;
}

.config-item label {
    font-weight: 500;
    color: var(--text-secondary);
}

.config-item span {
    color: var(--text-primary);
    background: var(--bg-accent);
    padding: 0.25rem 0.5rem;
    border-radius: 4px;
    font-size: 0.9rem;
}


==================================================
FICHIER: .\web_interface\styles\panels\dialogue.css
==================================================

/* ‚úÖ SOLUTION: Seulement les enfants directs de main-content */
.main-content > .dialogue-section {
    grid-row: 1;
    grid-column: 1; 
    display: flex;
    flex-direction: column;
    background: var(--bg-secondary);
    border-radius: 12px;
    border: 1px solid var(--border);
    overflow: hidden;
}

/* Section dialogue interne (celle qui contient les messages) */
.dialogue-section .dialogue-section {
    flex: 1; /* Prend tout l'espace disponible */
    display: flex;
    flex-direction: column;
    background: transparent;
    border: none;
    border-radius: 0;
    overflow: hidden;
}

/* üöÄ CORRIG√â - Arri√®re-plan sur la zone de dialogue (avec custom property) */
.dialogue-section.bg-image {
    position: relative;
    /* L'image sera affich√©e via le pseudo-√©l√©ment ::before avec opacit√© contr√¥l√©e */
}

/* üöÄ CORRIG√â - Pseudo-√©l√©ment pour l'image avec opacit√© par d√©faut */
.dialogue-section.bg-image::before {
    content: '';
    position: absolute;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-image: var(--bg-image-url);
    background-size: cover;
    background-position: center;
    background-repeat: no-repeat;
    opacity: 1; /* Opacit√© par d√©faut, modifiable par setBackgroundOpacity */
    z-index: 1;
    pointer-events: none;
    border-radius: inherit;
}

/* üöÄ IMPORTANT - Assurer que le contenu reste au-dessus ET OPAQUE */
.dialogue-section.bg-image .dialogue-header,
.dialogue-section.bg-image .dialogue-container {
    position: relative;
    z-index: 2;
    /* Pas d'opacit√© h√©rit√©e - le contenu reste √† 100% */
}

/* ‚úÖ Messages utilisateur (bleu + texte blanc) */
.dialogue-section.bg-image .message-bubble.user {
    background-color: rgba(var(--text-accent-rgb, 13, 110, 253), 0.95);
    color: white;
    backdrop-filter: blur(2px);
    border: 1px solid rgba(var(--text-accent-rgb, 13, 110, 253), 0.8);
}

/* ‚úÖ Messages assistant (gris + texte noir/blanc selon th√®me) */
.dialogue-section.bg-image .message-bubble.assistant {
    background-color: rgba(var(--bg-accent-rgb, 233, 236, 239), 0.95);
    color: var(--text-primary);
    backdrop-filter: blur(2px);
    border: 1px solid rgba(var(--border-rgb, 222, 226, 230), 0.5);
}

/* ‚úÖ Messages syst√®me */
.dialogue-section.bg-image .message-bubble.system {
    background-color: rgba(var(--bg-accent-rgb, 233, 236, 239), 0.9);
    color: var(--text-secondary);
    backdrop-filter: blur(2px);
}

/* Fallback pour th√®mes sans RGB variables */
.theme-dark .dialogue-section.bg-image .message-bubble {
    background-color: rgba(45, 45, 45, 0.95);
}

.theme-jarvis .dialogue-section.bg-image .message-bubble {
    background-color: rgba(26, 35, 50, 0.95);
}

.dialogue-header {
    display: grid;
    justify-content: space-between;
    align-items: center;
    padding: 1rem;
    background: var(--bg-accent);
    border-bottom: 1px solid var(--border);
}

.dialogue-header h2 {
    font-size: 1.2rem;
    color: var(--text-accent);
}

.dialogue-controls {
    display: flex;
    gap: 0.5rem;
}

.dialogue-container {
    flex: 1;
    padding: 1rem;
    overflow-y: auto;
    display: flex;
    flex-direction: column;
    gap: 1rem;
    /* üöÄ IMPORTANT - Pas de filter ou opacity ici ! */
}

/* Section saisie */
.input-section {
    grid-column: 1;
    grid-row: 2;
    background: var(--bg-secondary);
    border-radius: 12px;
    border: 1px solid var(--border);
    padding: 1rem;
}

.input-container {
    display: grid;
}

.input-row {
    gap: 1rem;
    display: flex;
}

.input-buttons {
    display: flex;
    flex-direction: column;
    gap: 0.5rem;
}

/* Messages */
.message-bubble {
    max-width: 80%;
    padding: 0.75rem 1rem;
    border-radius: 18px;
    animation: slideIn 0.3s ease;
}

.message-bubble.user {
    align-self: flex-end;
    background: var(--text-accent);
    color: var(--bg-primary);
    border-bottom-right-radius: 6px;
}

.message-bubble.assistant {
    align-self: flex-start;
    background: var(--bg-accent);
    color: var(--text-primary);
    border-bottom-left-radius: 6px;
}

.message-bubble.system {
    align-self: center;
    background: var(--bg-accent);
    color: var(--text-secondary);
    font-style: italic;
    text-align: center;
    border-radius: 12px;
}

.message-content {
    line-height: 1.4;
}

.message-time {
    font-size: 0.7rem;
    opacity: 0.6;
    margin-top: 0.25rem;
}

/* Input de message */
#message-input {
    flex: 1;
    padding: 0.75rem;
    border: 1px solid var(--border);
    border-radius: 8px;
    background: var(--bg-primary);
    color: var(--text-primary);
    resize: vertical;
    font-family: inherit;
    font-size: 0.9rem;
    transition: border-color 0.2s ease;
}

#message-input:focus {
    outline: none;
    border-color: var(--text-accent);
    box-shadow: 0 0 0 2px rgba(13, 110, 253, 0.25);
}

/* Boutons microphone et envoi */
.mic-button {
    display: flex;
    align-items: center;
    gap: 0.5rem;
    padding: 0.75rem 1rem;
    background: var(--success);
    color: white;
    border: none;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.2s ease;
    font-weight: 500;
}

.mic-button:hover {
    background: #157347;
    transform: scale(1.02);
}

.mic-button.active {
    background: var(--danger);
    animation: pulse 1s infinite;
}

.send-button {
    padding: 0.75rem 1rem;
    background: var(--text-accent);
    color: white;
    border: none;
    border-radius: 8px;
    cursor: pointer;
    transition: all 0.2s ease;
    font-size: 1.2rem;
    font-weight: bold;
}

.send-button:hover {
    background: #0b5ed7;
    transform: scale(1.02);
}

==================================================
FICHIER: .\web_interface\styles\panels\settings.css
==================================================

/* Settings */
.settings-tabs {
    display: flex;
    margin-bottom: 1.5rem;
    background: var(--bg-secondary);
    border-radius: 8px;
    padding: 0.25rem;
}

.settings-tab {
    display: none;
}

.settings-tab.active {
    display: block;
}

.setting-group {
    margin-bottom: 1.5rem;
}

.setting-group label {
    display: block;
    margin-bottom: 0.5rem;
    font-weight: 500;
    color: var(--text-primary);
}

/* Help */
.help-section {
    margin-bottom: 1.5rem;
    padding-bottom: 1rem;
    border-bottom: 1px solid var(--border);
}

.help-section:last-child {
    border-bottom: none;
}

.help-section h3 {
    color: var(--text-accent);
    margin-bottom: 0.5rem;
}

.help-section p {
    color: var(--text-secondary);
    line-height: 1.5;
}


==================================================
FICHIER: .\web_interface\styles\panels\voice.css
==================================================

.voice-section {
    grid-row: 1;
    grid-column: 2; 
    background: var(--bg-secondary);
    border-radius: 12px;
    border: 1px solid var(--border);
    display: flex;
    flex-direction: column;
    overflow: hidden;
    transition: all 0.3s ease;
    max-height: 90vh;  /* Augmenter la hauteur */
    overflow-y: auto;  /* Activer le scrolling */
}

/* Zone de liste des voix */
#cloned-voices-list {
    max-height: 400px;  /* Augmenter de 200px √† 400px */
    overflow-y: auto;
    padding: 10px;
}

.voice-section.hidden {
    display: none;
}

.voice-header {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 1rem;
    background: var(--bg-accent);
    border-bottom: 1px solid var(--border);
}

.voice-header h3 {
    font-size: 1.1rem;
    color: var(--text-accent);
}

.close-voice {
    background: transparent;
    border: none;
    color: var(--text-secondary);
    cursor: pointer;
    font-size: 1.2rem;
    padding: 0.25rem;
    border-radius: 4px;
    transition: all 0.2s ease;
}

.close-voice:hover {
    background: var(--bg-primary);
    color: var(--text-primary);
}

/* Recording section */
.recording-section {
    background: var(--bg-secondary);
    padding: 20px;
    border-radius: 10px;
    margin: 20px 0;
}

.recording-controls {
    display: flex;
    align-items: center;
    gap: 15px;
    margin-bottom: 20px;
}

/* Voice Lab */
.voice-clone-form {
    background: var(--bg-primary);
    padding: 20px;
    border-radius: 12px;
    margin-bottom: 20px;
}

/* Voice cards - premi√®re version */
.voice-card {
    background: var(--bg-primary);
    padding: 20px;
    border-radius: 12px;
    margin-bottom: 15px;
    display: flex;
    justify-content: space-between;
    align-items: center;
    transition: all 0.3s;
    border: 2px solid transparent;
}

.voice-card:hover {
    transform: translateX(-5px);
    box-shadow: 0 6px 20px rgba(0,0,0,0.15);
}

.voice-card.selected {
    border-color: var(--accent-color);
    background: rgba(var(--accent-rgb), 0.1);
}

.voice-info h4 {
    margin: 0 0 8px 0;
    font-size: 18px;
}

.voice-info p {
    margin: 0 0 8px 0;
    color: var(--text-secondary);
    font-size: 14px;
}

.voice-meta {
    display: flex;
    gap: 15px;
    font-size: 13px;
    color: var(--text-secondary);
}

.voice-actions {
    display: flex;
    gap: 8px;
}

.voice-btn {
    width: 40px;
    height: 40px;
    border: none;
    border-radius: 8px;
    cursor: pointer;
    font-size: 20px;
    transition: all 0.2s;
    background: var(--bg-secondary);
}

.voice-btn:hover:not(:disabled) {
    transform: scale(1.1);
    background: var(--accent-color);
}

.voice-btn:disabled {
    opacity: 0.3;
    cursor: not-allowed;
}

.badge-active {
    background: var(--accent-color);
    color: white;
    padding: 2px 8px;
    border-radius: 12px;
    font-size: 11px;
    margin-left: 10px;
}

.voice-stats-section {
    background: var(--bg-primary);
    padding: 20px;
    border-radius: 12px;
    margin-bottom: 20px;
}

.stats-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(100px, 1fr));
    gap: 15px;
}

/* Voice cards - deuxi√®me version (du fichier original) */
.voice-card {
    background: var(--bg-primary);
    padding: 15px;
    border-radius: 10px;
    margin-bottom: 12px;
    display: flex;
    justify-content: space-between;
    align-items: center;
    transition: all 0.2s;
}

.voice-card:hover {
    transform: translateX(-5px);
    box-shadow: 0 4px 12px rgba(0,0,0,0.2);
}

.voice-card.selected {
    border: 2px solid var(--accent-color);
}

.voice-actions {
    display: flex;
    gap: 8px;
}

.voice-btn {
    width: 35px;
    height: 35px;
    border: none;
    border-radius: 8px;
    cursor: pointer;
    font-size: 18px;
    transition: all 0.2s;
}

