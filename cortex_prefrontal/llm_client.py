"""
Client LLM  pour Jarvis avec streaming natif
Avec support streaming web et CMD
"""

import ollama
import yaml
from pathlib import Path
from hypothalamus.logger import log


class JarvisLLM:
    """LLM Jarvis unifi√© avec support streaming natif"""
    
    def __init__(self, personality="Jarvis"):
        # Charger config
        config_path = Path(__file__).parent.parent / "config/settings.yaml"
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        self.model = self.config['llm']['model']
        self.personality = personality
        self.conversation_history = []
        self._initialize_system_prompt()
        self._warmup_model()
        
        log.success(f"LLM pr√™t ({self.model}) - Mode: {personality}", "üß†")

    def _warmup_model(self):
        """Envoie une requ√™te silencieuse pour charger le mod√®le en m√©moire."""
        try:
            log.info(f"üî• Pr√©chauffage du mod√®le LLM: {self.model}...")
            ollama.generate(model=self.model, prompt=".", options={"num_predict": 1}, keep_alive=0)
            log.success(f"‚úÖ Mod√®le {self.model} pr√©chauff√©.")
        except Exception as e:
            log.error(f"‚ùå √âchec du pr√©chauffage du mod√®le {self.model}: {e}")

    def _initialize_system_prompt(self):
        if self.personality == "Jarvis":
            assistant_desc = (
                "Tu es Jarvis, un assistant fran√ßais intelligent, pr√©cis et un peu ironique. "
                "R√©ponds toujours en fran√ßais, de fa√ßon claire, naturelle et concise."
            )
        else:
            assistant_desc = (
                "Tu es Samantha, une assistante fran√ßaise douce, empathique et professionnelle. "
                "R√©ponds toujours en fran√ßais, de fa√ßon fluide, naturelle et concise."
            )

        self.conversation_history = [{'role': 'system', 'content': assistant_desc}]

    def estimate_complexity(self, text: str) -> str:
        """Analyse simple de la complexit√© (mots-cl√©s + longueur)"""
        text_lower = text.lower()
        word_count = len(text_lower.split())

        expert_keywords = [
            "analyse", "explique", "pourquoi", "comment", "comparer", "th√©orie",
            "concept", "quantique", "algorithme", "d√©veloppe", "projet", "plan",
            "fonctionnement", "m√©canisme", "histoire", "impact", "diff√©rence"
        ]

        simple_keywords = [
            "salut", "bonjour", "heure", "merci", "date",
            "temp√©rature", "m√©t√©o", "au revoir"
        ]

        # Cas simples : social / commande
        if any(k in text_lower for k in simple_keywords):
            return "Express"

        # Cas complexes : question profonde / notion avanc√©e
        if any(k in text_lower for k in expert_keywords):
            return "Expert"

        # Sinon on se base sur la longueur
        if word_count <= 8:
            return "Express"
        elif word_count <= 30:
            return "Standard"
        else:
            return "Expert"

    def generate_response_stream(self, user_input: str):
        """
        üî• STREAMING NATIF avec CONTEXTE - Yield les tokens un par un depuis Ollama
        Utilise ollama.chat pour maintenir l'historique.
        """
        try:
            # Ajouter le message de l'utilisateur √† l'historique
            self.conversation_history.append({'role': 'user', 'content': user_input})

            log.debug("D√©marrage streaming Ollama avec contexte...")
            
            # Utiliser ollama.chat pour le streaming avec historique
            stream = ollama.chat(
                model=self.model,
                messages=self.conversation_history,
                stream=True
            )
            
            assistant_response = ""
            for chunk in stream:
                token = chunk['message']['content']
                if token:
                    assistant_response += token
                    yield token
            
            # Ajouter la r√©ponse compl√®te de l'assistant √† l'historique
            self.conversation_history.append({'role': 'assistant', 'content': assistant_response})
            log.debug("Streaming termin√© et contexte mis √† jour.")

        except Exception as e:
            log.error(f"Erreur streaming Ollama: {e}")
            yield "D√©sol√©, une erreur est survenue pendant la r√©ponse."

    def generate_response(self, user_input: str) -> str:
        """
        M√©thode de compatibilit√© (non-streaming)
        R√©cup√®re tout le stream et le joint pour retourner une string compl√®te
        Utilis√© pour compatibilit√© avec ancien code ou usage simple
        """
        # R√©cup√©rer tout le stream et le joindre
        tokens = list(self.generate_response_stream(user_input))
        return ''.join(tokens)

    def ask(self, user_input: str) -> str:
        """M√©thode courte (compatibilit√©)"""
        return self.generate_response(user_input)

    def change_model(self, new_model: str):
        """Change le mod√®le LLM √† la vol√©e, r√©initialise l'historique et pr√©chauffe le nouveau mod√®le."""
        old_model = self.model
        self.model = new_model
        self.clear_history()
        self._warmup_model()
        log.info(f"üîÑ Mod√®le chang√©: {old_model} ‚Üí {new_model}. L'historique de la conversation a √©t√© r√©initialis√©.")
        return True

    def clear_history(self):
        """R√©initialise l'historique de la conversation en ne gardant que le prompt syst√®me."""
        self._initialize_system_prompt()
        log.info("Historique de la conversation LLM r√©initialis√©.")

    def get_current_model(self) -> str:
        """Retourne le mod√®le actuellement utilis√©"""
        return self.model

# Test standalone
if __name__ == "__main__":
    print("üß™ Test LLM Unifi√©")
    
    try:
        llm = JarvisLLM("Samantha")
        
        print("\nüî• Test streaming:")
        print("Question: Raconte-moi une blague")
        print("R√©ponse: ", end="", flush=True)
        
        for token in llm.generate_response_stream("Raconte-moi une blague"):
            print(token, end="", flush=True)
        
        print("\n\n‚úÖ Test termin√©")
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")