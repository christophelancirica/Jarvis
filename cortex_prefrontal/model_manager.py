"""
model_manager.py - Gestionnaire des mod√®les LLM avec Ollama
Responsabilit√© : Installation, changement, et v√©rification des mod√®les
Adapt√© pour l'architecture plate de Jarvis
"""

import ollama
import json
import asyncio
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Callable
from hypothalamus.logger import log


class ModelManager:
    """Gestionnaire des mod√®les LLM pour Ollama"""
    
    def __init__(self, models_config_path: str = "models.json"):
        self.config_path = Path(__file__).parent.parent / "config/models.json"
        self.current_model = None
        self.download_callbacks = {}  # Pour les callbacks de progression
        
    def load_available_models(self) -> Dict:
        """Charge la liste des mod√®les disponibles depuis la config"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            log.error(f"Erreur chargement config mod√®les: {e}")
            return {"llm_models": {}, "config": {"default_model": "llama3.1:8b"}}
    
    def get_installed_models(self) -> List[str]:
        """R√©cup√®re la liste des mod√®les install√©s dans Ollama"""
        try:
            models = ollama.list()
            result = []
            for model in models.get('models', []):
                # Essayer plusieurs champs possibles
                model_name = model.get('name') or model.get('model') or str(model)
                if model_name:
                    result.append(model_name)
            return result
        except Exception as e:
            log.error(f"Erreur r√©cup√©ration mod√®les install√©s: {e}")
            return []
    
    def get_model_status(self) -> Dict:
        """Retourne le statut de tous les mod√®les (install√©/non install√©)"""
        config = self.load_available_models()
        installed = self.get_installed_models()
        
        # Si pas de mod√®le actuel d√©fini, prendre celui par d√©faut s'il est install√©
        if not self.current_model and installed:
            default_model = config.get('config', {}).get('default_model')
            if default_model and default_model in installed:
                self.current_model = default_model
        
        status = {}
        for model_id, model_info in config['llm_models'].items():
            status[model_id] = {
                **model_info,
                'installed': model_id in installed,
                'current': model_id == self.current_model
            }
        
        return {
            'models': status,
            'current_model': self.current_model,
            'installed_count': len(installed)
        }
    
    def is_model_available(self, model_id: str) -> bool:
        """V√©rifie si un mod√®le est install√© dans Ollama"""
        installed = self.get_installed_models()
        return model_id in installed
    
    async def download_model(self, model_id: str, progress_callback: Optional[Callable] = None) -> bool:
        """
        T√©l√©charge un mod√®le avec Ollama (asynchrone)
        
        Args:
            model_id: ID du mod√®le √† t√©l√©charger
            progress_callback: Fonction appel√©e pour les updates de progression
        
        Returns:
            bool: Succ√®s du t√©l√©chargement
        """
        try:
            config = self.load_available_models()
            if model_id not in config['llm_models']:
                log.error(f"Mod√®le {model_id} non trouv√© dans la config")
                return False
            
            install_command = config['llm_models'][model_id].get('install_command')
            if not install_command:
                log.error(f"Commande d'installation manquante pour {model_id}")
                return False
            
            log.info(f"üì• D√©but t√©l√©chargement {model_id}...")
            if progress_callback:
                progress_callback({"status": "starting", "model": model_id})
            
            # Lancer ollama pull de mani√®re asynchrone
            process = await asyncio.create_subprocess_shell(
                install_command,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            # Monitorer la progression (basique)
            while True:
                line = await process.stdout.readline()
                if not line:
                    break
                
                line_str = line.decode().strip()
                log.debug(f"Ollama pull: {line_str}")
                
                if progress_callback:
                    # Parsing basique de la progression d'Ollama
                    if "pulling" in line_str.lower():
                        progress_callback({
                            "status": "downloading", 
                            "model": model_id, 
                            "message": line_str
                        })
                    elif "verifying" in line_str.lower():
                        progress_callback({
                            "status": "verifying", 
                            "model": model_id, 
                            "message": line_str
                        })
            
            # Attendre la fin du processus
            await process.wait()
            
            if process.returncode == 0:
                log.success(f"‚úÖ Mod√®le {model_id} t√©l√©charg√© avec succ√®s")
                if progress_callback:
                    progress_callback({"status": "completed", "model": model_id})
                return True
            else:
                error_output = await process.stderr.read()
                log.error(f"‚ùå √âchec t√©l√©chargement {model_id}: {error_output.decode()}")
                if progress_callback:
                    progress_callback({
                        "status": "error", 
                        "model": model_id, 
                        "error": error_output.decode()
                    })
                return False
                
        except Exception as e:
            log.error(f"Erreur t√©l√©chargement {model_id}: {e}")
            if progress_callback:
                progress_callback({"status": "error", "model": model_id, "error": str(e)})
            return False
    
    def set_current_model(self, model_id: str) -> bool:
        """
        Change le mod√®le actuel (sans red√©marrer tout le syst√®me)
        
        Args:
            model_id: ID du nouveau mod√®le
            
        Returns:
            bool: Succ√®s du changement
        """
        try:
            # V√©rifier que le mod√®le est install√©
            if not self.is_model_available(model_id):
                log.warning(f"Mod√®le {model_id} non install√©, impossible de basculer")
                return False
            
            # Test rapide du mod√®le
            try:
                test_response = ollama.generate(
                    model=model_id,
                    prompt="Test",
                    options={"num_predict": 1}
                )
                if not test_response:
                    log.error(f"Test du mod√®le {model_id} √©chou√©")
                    return False
            except Exception as e:
                log.error(f"Erreur test mod√®le {model_id}: {e}")
                return False
            
            # Mettre √† jour le mod√®le actuel
            self.current_model = model_id
            log.success(f"‚úÖ Mod√®le bascul√© vers {model_id}")
            return True
            
        except Exception as e:
            log.error(f"Erreur changement mod√®le vers {model_id}: {e}")
            return False
    
    def get_current_model(self) -> Optional[str]:
        """Retourne le mod√®le actuellement utilis√©"""
        return self.current_model
    
    def update_llm_client_model(self, llm_client, model_id: str) -> bool:
        """
        Met √† jour le mod√®le d'un client LLM existant
        
        Args:
            llm_client: Instance de JarvisLLM
            model_id: Nouveau mod√®le √† utiliser
            
        Returns:
            bool: Succ√®s de la mise √† jour
        """
        try:
            if not self.is_model_available(model_id):
                log.warning(f"Mod√®le {model_id} non disponible")
                return False
            
            # Mettre √† jour le mod√®le dans le client LLM
            llm_client.model = model_id
            self.current_model = model_id
            
            log.success(f"‚úÖ Client LLM mis √† jour avec {model_id}")
            return True
            
        except Exception as e:
            log.error(f"Erreur mise √† jour client LLM: {e}")
            return False


# Test standalone
if __name__ == "__main__":
    import asyncio
    
    async def test_manager():
        manager = ModelManager()
        
        print("üìã Statut des mod√®les:")
        status = manager.get_model_status()
        for model_id, info in status['models'].items():
            installed = "‚úÖ" if info['installed'] else "‚ùå"
            print(f"  {installed} {model_id} - {info['display_name']}")
        
        print(f"\nüéØ Mod√®le actuel: {manager.get_current_model()}")
        
    asyncio.run(test_manager())