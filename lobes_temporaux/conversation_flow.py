"""
conversation_flow.py - Flux de conversation unifiÃ© (Lobes Temporaux) - VERSION FINALE
ResponsabilitÃ© : Messages, streaming LLM, STT/TTS
MODIFIÃ‰ pour supporter AudioGenerator + AudioPipeline + VoiceCloner
INCLUDES: auto_initialize(), stop(), et toutes les mÃ©thodes requises
"""

import time
import asyncio
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Callable, Optional
import sys

# Imports des modules - RÃ©utilisation modules existants
sys.path.append(str(Path(__file__).parent.parent))
from cortex_prefrontal.llm_client import JarvisLLM  # LLM unifiÃ© avec streaming
from lobes_temporaux.stt import SpeechToText  # Module local
from lobes_temporaux.tts import TextToSpeech  # Module local (maintenant avec NOUVELLE architecture)
from hypothalamus.device_manager import DeviceManager
from hypothalamus.voice_manager import VoiceManager
from hypothalamus.config_manager import ConfigManager
from hypothalamus.logger import log

class ConversationFlow:
    """Flux de conversation unifiÃ© avec vrai streaming (Lobes Temporaux)"""
    
    def __init__(self):
        # Instances des modules (RÃ‰UTILISATION maximale)
        self.llm = None  # LLM unifiÃ©
        self.stt = None  # Module local lobes_temporaux
        self.tts = None  # Module local lobes_temporaux avec NOUVELLE architecture
        
        # Configuration actuelle
        self.personality = None
        self.display_name = None
        self.is_initialized = False
        
        # Queue TTS pour streaming sÃ©quentiel (LEGACY - pour compatibilitÃ©)
        self.tts_queue = asyncio.Queue()
        self.tts_worker_running = False

        # Historique de conversation
        self.conversation_history = []

        # SystÃ¨me anti-duplication
        self.processing_lock = asyncio.Lock()
        self.recent_messages = {}  # Hash -> timestamp

        # Callback WebSocket
        self.websocket_callback: Optional[Callable] = None
        
        # Stats de session (ajout mÃ©triques pipeline)
        self.session_stats = {
            'messages_count': 0,
            'total_tokens': 0,
            'total_time': 0.0,
            'avg_response_time': 0.0,
            'avg_ttft': 0.0,
            'pipeline_efficiency': 0.0
        }
        
        log.info("ConversationFlow crÃ©Ã© (Lobes Temporaux - Pipeline parallÃ¨le)")
    
    async def auto_initialize(self) -> bool:
        """Initialisation automatique sans interaction utilisateur"""
        try:
            log.info("Initialisation automatique ConversationFlow...")
            
            # 1. Configuration microphone automatique (RÃ‰UTILISE device_manager)
            device_mgr = DeviceManager()
            saved_index, _ = device_mgr.load_saved_device()
            device_index = saved_index if saved_index and device_mgr.verify_device(saved_index)[0] else None
            
            if device_index is None:
                log.error("Aucun microphone disponible")
                return False
            
            # 2. Configuration voix automatique (RÃ‰UTILISE voice_manager)
            voice_mgr = VoiceManager()
            _, personality, tts_model, edge_voice, sample_path, embedding_path = voice_mgr.load_saved_voice()
            if not personality:  # Pas de config sauvÃ©e
                personality, tts_model, edge_voice, sample_path, embedding_path = "Samantha", "edge-tts", "fr-FR-DeniseNeural", None, None
            
            # 3. Initialiser les modules (RÃ‰UTILISATION COMPLÃˆTE)
            self.llm = JarvisLLM(personality=personality)  # Cortex prÃ©frontal
            self.stt = SpeechToText(device_index=device_index)  # Module local
            
            # NOUVEAU: Initialisation TTS avec nouvelle architecture
            try:
                # PRIORITÃ‰ 1: Nouvelle architecture
                self.tts = TextToSpeech(personality=personality)
                log.success(f"TTS nouvelle architecture initialisÃ©: {personality}")
            except Exception as tts_error:
                log.warning(f"Fallback ancienne architecture: {tts_error}")
                # PRIORITÃ‰ 2: Ancienne architecture
                if sample_path:
                    from pathlib import Path
                    sample_path_obj = Path(sample_path)
                    if not sample_path_obj.is_absolute():
                        sample_path = str(Path('config') / sample_path)
                self.tts = TextToSpeech(
                    model_name=tts_model, 
                    personality=personality, 
                    edge_voice=edge_voice,
                    sample_path=sample_path,
                    embedding_path=embedding_path
                )
                log.success(f"TTS fallback initialisÃ©: {personality}")
            
            self.personality = personality
            self.display_name = f"Assistant virtuel - {personality}"
            self.is_initialized = True
            
            log.success(f"ConversationFlow initialisÃ© - {personality} prÃªt (pipeline actif)")
            return True
            
        except Exception as e:
            log.error(f"Erreur initialisation ConversationFlow: {e}")
            return False
    
    async def initialize(self, personality: str = "Jarvis") -> bool:
        """Initialise tous les modules requis pour la conversation"""
        try:
            self.personality = personality
            
            # 1. Configuration microphone automatique (RÃ‰UTILISE device_manager)
            device_mgr = DeviceManager()
            saved_index, _ = device_mgr.load_saved_device()
            device_index = saved_index if saved_index and device_mgr.verify_device(saved_index)[0] else None
            
            if device_index is None:
                log.error("Aucun microphone disponible")
                return False
            
            # 2. Configuration voix automatique (RÃ‰UTILISE voice_manager)
            voice_mgr = VoiceManager()
            _, personality, tts_model, edge_voice, sample_path, embedding_path = voice_mgr.load_saved_voice()
            if not personality:  # Pas de config sauvÃ©e
                personality, tts_model, edge_voice, sample_path, embedding_path = "Samantha", "edge-tts", "fr-FR-DeniseNeural", None, None
            
            # 3. Initialisation LLM (Ollama unifiÃ©)
            self.llm = JarvisLLM()
            if not await self.llm.initialize():
                log.error("Ã‰chec initialisation LLM")
                return False
            
            # 4. Initialisation STT (local)
            self.stt = SpeechToText(device_index=device_index)
            if not self.stt.initialize():
                log.error("Ã‰chec initialisation STT")
                return False
            
            # 5. Initialisation TTS (NOUVELLE ARCHITECTURE)
            try:
                # NOUVEAU: Utiliser factory function ou constructeur direct
                self.tts = TextToSpeech(personality=personality)
                log.success(f"TTS nouvelle architecture initialisÃ©: {personality}")
            except Exception as tts_error:
                log.warning(f"Ã‰chec nouvelle architecture TTS: {tts_error}")
                # Fallback vers ancienne mÃ©thode si nÃ©cessaire
                self.tts = TextToSpeech(tts_model, personality, edge_voice, sample_path, embedding_path)
                log.info(f"TTS fallback initialisÃ©: {personality}")
            
            self.is_initialized = True
            
            log.success(f"ConversationFlow initialisÃ© - {personality} prÃªt (pipeline actif)")
            return True
            
        except Exception as e:
            log.error(f"Erreur initialisation ConversationFlow: {e}")
            return False
    
    async def process_voice_input(self):
        """Traite un message vocal complet : Ã©coute + traitement + rÃ©ponse"""
        if not self.is_initialized or not self.stt:
            await self._send_error("STT non disponible")
            return
        
        try:
            log.info("DÃ©but de l'Ã©coute vocale")
            await self._send_event('listening_start', '')
            
            # Ã‰couter
            loop = asyncio.get_event_loop()
            transcription = await loop.run_in_executor(
                None, 
                self.stt.listen_with_whisper_vad, 
                15
            )
            
            await self._send_event('listening_end', '')
            
            if transcription and transcription.strip():
                await self._send_event('transcription', transcription)
                
                # TRAITER DIRECTEMENT LE MESSAGE (Ã©vite la double transcription)
                await self.process_text_message(transcription)
            else:
                log.info("Aucune voix dÃ©tectÃ©e")
                
        except Exception as e:
            log.error(f"Erreur STT: {e}")
            await self._send_event('listening_end', '')
            await self._send_error(f"Erreur microphone: {str(e)}")
            
    async def process_text_message(self, message: str):
        """Traite un message texte utilisateur avec VRAI streaming + Pipeline TTS"""
        # Anti-duplication
        message_hash = hashlib.md5(message.encode()).hexdigest()
        current_time = time.time()
        
        # VÃ©rifier les doublons
        if message_hash in self.recent_messages:
            if current_time - self.recent_messages[message_hash] < 2.0:
                log.warning(f"Message dupliquÃ© ignorÃ©: {message[:30]}...")
                return
        
        self.recent_messages[message_hash] = current_time
        
        # Nettoyer les vieux hashes (>10s)
        self.recent_messages = {
            h: t for h, t in self.recent_messages.items() 
            if current_time - t < 10
        }
        
        # Lock pour Ã©viter les traitements simultanÃ©s
        async with self.processing_lock:
                
            if not self.is_initialized:
                await self._send_error("SystÃ¨me non initialisÃ©")
                return
            
            try:
                log.info(f"Message texte reÃ§u: {message[:50]}...")
                
                # Ajouter Ã  l'historique
                self._add_to_history('user', message)
                
                # Notifier le dÃ©but de traitement
                await self._send_event('message_processing_start', message)
                
                # ğŸš€ NOUVEAU: Pipeline complet LLM + TTS parallÃ¨le
                await self._process_with_parallel_pipeline(message)
                
            except Exception as e:
                log.error(f"Erreur traitement message: {e}")
                await self._send_error(f"Erreur traitement: {str(e)}")
    
    def _supports_pipeline(self) -> bool:
        """DÃ©termine si le TTS supporte le pipeline parallÃ¨le - ADAPTÃ‰ NOUVELLE ARCHITECTURE"""
        # PRIORITÃ‰ 1: VÃ©rifier si c'est la NOUVELLE architecture
        if hasattr(self.tts, 'pipeline') and hasattr(self.tts.pipeline, 'queue_text_chunk'):
            log.debug("âœ… NOUVELLE architecture TTS dÃ©tectÃ©e", "ğŸ”Š")
            return True
        
        # PRIORITÃ‰ 2: CompatibilitÃ© avec ancienne architecture
        if (hasattr(self.tts, 'is_edge') and 
            self.tts.is_edge and 
            hasattr(self.tts, 'add_text_chunk')):
            log.debug("âš ï¸ Ancienne architecture TTS dÃ©tectÃ©e", "ğŸ”Š")
            return True
            
        log.debug("âŒ Aucune architecture pipeline dÃ©tectÃ©e", "âš ï¸")
        return False

    async def _send_to_tts(self, text: str):
        """Envoie du texte au TTS - ADAPTÃ‰ NOUVELLE ARCHITECTURE"""
        # VÃ©rifier si l'audio est en sourdine
        config_manager = ConfigManager()
        is_muted = config_manager.get_config().get('audio', {}).get('output', {}).get('muted', False)

        if is_muted:
            log.debug("ğŸ”‡ Audio en sourdine, chunk TTS ignorÃ©.", "ğŸ”Š")
            return

        # PRIORITÃ‰ 1: Nouvelle architecture avec AudioPipeline
        if hasattr(self.tts, 'pipeline') and hasattr(self.tts.pipeline, 'queue_text_chunk'):
            await self.tts.pipeline.queue_text_chunk(text)
            log.debug(f"âœ… NOUVEAU pipeline: chunk envoyÃ©", "ğŸ”Š")
        
        # PRIORITÃ‰ 2: Ancienne architecture pipeline
        elif (hasattr(self.tts, 'is_edge') and 
              self.tts.is_edge and 
              hasattr(self.tts, 'add_text_chunk')):
            await self.tts.add_text_chunk(text)
            log.debug(f"âœ… ANCIEN pipeline: chunk envoyÃ©", "ğŸ”Š")
        
        # PRIORITÃ‰ 3: Fallback legacy
        else:
            await self.tts_queue.put(text)
            if not self.tts_worker_running:
                asyncio.create_task(self._tts_worker())
            log.debug(f"âš ï¸ Legacy: chunk envoyÃ©", "âš ï¸")

    async def _process_with_parallel_pipeline(self, message: str):
        """Pipeline complet LLM streaming + TTS parallÃ¨le ADAPTÃ‰ NOUVELLE ARCHITECTURE"""
        session_start = time.time()
        full_response = ""
        token_count = 0
        first_token_time = None
        first_audio_time = None
        sentence_buffer = ""
        
        try:
            log.debug("ğŸš€ DÃ©marrage pipeline complet LLM + TTS", "ğŸ”Š")

            # ğŸ”‡ VÃ©rification mode muet (Optimisation P1)
            config_manager = ConfigManager()
            is_muted = config_manager.get_config().get('audio', {}).get('output', {}).get('muted', False)

            if is_muted:
                log.debug("ğŸ”‡ Mode Muet activÃ© : Pipeline TTS dÃ©sactivÃ© (Optimisation)", "ğŸ”Š")
            
            # DÃ©marrer le pipeline TTS si supportÃ© ET non muet
            if not is_muted and self._supports_pipeline():
                log.debug("ğŸš€ PIPELINE: DÃ©marrage workers...", "ğŸ”Š")
                
                # NOUVEAU: DÃ©marrage pipeline selon architecture
                if hasattr(self.tts, 'pipeline'):
                    self.tts.pipeline.start_streaming_workers()
                    log.debug("âœ… NOUVEAU pipeline TTS dÃ©marrÃ©", "ğŸ”Š")
                elif hasattr(self.tts, '_start_parallel_workers'):
                    await self.tts._start_parallel_workers()
                    log.debug("âœ… ANCIEN pipeline TTS dÃ©marrÃ©", "ğŸ”Š")
                
                log.debug("âœ… Pipeline TTS dÃ©marrÃ©", "ğŸ”Š")
            elif is_muted:
                log.debug("ğŸ”‡ Pas de dÃ©marrage workers (Muet)", "ğŸ”Š")
            else:
                log.debug("âš ï¸ Utilisation ancien systÃ¨me TTS", "âš ï¸")
            
            # ğŸ”¥ STREAMING depuis Ollama (LLM unifiÃ©)
            # ğŸ§  NOUVEAU: PrÃ©chauffer TTS pendant que LLM dÃ©marre sa rÃ©flexion
            if not is_muted and self._supports_pipeline():
                # NOUVEAU: Warm-up selon architecture
                if hasattr(self.tts, 'pipeline'):
                    # Le warm-up est automatique dans AudioPipeline
                    log.debug("ğŸ”¥ Warm-up automatique NOUVEAU pipeline", "ğŸ”Š")
                elif hasattr(self.tts, 'warm_up_during_llm_thinking'):
                    asyncio.create_task(self.tts.warm_up_during_llm_thinking())
                    log.debug("ğŸ”¥ Warm-up ANCIEN pipeline", "ğŸ”Š")
            
            for token in self.llm.generate_response_stream(message):
                # Premier token - mesurer TTFT
                if first_token_time is None:
                    first_token_time = time.time() - session_start
                    await self._send_event('first_token', token, {
                        'ttft': first_token_time
                    })
                
                # Envoyer chaque token Ã  l'interface
                await self._send_event('llm_token', token)
                full_response += token
                token_count += 1
                sentence_buffer += token
                
                # ğŸ”¥ OPTIMISATION: DÃ©tection phrase complÃ¨te â†’ Envoi IMMÃ‰DIAT TTS
                # Ne traiter pour le TTS que si non muet
                if not is_muted and self._is_sentence_complete(sentence_buffer):
                    sentence_to_process = sentence_buffer.strip()
                    
                    if sentence_to_process:
                        # Nettoyer le texte pour le TTS
                        clean_sentence = self._clean_text_for_tts(sentence_to_process)
                        
                        if clean_sentence:
                            # Mesurer temps premier audio
                            if first_audio_time is None:
                                first_audio_time = time.time() - session_start

                            # Envoi au TTS (nouvelle architecture compatible)
                            await self._send_to_tts(clean_sentence)
                            log.debug(f"âœ… Chunk envoyÃ©: {clean_sentence[:40]}...", "ğŸ”Š")
                    
                    sentence_buffer = ""  # Reset buffer
                
                # Petite pause pour Ã©viter l'inondation WebSocket
                if token_count % 10 == 0:
                    await asyncio.sleep(0.001)
            
            # Traiter le reste du buffer s'il y a du contenu
            if not is_muted and sentence_buffer.strip():
                clean_last_chunk = self._clean_text_for_tts(sentence_buffer.strip())
                if clean_last_chunk:
                    await self._send_to_tts(clean_last_chunk)
                    log.debug("âœ… Dernier chunk envoyÃ©", "ğŸ”Š")
            
            # Finaliser le pipeline si actif avec timeout dynamique
            if not is_muted and self._supports_pipeline():
                # Timeout adaptatif selon la longueur de la rÃ©ponse
                estimated_time = token_count * 0.3  # 0.3s par token
                dynamic_timeout = max(60.0, estimated_time)  # Minimum 60s
                
                log.debug(f"â³ Attente fin conversation ({dynamic_timeout:.0f}s max)...", "ğŸ”Š")
                
                # NOUVEAU: Finalisation selon architecture
                if hasattr(self.tts, 'pipeline'):
                    # Attendre que le NOUVEAU pipeline se vide
                    start_wait = time.time()
                    while (time.time() - start_wait) < dynamic_timeout:
                        status = self.tts.pipeline.get_status()
                        if (status['chunks_in_generation_queue'] == 0 and 
                            status['chunks_in_playback_queue'] == 0):
                            break
                        await asyncio.sleep(0.5)
                    log.debug("âœ… NOUVEAU pipeline terminÃ©", "ğŸ”Š")
                    
                elif hasattr(self.tts, 'finalize_pipeline'):
                    await self.tts.finalize_pipeline(timeout=dynamic_timeout)
                    log.debug("âœ… ANCIEN pipeline terminÃ©", "ğŸ”Š")
                
                log.debug("âœ… Conversation terminÃ©e", "ğŸ”Š")
            
            total_time = time.time() - session_start
            tokens_per_second = token_count / max(total_time, 0.001)
            
            # Calculer efficacitÃ© du pipeline
            pipeline_efficiency = 0.0
            
            # NOUVEAU: Stats selon architecture
            if hasattr(self.tts, 'pipeline'):
                status = self.tts.pipeline.get_status()
                if status.get('stats', {}).get('chunks_generated', 0) > 0:
                    stats = status['stats']
                    if stats['total_generation_time'] > 0 and stats['total_playback_time'] > 0:
                        sequential_time = stats['total_generation_time'] + stats['total_playback_time']
                        parallel_time = max(stats['total_generation_time'], stats['total_playback_time'])
                        pipeline_efficiency = ((sequential_time - parallel_time) / sequential_time * 100)
            
            # ANCIEN: Stats ancienne architecture
            elif hasattr(self.tts, 'is_edge') and self.tts.is_edge and hasattr(self.tts, 'pipeline_stats'):
                stats = self.tts.pipeline_stats
                if stats['total_generation_time'] > 0 and stats['total_playback_time'] > 0:
                    sequential_time = stats['total_generation_time'] + stats['total_playback_time']
                    parallel_time = max(stats['total_generation_time'], stats['total_playback_time'])
                    pipeline_efficiency = ((sequential_time - parallel_time) / sequential_time * 100)
            
            await self._send_event('llm_complete', full_response, {
                'total_time': total_time,
                'token_count': token_count,
                'ttft': first_token_time or 0,
                'first_audio_time': first_audio_time or 0,
                'tokens_per_second': tokens_per_second,
                'pipeline_efficiency': pipeline_efficiency
            })
            
            # Ajouter la rÃ©ponse complÃ¨te Ã  l'historique
            self._add_to_history('assistant', full_response, token_count)
            
            # Mettre Ã  jour les stats
            self._update_session_stats({
                'total_time': total_time,
                'token_count': token_count,
                'ttft': first_token_time or 0,
                'first_audio_time': first_audio_time or 0,
                'tokens_per_second': tokens_per_second,
                'pipeline_efficiency': pipeline_efficiency
            })
            
            log.success(f"Message traitÃ©: {token_count} tokens en {total_time:.2f}s ({tokens_per_second:.1f} tok/s, gain: {pipeline_efficiency:.1f}%)")
            
        except Exception as e:
            log.error(f"Erreur pipeline parallÃ¨le: {e}")
            raise
    
    def _is_sentence_complete(self, text: str) -> bool:
        """DÃ©tecte si une phrase est complÃ¨te pour envoyer au TTS"""
        if not text.strip():
            return False
        
        # DÃ©limiteurs de fin de phrase
        sentence_enders = ['.', '!', '?', ':', ';']
        
        # VÃ©rifier fin de phrase
        if any(text.rstrip().endswith(ender) for ender in sentence_enders):
            return True
        
        # Phrases courtes (questions, exclamations)
        if len(text.split()) >= 4 and text.rstrip().endswith(('?', '!')):
            return True
        
        return False

    def _clean_text_for_tts(self, text: str) -> str:
        """Nettoie le texte avant de l'envoyer au TTS."""
        import re
        # Supprime le contenu entre les balises <think> et </think>
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        # Supprime les Ã©mojis
        emoji_pattern = re.compile(
            "["
            "\U0001F600-\U0001F64F"  # emoticons
            "\U0001F300-\U0001F5FF"  # symbols & pictographs
            "\U0001F680-\U0001F6FF"  # transport & map symbols
            "\U0001F1E0-\U0001F1FF"  # flags (iOS)
            "\U00002702-\U000027B0"
            "\U000024C2-\U0001F251"
            "]+",
            flags=re.UNICODE,
        )
        text = emoji_pattern.sub(r"", text)
        # Supprime les astÃ©risques d'action (ex: *sourit*)
        text = re.sub(r'\*.*?\*', '', text)
        return text.strip()
    
    async def reload_tts(self, model_name, personality, edge_voice=None, sample_path=None, embedding_path=None):
        """Recharge le TTS avec une nouvelle voix"""
        try:
            log.info(f"Rechargement TTS : {personality}")
            
            # ArrÃªter pipeline actuel si nÃ©cessaire
            if hasattr(self.tts, 'pipeline') and self.tts.pipeline.pipeline_active:
                self.tts.pipeline.stop_pipeline()
                await asyncio.sleep(0.5)  # Laisser temps d'arrÃªt
            elif hasattr(self.tts, 'parallel_pipeline_active') and self.tts.parallel_pipeline_active:
                self.tts.parallel_pipeline_active = False
                await asyncio.sleep(0.5)  # Laisser temps d'arrÃªt
            
            # NOUVEAU: CrÃ©er nouvelle instance TTS avec nouvelle architecture
            try:
                self.tts = TextToSpeech(personality=personality)
                log.success(f"TTS nouvelle architecture rechargÃ©: {personality}")
            except Exception as e:
                log.warning(f"Fallback ancienne architecture: {e}")
                # Fallback vers ancienne mÃ©thode
                self.tts = TextToSpeech(
                    model_name=model_name,
                    personality=personality,
                    edge_voice=edge_voice,
                    sample_path=sample_path,
                    embedding_path=embedding_path
                )
                log.success(f"TTS fallback rechargÃ©: {personality}")
            
            log.success(f"TTS rechargÃ© avec pipeline : {personality}")
            
        except Exception as e:
            log.error(f"Erreur rechargement TTS: {e}")
            raise

    async def reload_llm(self, model_name: str):
        """Change le modÃ¨le du client LLM existant."""
        try:
            if self.llm:
                log.info(f"Changement du modÃ¨le LLM vers : {model_name}")
                self.llm.change_model(model_name)
                log.success(f"ModÃ¨le LLM changÃ© vers : {model_name}")
            else:
                log.warning("Le client LLM n'est pas initialisÃ©, impossible de changer de modÃ¨le.")
        except Exception as e:
            log.error(f"Erreur lors du changement de modÃ¨le LLM : {e}")
            raise
    
    def get_personality(self) -> str:
        """Retourne la personnalitÃ© actuelle"""
        return self.personality or "Samantha"
    
    def get_display_name(self) -> str:
        """Retourne le nom d'affichage formatÃ©"""
        if hasattr(self, 'display_name') and self.display_name:
            return self.display_name
        else:
            personality = self.get_personality()
            return f"Assistant virtuel - {personality}"
        
    def update_voice_settings(self, speed: float = None, volume: int = None):
        """Met Ã  jour les paramÃ¨tres de voix en direct"""
        if self.tts:
            self.tts.update_voice_settings(speed, volume)
            log.info(f"ğŸ”Š ParamÃ¨tres voix mis Ã  jour: Speed={speed}, Volume={volume}")

    def get_history(self) -> Dict[str, Any]:
        """Retourne l'historique de conversation avec stats pipeline"""
        return {
            'success': True,
            'history': self.conversation_history,
            'stats': self.session_stats
        }
    
    def clear_history(self) -> Dict[str, Any]:
        """Efface l'historique de conversation"""
        self.conversation_history.clear()
        self.session_stats = {
            'messages_count': 0,
            'total_tokens': 0,
            'total_time': 0.0,
            'avg_response_time': 0.0,
            'avg_ttft': 0.0,
            'pipeline_efficiency': 0.0
        }
        
        log.info("Historique de conversation effacÃ©")
        return {'success': True}
    
    def set_websocket_callback(self, callback: Callable):
        """DÃ©finit le callback WebSocket pour les Ã©vÃ©nements"""
        self.websocket_callback = callback
    
    def _add_to_history(self, sender: str, content: str, token_count: int = 0):
        """Ajoute un message Ã  l'historique"""
        entry = {
            'sender': sender,
            'content': content,
            'timestamp': time.time(),
            'token_count': token_count if sender == 'assistant' else 0
        }
        
        self.conversation_history.append(entry)
        
        # Limiter l'historique (garder les 100 derniers messages)
        if len(self.conversation_history) > 100:
            self.conversation_history = self.conversation_history[-100:]
    
    def _update_session_stats(self, result: Dict[str, Any]):
        """Met Ã  jour les statistiques de session avec mÃ©triques pipeline"""
        self.session_stats['messages_count'] += 1
        self.session_stats['total_tokens'] += result.get('token_count', 0)
        self.session_stats['total_time'] += result.get('total_time', 0)
        
        # TTFT moyen
        if 'ttft' in result:
            current_ttft = self.session_stats.get('avg_ttft', 0)
            count = self.session_stats['messages_count']
            self.session_stats['avg_ttft'] = (current_ttft * (count - 1) + result['ttft']) / count
        
        # Temps de rÃ©ponse moyen
        if self.session_stats['messages_count'] > 0:
            self.session_stats['avg_response_time'] = (
                self.session_stats['total_time'] / self.session_stats['messages_count']
            )
        
        # NOUVEAU: EfficacitÃ© pipeline moyenne
        if 'pipeline_efficiency' in result and result['pipeline_efficiency'] > 0:
            current_eff = self.session_stats.get('pipeline_efficiency', 0)
            count = self.session_stats['messages_count']
            self.session_stats['pipeline_efficiency'] = (current_eff * (count - 1) + result['pipeline_efficiency']) / count
    
    async def _send_event(self, event_type: str, content: str, metadata: Dict = None):
        """Envoie un Ã©vÃ©nement via WebSocket"""
        if self.websocket_callback:
            event_data = {
                'type': event_type,
                'content': content,
                'timestamp': time.time(),
                'metadata': metadata or {}
            }
            await self.websocket_callback(event_data)
        else:
            log.warning(f"âš ï¸ WebSocket callback non dÃ©fini pour: {event_type}")
    
    async def _send_error(self, error_message: str):
        """Envoie une erreur via WebSocket"""
        await self._send_event('error', error_message)
    
    def stop(self):
        """ArrÃªte proprement le gestionnaire et le pipeline"""
        # ArrÃªter pipeline TTS si actif
        if hasattr(self.tts, 'pipeline') and hasattr(self.tts.pipeline, 'stop_pipeline'):
            self.tts.pipeline.stop_pipeline()
        elif hasattr(self.tts, 'parallel_pipeline_active') and self.tts.parallel_pipeline_active:
            self.tts.parallel_pipeline_active = False
        
        log.info("ConversationFlow arrÃªtÃ© avec pipeline")
    
    # === MÃ‰THODES LEGACY (pour compatibilitÃ©) ===
    
    async def _tts_worker(self):
        """Worker TTS simplifiÃ© et robuste (LEGACY - pour compatibilitÃ©)"""
        if self.tts_worker_running:
            return  # DÃ©jÃ  actif
            
        self.tts_worker_running = True
        log.debug("ğŸ”Š TTS worker legacy dÃ©marrÃ©")
        
        try:
            while self.tts_worker_running:
                try:
                    # Attendre segment (timeout plus long pour le streaming LLM)
                    segment = await asyncio.wait_for(self.tts_queue.get(), timeout=10.0)
                    
                    log.debug(f"ğŸ”Š TTS traite (legacy): {segment[:30]}...")
                    
                    # Utiliser l'ancienne mÃ©thode fiable OU la nouvelle
                    if hasattr(self.tts, 'speak'):
                        # NOUVEAU systÃ¨me: utiliser speak() async
                        await self.tts.speak(segment)
                    elif hasattr(self.tts, '_speak_response'):
                        # ANCIEN systÃ¨me: utiliser _speak_response
                        await self.tts._speak_response(segment)
                    else:
                        log.error("Aucune mÃ©thode TTS disponible")
                    
                    self.tts_queue.task_done()
                    
                except asyncio.TimeoutError:
                    # Plus rien depuis 10s â†’ arrÃªter
                    log.debug("ğŸ”Š TTS timeout - arrÃªt propre")
                    break
                    
        except Exception as e:
            log.error(f"âŒ Erreur TTS worker legacy: {e}")
        finally:
            self.tts_worker_running = False
            log.debug("ğŸ”Š TTS worker legacy arrÃªtÃ© proprement")